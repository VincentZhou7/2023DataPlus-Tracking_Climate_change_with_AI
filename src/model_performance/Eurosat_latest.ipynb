{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzxnWPDxlg7N",
        "outputId": "6f3de157-2ea6-4e82-cb8e-087d1c0f562a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ftfy in /data/users/xz306/.local/lib/python3.8/site-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /data/users/xz306/.local/lib/python3.8/site-packages (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /data/users/xz306/.local/lib/python3.8/site-packages (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /data/users/xz306/.local/lib/python3.8/site-packages (from ftfy) (0.2.6)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-hiqbosdc\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-hiqbosdc\n",
            "Requirement already satisfied (use --upgrade to upgrade): clip==1.0 from git+https://github.com/openai/CLIP.git in /data/users/xz306/.local/lib/python3.8/site-packages\n",
            "Requirement already satisfied: ftfy in /data/users/xz306/.local/lib/python3.8/site-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /data/users/xz306/.local/lib/python3.8/site-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: torch in /data/users/xz306/.local/lib/python3.8/site-packages (from clip==1.0) (2.0.1)\n",
            "Requirement already satisfied: torchvision in /data/users/xz306/.local/lib/python3.8/site-packages (from clip==1.0) (0.15.2)\n",
            "Requirement already satisfied: tqdm in /data/users/xz306/.local/lib/python3.8/site-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /data/users/xz306/.local/lib/python3.8/site-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /data/users/xz306/.local/lib/python3.8/site-packages (from torch->clip==1.0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /data/users/xz306/.local/lib/python3.8/site-packages (from torch->clip==1.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /data/users/xz306/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /data/users/xz306/.local/lib/python3.8/site-packages (from torch->clip==1.0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /data/users/xz306/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /data/users/xz306/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.7.91)\n",
            "Requirement already satisfied: typing-extensions in /data/users/xz306/.local/lib/python3.8/site-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /data/users/xz306/.local/lib/python3.8/site-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /data/users/xz306/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.10.3.66)\n",
            "Requirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch->clip==1.0) (2.4)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /data/users/xz306/.local/lib/python3.8/site-packages (from torch->clip==1.0) (10.9.0.58)\n",
            "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch->clip==1.0) (3.0.12)\n",
            "Requirement already satisfied: triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /data/users/xz306/.local/lib/python3.8/site-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch->clip==1.0) (2.10.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /data/users/xz306/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /data/users/xz306/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /data/users/xz306/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.7.99)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision->clip==1.0) (7.0.0)\n",
            "Requirement already satisfied: numpy in /data/users/xz306/.local/lib/python3.8/site-packages (from torchvision->clip==1.0) (1.23.1)\n",
            "Requirement already satisfied: requests in /data/users/xz306/.local/lib/python3.8/site-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch->clip==1.0) (45.2.0)\n",
            "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch->clip==1.0) (0.34.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /data/users/xz306/.local/lib/python3.8/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: cmake in /data/users/xz306/.local/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch->clip==1.0) (3.26.4)\n",
            "Requirement already satisfied: lit in /data/users/xz306/.local/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch->clip==1.0) (16.0.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/users/xz306/.local/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (2.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /data/users/xz306/.local/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision->clip==1.0) (2019.11.28)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision->clip==1.0) (2.8)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369522 sha256=b3af5e46622ea1209e57f9c4e38cf62978b00acbae7b11458a44587677af9bee\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wpdlp7lh/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n",
            "Successfully built clip\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXpvCXwLlnkF",
        "outputId": "8ac156b3-7074-4ce9-9994-7334ddeed585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 2.0.1+cu117\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for CLIP:\n\tMissing key(s) in state_dict: \"positional_embedding\", \"text_projection\", \"visual.conv1.weight\", \"visual.bn1.weight\", \"visual.bn1.bias\", \"visual.bn1.running_mean\", \"visual.bn1.running_var\", \"visual.conv2.weight\", \"visual.bn2.weight\", \"visual.bn2.bias\", \"visual.bn2.running_mean\", \"visual.bn2.running_var\", \"visual.conv3.weight\", \"visual.bn3.weight\", \"visual.bn3.bias\", \"visual.bn3.running_mean\", \"visual.bn3.running_var\", \"visual.layer1.0.conv1.weight\", \"visual.layer1.0.bn1.weight\", \"visual.layer1.0.bn1.bias\", \"visual.layer1.0.bn1.running_mean\", \"visual.layer1.0.bn1.running_var\", \"visual.layer1.0.conv2.weight\", \"visual.layer1.0.bn2.weight\", \"visual.layer1.0.bn2.bias\", \"visual.layer1.0.bn2.running_mean\", \"visual.layer1.0.bn2.running_var\", \"visual.layer1.0.conv3.weight\", \"visual.layer1.0.bn3.weight\", \"visual.layer1.0.bn3.bias\", \"visual.layer1.0.bn3.running_mean\", \"visual.layer1.0.bn3.running_var\", \"visual.layer1.0.downsample.0.weight\", \"visual.layer1.0.downsample.1.weight\", \"visual.layer1.0.downsample.1.bias\", \"visual.layer1.0.downsample.1.running_mean\", \"visual.layer1.0.downsample.1.running_var\", \"visual.layer1.1.conv1.weight\", \"visual.layer1.1.bn1.weight\", \"visual.layer1.1.bn1.bias\", \"visual.layer1.1.bn1.running_mean\", \"visual.layer1.1.bn1.running_var\", \"visual.layer1.1.conv2.weight\", \"visual.layer1.1.bn2.weight\", \"visual.layer1.1.bn2.bias\", \"visual.layer1.1.bn2.running_mean\", \"visual.layer1.1.bn2.running_var\", \"visual.layer1.1.conv3.weight\", \"visual.layer1.1.bn3.weight\", \"visual.layer1.1.bn3.bias\", \"visual.layer1.1.bn3.running_mean\", \"visual.layer1.1.bn3.running_var\", \"visual.layer1.2.conv1.weight\", \"visual.layer1.2.bn1.weight\", \"visual.layer1.2.bn1.bias\", \"visual.layer1.2.bn1.running_mean\", \"visual.layer1.2.bn1.running_var\", \"visual.layer1.2.conv2.weight\", \"visual.layer1.2.bn2.weight\", \"visual.layer1.2.bn2.bias\", \"visual.layer1.2.bn2.running_mean\", \"visual.layer1.2.bn2.running_var\", \"visual.layer1.2.conv3.weight\", \"visual.layer1.2.bn3.weight\", \"visual.layer1.2.bn3.bias\", \"visual.layer1.2.bn3.running_mean\", \"visual.layer1.2.bn3.running_var\", \"visual.layer2.0.conv1.weight\", \"visual.layer2.0.bn1.weight\", \"visual.layer2.0.bn1.bias\", \"visual.layer2.0.bn1.running_mean\", \"visual.layer2.0.bn1.running_var\", \"visual.layer2.0.conv2.weight\", \"visual.layer2.0.bn2.weight\", \"visual.layer2.0.bn2.bias\", \"visual.layer2.0.bn2.running_mean\", \"visual.layer2.0.bn2.running_var\", \"visual.layer2.0.conv3.weight\", \"visual.layer2.0.bn3.weight\", \"visual.layer2.0.bn3.bias\", \"visual.layer2.0.bn3.running_mean\", \"visual.layer2.0.bn3.running_var\", \"visual.layer2.0.downsample.0.weight\", \"visual.layer2.0.downsample.1.weight\", \"visual.layer2.0.downsample.1.bias\", \"visual.layer2.0.downsample.1.running_mean\", \"visual.layer2.0.downsample.1.running_var\", \"visual.layer2.1.conv1.weight\", \"visual.layer2.1.bn1.weight\", \"visual.layer2.1.bn1.bias\", \"visual.layer2.1.bn1.running_mean\", \"visual.layer2.1.bn1.running_var\", \"visual.layer2.1.conv2.weight\", \"visual.layer2.1.bn2.weight\", \"visual.layer2.1.bn2.bias\", \"visual.layer2.1.bn2.running_mean\", \"visual.layer2.1.bn2.running_var\", \"visual.layer2.1.conv3.weight\", \"visual.layer2.1.bn3.weight\", \"visual.layer2.1.bn3.bias\", \"visual.layer2.1.bn3.running_mean\", \"visual.layer2.1.bn3.running_var\", \"visual.layer2.2.conv1.weight\", \"visual.layer2.2.bn1.weight\", \"visual.layer2.2.bn1.bias\", \"visual.layer2.2.bn1.running_mean\", \"visual.layer2.2.bn1.running_var\", \"visual.layer2.2.conv2.weight\", \"visual.layer2.2.bn2.weight\", \"visual.layer2.2.bn2.bias\", \"visual.layer2.2.bn2.running_mean\", \"visual.layer2.2.bn2.running_var\", \"visual.layer2.2.conv3.weight\", \"visual.layer2.2.bn3.weight\", \"visual.layer2.2.bn3.bias\", \"visual.layer2.2.bn3.running_mean\", \"visual.layer2.2.bn3.running_var\", \"visual.layer2.3.conv1.weight\", \"visual.layer2.3.bn1.weight\", \"visual.layer2.3.bn1.bias\", \"visual.layer2.3.bn1.running_mean\", \"visual.layer2.3.bn1.running_var\", \"visual.layer2.3.conv2.weight\", \"visual.layer2.3.bn2.weight\", \"visual.layer2.3.bn2.bias\", \"visual.layer2.3.bn2.running_mean\", \"visual.layer2.3.bn2.running_var\", \"visual.layer2.3.conv3.weight\", \"visual.layer2.3.bn3.weight\", \"visual.layer2.3.bn3.bias\", \"visual.layer2.3.bn3.running_mean\", \"visual.layer2.3.bn3.running_var\", \"visual.layer3.0.conv1.weight\", \"visual.layer3.0.bn1.weight\", \"visual.layer3.0.bn1.bias\", \"visual.layer3.0.bn1.running_mean\", \"visual.layer3.0.bn1.running_var\", \"visual.layer3.0.conv2.weight\", \"visual.layer3.0.bn2.weight\", \"visual.layer3.0.bn2.bias\", \"visual.layer3.0.bn2.running_mean\", \"visual.layer3.0.bn2.running_var\", \"visual.layer3.0.conv3.weight\", \"visual.layer3.0.bn3.weight\", \"visual.layer3.0.bn3.bias\", \"visual.layer3.0.bn3.running_mean\", \"visual.layer3.0.bn3.running_var\", \"visual.layer3.0.downsample.0.weight\", \"visual.layer3.0.downsample.1.weight\", \"visual.layer3.0.downsample.1.bias\", \"visual.layer3.0.downsample.1.running_mean\", \"visual.layer3.0.downsample.1.running_var\", \"visual.layer3.1.conv1.weight\", \"visual.layer3.1.bn1.weight\", \"visual.layer3.1.bn1.bias\", \"visual.layer3.1.bn1.running_mean\", \"visual.layer3.1.bn1.running_var\", \"visual.layer3.1.conv2.weight\", \"visual.layer3.1.bn2.weight\", \"visual.layer3.1.bn2.bias\", \"visual.layer3.1.bn2.running_mean\", \"visual.layer3.1.bn2.running_var\", \"visual.layer3.1.conv3.weight\", \"visual.layer3.1.bn3.weight\", \"visual.layer3.1.bn3.bias\", \"visual.layer3.1.bn3.running_mean\", \"visual.layer3.1.bn3.running_var\", \"visual.layer3.2.conv1.weight\", \"visual.layer3.2.bn1.weight\", \"visual.layer3.2.bn1.bias\", \"visual.layer3.2.bn1.running_mean\", \"visual.layer3.2.bn1.running_var\", \"visual.layer3.2.conv2.weight\", \"visual.layer3.2.bn2.weight\", \"visual.layer3.2.bn2.bias\", \"visual.layer3.2.bn2.running_mean\", \"visual.layer3.2.bn2.running_var\", \"visual.layer3.2.conv3.weight\", \"visual.layer3.2.bn3.weight\", \"visual.layer3.2.bn3.bias\", \"visual.layer3.2.bn3.running_mean\", \"visual.layer3.2.bn3.running_var\", \"visual.layer3.3.conv1.weight\", \"visual.layer3.3.bn1.weight\", \"visual.layer3.3.bn1.bias\", \"visual.layer3.3.bn1.running_mean\", \"visual.layer3.3.bn1.running_var\", \"visual.layer3.3.conv2.weight\", \"visual.layer3.3.bn2.weight\", \"visual.layer3.3.bn2.bias\", \"visual.layer3.3.bn2.running_mean\", \"visual.layer3.3.bn2.running_var\", \"visual.layer3.3.conv3.weight\", \"visual.layer3.3.bn3.weight\", \"visual.layer3.3.bn3.bias\", \"visual.layer3.3.bn3.running_mean\", \"visual.layer3.3.bn3.running_var\", \"visual.layer3.4.conv1.weight\", \"visual.layer3.4.bn1.weight\", \"visual.layer3.4.bn1.bias\", \"visual.layer3.4.bn1.running_mean\", \"visual.layer3.4.bn1.running_var\", \"visual.layer3.4.conv2.weight\", \"visual.layer3.4.bn2.weight\", \"visual.layer3.4.bn2.bias\", \"visual.layer3.4.bn2.running_mean\", \"visual.layer3.4.bn2.running_var\", \"visual.layer3.4.conv3.weight\", \"visual.layer3.4.bn3.weight\", \"visual.layer3.4.bn3.bias\", \"visual.layer3.4.bn3.running_mean\", \"visual.layer3.4.bn3.running_var\", \"visual.layer3.5.conv1.weight\", \"visual.layer3.5.bn1.weight\", \"visual.layer3.5.bn1.bias\", \"visual.layer3.5.bn1.running_mean\", \"visual.layer3.5.bn1.running_var\", \"visual.layer3.5.conv2.weight\", \"visual.layer3.5.bn2.weight\", \"visual.layer3.5.bn2.bias\", \"visual.layer3.5.bn2.running_mean\", \"visual.layer3.5.bn2.running_var\", \"visual.layer3.5.conv3.weight\", \"visual.layer3.5.bn3.weight\", \"visual.layer3.5.bn3.bias\", \"visual.layer3.5.bn3.running_mean\", \"visual.layer3.5.bn3.running_var\", \"visual.layer4.0.conv1.weight\", \"visual.layer4.0.bn1.weight\", \"visual.layer4.0.bn1.bias\", \"visual.layer4.0.bn1.running_mean\", \"visual.layer4.0.bn1.running_var\", \"visual.layer4.0.conv2.weight\", \"visual.layer4.0.bn2.weight\", \"visual.layer4.0.bn2.bias\", \"visual.layer4.0.bn2.running_mean\", \"visual.layer4.0.bn2.running_var\", \"visual.layer4.0.conv3.weight\", \"visual.layer4.0.bn3.weight\", \"visual.layer4.0.bn3.bias\", \"visual.layer4.0.bn3.running_mean\", \"visual.layer4.0.bn3.running_var\", \"visual.layer4.0.downsample.0.weight\", \"visual.layer4.0.downsample.1.weight\", \"visual.layer4.0.downsample.1.bias\", \"visual.layer4.0.downsample.1.running_mean\", \"visual.layer4.0.downsample.1.running_var\", \"visual.layer4.1.conv1.weight\", \"visual.layer4.1.bn1.weight\", \"visual.layer4.1.bn1.bias\", \"visual.layer4.1.bn1.running_mean\", \"visual.layer4.1.bn1.running_var\", \"visual.layer4.1.conv2.weight\", \"visual.layer4.1.bn2.weight\", \"visual.layer4.1.bn2.bias\", \"visual.layer4.1.bn2.running_mean\", \"visual.layer4.1.bn2.running_var\", \"visual.layer4.1.conv3.weight\", \"visual.layer4.1.bn3.weight\", \"visual.layer4.1.bn3.bias\", \"visual.layer4.1.bn3.running_mean\", \"visual.layer4.1.bn3.running_var\", \"visual.layer4.2.conv1.weight\", \"visual.layer4.2.bn1.weight\", \"visual.layer4.2.bn1.bias\", \"visual.layer4.2.bn1.running_mean\", \"visual.layer4.2.bn1.running_var\", \"visual.layer4.2.conv2.weight\", \"visual.layer4.2.bn2.weight\", \"visual.layer4.2.bn2.bias\", \"visual.layer4.2.bn2.running_mean\", \"visual.layer4.2.bn2.running_var\", \"visual.layer4.2.conv3.weight\", \"visual.layer4.2.bn3.weight\", \"visual.layer4.2.bn3.bias\", \"visual.layer4.2.bn3.running_mean\", \"visual.layer4.2.bn3.running_var\", \"visual.attnpool.positional_embedding\", \"visual.attnpool.k_proj.weight\", \"visual.attnpool.k_proj.bias\", \"visual.attnpool.q_proj.weight\", \"visual.attnpool.q_proj.bias\", \"visual.attnpool.v_proj.weight\", \"visual.attnpool.v_proj.bias\", \"visual.attnpool.c_proj.weight\", \"visual.attnpool.c_proj.bias\", \"transformer.resblocks.0.attn.in_proj_weight\", \"transformer.resblocks.0.attn.in_proj_bias\", \"transformer.resblocks.0.attn.out_proj.weight\", \"transformer.resblocks.0.attn.out_proj.bias\", \"transformer.resblocks.0.ln_1.weight\", \"transformer.resblocks.0.ln_1.bias\", \"transformer.resblocks.0.mlp.c_fc.weight\", \"transformer.resblocks.0.mlp.c_fc.bias\", \"transformer.resblocks.0.mlp.c_proj.weight\", \"transformer.resblocks.0.mlp.c_proj.bias\", \"transformer.resblocks.0.ln_2.weight\", \"transformer.resblocks.0.ln_2.bias\", \"transformer.resblocks.1.attn.in_proj_weight\", \"transformer.resblocks.1.attn.in_proj_bias\", \"transformer.resblocks.1.attn.out_proj.weight\", \"transformer.resblocks.1.attn.out_proj.bias\", \"transformer.resblocks.1.ln_1.weight\", \"transformer.resblocks.1.ln_1.bias\", \"transformer.resblocks.1.mlp.c_fc.weight\", \"transformer.resblocks.1.mlp.c_fc.bias\", \"transformer.resblocks.1.mlp.c_proj.weight\", \"transformer.resblocks.1.mlp.c_proj.bias\", \"transformer.resblocks.1.ln_2.weight\", \"transformer.resblocks.1.ln_2.bias\", \"transformer.resblocks.2.attn.in_proj_weight\", \"transformer.resblocks.2.attn.in_proj_bias\", \"transformer.resblocks.2.attn.out_proj.weight\", \"transformer.resblocks.2.attn.out_proj.bias\", \"transformer.resblocks.2.ln_1.weight\", \"transformer.resblocks.2.ln_1.bias\", \"transformer.resblocks.2.mlp.c_fc.weight\", \"transformer.resblocks.2.mlp.c_fc.bias\", \"transformer.resblocks.2.mlp.c_proj.weight\", \"transformer.resblocks.2.mlp.c_proj.bias\", \"transformer.resblocks.2.ln_2.weight\", \"transformer.resblocks.2.ln_2.bias\", \"transformer.resblocks.3.attn.in_proj_weight\", \"transformer.resblocks.3.attn.in_proj_bias\", \"transformer.resblocks.3.attn.out_proj.weight\", \"transformer.resblocks.3.attn.out_proj.bias\", \"transformer.resblocks.3.ln_1.weight\", \"transformer.resblocks.3.ln_1.bias\", \"transformer.resblocks.3.mlp.c_fc.weight\", \"transformer.resblocks.3.mlp.c_fc.bias\", \"transformer.resblocks.3.mlp.c_proj.weight\", \"transformer.resblocks.3.mlp.c_proj.bias\", \"transformer.resblocks.3.ln_2.weight\", \"transformer.resblocks.3.ln_2.bias\", \"transformer.resblocks.4.attn.in_proj_weight\", \"transformer.resblocks.4.attn.in_proj_bias\", \"transformer.resblocks.4.attn.out_proj.weight\", \"transformer.resblocks.4.attn.out_proj.bias\", \"transformer.resblocks.4.ln_1.weight\", \"transformer.resblocks.4.ln_1.bias\", \"transformer.resblocks.4.mlp.c_fc.weight\", \"transformer.resblocks.4.mlp.c_fc.bias\", \"transformer.resblocks.4.mlp.c_proj.weight\", \"transformer.resblocks.4.mlp.c_proj.bias\", \"transformer.resblocks.4.ln_2.weight\", \"transformer.resblocks.4.ln_2.bias\", \"transformer.resblocks.5.attn.in_proj_weight\", \"transformer.resblocks.5.attn.in_proj_bias\", \"transformer.resblocks.5.attn.out_proj.weight\", \"transformer.resblocks.5.attn.out_proj.bias\", \"transformer.resblocks.5.ln_1.weight\", \"transformer.resblocks.5.ln_1.bias\", \"transformer.resblocks.5.mlp.c_fc.weight\", \"transformer.resblocks.5.mlp.c_fc.bias\", \"transformer.resblocks.5.mlp.c_proj.weight\", \"transformer.resblocks.5.mlp.c_proj.bias\", \"transformer.resblocks.5.ln_2.weight\", \"transformer.resblocks.5.ln_2.bias\", \"transformer.resblocks.6.attn.in_proj_weight\", \"transformer.resblocks.6.attn.in_proj_bias\", \"transformer.resblocks.6.attn.out_proj.weight\", \"transformer.resblocks.6.attn.out_proj.bias\", \"transformer.resblocks.6.ln_1.weight\", \"transformer.resblocks.6.ln_1.bias\", \"transformer.resblocks.6.mlp.c_fc.weight\", \"transformer.resblocks.6.mlp.c_fc.bias\", \"transformer.resblocks.6.mlp.c_proj.weight\", \"transformer.resblocks.6.mlp.c_proj.bias\", \"transformer.resblocks.6.ln_2.weight\", \"transformer.resblocks.6.ln_2.bias\", \"transformer.resblocks.7.attn.in_proj_weight\", \"transformer.resblocks.7.attn.in_proj_bias\", \"transformer.resblocks.7.attn.out_proj.weight\", \"transformer.resblocks.7.attn.out_proj.bias\", \"transformer.resblocks.7.ln_1.weight\", \"transformer.resblocks.7.ln_1.bias\", \"transformer.resblocks.7.mlp.c_fc.weight\", \"transformer.resblocks.7.mlp.c_fc.bias\", \"transformer.resblocks.7.mlp.c_proj.weight\", \"transformer.resblocks.7.mlp.c_proj.bias\", \"transformer.resblocks.7.ln_2.weight\", \"transformer.resblocks.7.ln_2.bias\", \"transformer.resblocks.8.attn.in_proj_weight\", \"transformer.resblocks.8.attn.in_proj_bias\", \"transformer.resblocks.8.attn.out_proj.weight\", \"transformer.resblocks.8.attn.out_proj.bias\", \"transformer.resblocks.8.ln_1.weight\", \"transformer.resblocks.8.ln_1.bias\", \"transformer.resblocks.8.mlp.c_fc.weight\", \"transformer.resblocks.8.mlp.c_fc.bias\", \"transformer.resblocks.8.mlp.c_proj.weight\", \"transformer.resblocks.8.mlp.c_proj.bias\", \"transformer.resblocks.8.ln_2.weight\", \"transformer.resblocks.8.ln_2.bias\", \"transformer.resblocks.9.attn.in_proj_weight\", \"transformer.resblocks.9.attn.in_proj_bias\", \"transformer.resblocks.9.attn.out_proj.weight\", \"transformer.resblocks.9.attn.out_proj.bias\", \"transformer.resblocks.9.ln_1.weight\", \"transformer.resblocks.9.ln_1.bias\", \"transformer.resblocks.9.mlp.c_fc.weight\", \"transformer.resblocks.9.mlp.c_fc.bias\", \"transformer.resblocks.9.mlp.c_proj.weight\", \"transformer.resblocks.9.mlp.c_proj.bias\", \"transformer.resblocks.9.ln_2.weight\", \"transformer.resblocks.9.ln_2.bias\", \"transformer.resblocks.10.attn.in_proj_weight\", \"transformer.resblocks.10.attn.in_proj_bias\", \"transformer.resblocks.10.attn.out_proj.weight\", \"transformer.resblocks.10.attn.out_proj.bias\", \"transformer.resblocks.10.ln_1.weight\", \"transformer.resblocks.10.ln_1.bias\", \"transformer.resblocks.10.mlp.c_fc.weight\", \"transformer.resblocks.10.mlp.c_fc.bias\", \"transformer.resblocks.10.mlp.c_proj.weight\", \"transformer.resblocks.10.mlp.c_proj.bias\", \"transformer.resblocks.10.ln_2.weight\", \"transformer.resblocks.10.ln_2.bias\", \"transformer.resblocks.11.attn.in_proj_weight\", \"transformer.resblocks.11.attn.in_proj_bias\", \"transformer.resblocks.11.attn.out_proj.weight\", \"transformer.resblocks.11.attn.out_proj.bias\", \"transformer.resblocks.11.ln_1.weight\", \"transformer.resblocks.11.ln_1.bias\", \"transformer.resblocks.11.mlp.c_fc.weight\", \"transformer.resblocks.11.mlp.c_fc.bias\", \"transformer.resblocks.11.mlp.c_proj.weight\", \"transformer.resblocks.11.mlp.c_proj.bias\", \"transformer.resblocks.11.ln_2.weight\", \"transformer.resblocks.11.ln_2.bias\", \"token_embedding.weight\", \"ln_final.weight\", \"ln_final.bias\". \n\tUnexpected key(s) in state_dict: \"vision_model.vision_model.embeddings.class_embedding\", \"vision_model.vision_model.embeddings.patch_embedding.weight\", \"vision_model.vision_model.embeddings.position_embedding.weight\", \"vision_model.vision_model.pre_layrnorm.weight\", \"vision_model.vision_model.pre_layrnorm.bias\", \"vision_model.vision_model.encoder.layers.0.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.0.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.0.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.0.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.0.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.0.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.0.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.0.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.0.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.0.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.0.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.0.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.0.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.0.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.0.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.0.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.1.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.1.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.1.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.1.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.1.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.1.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.1.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.1.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.1.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.1.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.1.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.1.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.1.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.1.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.1.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.1.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.2.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.2.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.2.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.2.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.2.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.2.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.2.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.2.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.2.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.2.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.2.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.2.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.2.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.2.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.2.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.2.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.3.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.3.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.3.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.3.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.3.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.3.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.3.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.3.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.3.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.3.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.3.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.3.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.3.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.3.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.3.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.3.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.4.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.4.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.4.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.4.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.4.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.4.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.4.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.4.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.4.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.4.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.4.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.4.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.4.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.4.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.4.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.4.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.5.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.5.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.5.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.5.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.5.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.5.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.5.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.5.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.5.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.5.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.5.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.5.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.5.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.5.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.5.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.5.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.6.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.6.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.6.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.6.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.6.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.6.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.6.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.6.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.6.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.6.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.6.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.6.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.6.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.6.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.6.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.6.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.7.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.7.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.7.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.7.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.7.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.7.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.7.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.7.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.7.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.7.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.7.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.7.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.7.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.7.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.7.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.7.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.8.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.8.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.8.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.8.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.8.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.8.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.8.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.8.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.8.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.8.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.8.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.8.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.8.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.8.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.8.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.8.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.9.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.9.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.9.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.9.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.9.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.9.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.9.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.9.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.9.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.9.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.9.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.9.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.9.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.9.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.9.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.9.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.10.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.10.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.10.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.10.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.10.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.10.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.10.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.10.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.10.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.10.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.10.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.10.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.10.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.10.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.10.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.10.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.11.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.11.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.11.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.11.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.11.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.11.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.11.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.11.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.11.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.11.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.11.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.11.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.11.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.11.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.11.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.11.layer_norm2.bias\", \"vision_model.vision_model.post_layernorm.weight\", \"vision_model.vision_model.post_layernorm.bias\", \"text_model.embeddings.word_embeddings.weight\", \"text_model.embeddings.position_embeddings.weight\", \"text_model.embeddings.token_type_embeddings.weight\", \"text_model.embeddings.LayerNorm.weight\", \"text_model.embeddings.LayerNorm.bias\", \"text_model.encoder.layer.0.attention.self.query.weight\", \"text_model.encoder.layer.0.attention.self.query.bias\", \"text_model.encoder.layer.0.attention.self.key.weight\", \"text_model.encoder.layer.0.attention.self.key.bias\", \"text_model.encoder.layer.0.attention.self.value.weight\", \"text_model.encoder.layer.0.attention.self.value.bias\", \"text_model.encoder.layer.0.attention.output.dense.weight\", \"text_model.encoder.layer.0.attention.output.dense.bias\", \"text_model.encoder.layer.0.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.0.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.0.intermediate.dense.weight\", \"text_model.encoder.layer.0.intermediate.dense.bias\", \"text_model.encoder.layer.0.output.dense.weight\", \"text_model.encoder.layer.0.output.dense.bias\", \"text_model.encoder.layer.0.output.LayerNorm.weight\", \"text_model.encoder.layer.0.output.LayerNorm.bias\", \"text_model.encoder.layer.1.attention.self.query.weight\", \"text_model.encoder.layer.1.attention.self.query.bias\", \"text_model.encoder.layer.1.attention.self.key.weight\", \"text_model.encoder.layer.1.attention.self.key.bias\", \"text_model.encoder.layer.1.attention.self.value.weight\", \"text_model.encoder.layer.1.attention.self.value.bias\", \"text_model.encoder.layer.1.attention.output.dense.weight\", \"text_model.encoder.layer.1.attention.output.dense.bias\", \"text_model.encoder.layer.1.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.1.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.1.intermediate.dense.weight\", \"text_model.encoder.layer.1.intermediate.dense.bias\", \"text_model.encoder.layer.1.output.dense.weight\", \"text_model.encoder.layer.1.output.dense.bias\", \"text_model.encoder.layer.1.output.LayerNorm.weight\", \"text_model.encoder.layer.1.output.LayerNorm.bias\", \"text_model.encoder.layer.2.attention.self.query.weight\", \"text_model.encoder.layer.2.attention.self.query.bias\", \"text_model.encoder.layer.2.attention.self.key.weight\", \"text_model.encoder.layer.2.attention.self.key.bias\", \"text_model.encoder.layer.2.attention.self.value.weight\", \"text_model.encoder.layer.2.attention.self.value.bias\", \"text_model.encoder.layer.2.attention.output.dense.weight\", \"text_model.encoder.layer.2.attention.output.dense.bias\", \"text_model.encoder.layer.2.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.2.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.2.intermediate.dense.weight\", \"text_model.encoder.layer.2.intermediate.dense.bias\", \"text_model.encoder.layer.2.output.dense.weight\", \"text_model.encoder.layer.2.output.dense.bias\", \"text_model.encoder.layer.2.output.LayerNorm.weight\", \"text_model.encoder.layer.2.output.LayerNorm.bias\", \"text_model.encoder.layer.3.attention.self.query.weight\", \"text_model.encoder.layer.3.attention.self.query.bias\", \"text_model.encoder.layer.3.attention.self.key.weight\", \"text_model.encoder.layer.3.attention.self.key.bias\", \"text_model.encoder.layer.3.attention.self.value.weight\", \"text_model.encoder.layer.3.attention.self.value.bias\", \"text_model.encoder.layer.3.attention.output.dense.weight\", \"text_model.encoder.layer.3.attention.output.dense.bias\", \"text_model.encoder.layer.3.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.3.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.3.intermediate.dense.weight\", \"text_model.encoder.layer.3.intermediate.dense.bias\", \"text_model.encoder.layer.3.output.dense.weight\", \"text_model.encoder.layer.3.output.dense.bias\", \"text_model.encoder.layer.3.output.LayerNorm.weight\", \"text_model.encoder.layer.3.output.LayerNorm.bias\", \"text_model.encoder.layer.4.attention.self.query.weight\", \"text_model.encoder.layer.4.attention.self.query.bias\", \"text_model.encoder.layer.4.attention.self.key.weight\", \"text_model.encoder.layer.4.attention.self.key.bias\", \"text_model.encoder.layer.4.attention.self.value.weight\", \"text_model.encoder.layer.4.attention.self.value.bias\", \"text_model.encoder.layer.4.attention.output.dense.weight\", \"text_model.encoder.layer.4.attention.output.dense.bias\", \"text_model.encoder.layer.4.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.4.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.4.intermediate.dense.weight\", \"text_model.encoder.layer.4.intermediate.dense.bias\", \"text_model.encoder.layer.4.output.dense.weight\", \"text_model.encoder.layer.4.output.dense.bias\", \"text_model.encoder.layer.4.output.LayerNorm.weight\", \"text_model.encoder.layer.4.output.LayerNorm.bias\", \"text_model.encoder.layer.5.attention.self.query.weight\", \"text_model.encoder.layer.5.attention.self.query.bias\", \"text_model.encoder.layer.5.attention.self.key.weight\", \"text_model.encoder.layer.5.attention.self.key.bias\", \"text_model.encoder.layer.5.attention.self.value.weight\", \"text_model.encoder.layer.5.attention.self.value.bias\", \"text_model.encoder.layer.5.attention.output.dense.weight\", \"text_model.encoder.layer.5.attention.output.dense.bias\", \"text_model.encoder.layer.5.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.5.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.5.intermediate.dense.weight\", \"text_model.encoder.layer.5.intermediate.dense.bias\", \"text_model.encoder.layer.5.output.dense.weight\", \"text_model.encoder.layer.5.output.dense.bias\", \"text_model.encoder.layer.5.output.LayerNorm.weight\", \"text_model.encoder.layer.5.output.LayerNorm.bias\", \"text_model.encoder.layer.6.attention.self.query.weight\", \"text_model.encoder.layer.6.attention.self.query.bias\", \"text_model.encoder.layer.6.attention.self.key.weight\", \"text_model.encoder.layer.6.attention.self.key.bias\", \"text_model.encoder.layer.6.attention.self.value.weight\", \"text_model.encoder.layer.6.attention.self.value.bias\", \"text_model.encoder.layer.6.attention.output.dense.weight\", \"text_model.encoder.layer.6.attention.output.dense.bias\", \"text_model.encoder.layer.6.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.6.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.6.intermediate.dense.weight\", \"text_model.encoder.layer.6.intermediate.dense.bias\", \"text_model.encoder.layer.6.output.dense.weight\", \"text_model.encoder.layer.6.output.dense.bias\", \"text_model.encoder.layer.6.output.LayerNorm.weight\", \"text_model.encoder.layer.6.output.LayerNorm.bias\", \"text_model.encoder.layer.7.attention.self.query.weight\", \"text_model.encoder.layer.7.attention.self.query.bias\", \"text_model.encoder.layer.7.attention.self.key.weight\", \"text_model.encoder.layer.7.attention.self.key.bias\", \"text_model.encoder.layer.7.attention.self.value.weight\", \"text_model.encoder.layer.7.attention.self.value.bias\", \"text_model.encoder.layer.7.attention.output.dense.weight\", \"text_model.encoder.layer.7.attention.output.dense.bias\", \"text_model.encoder.layer.7.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.7.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.7.intermediate.dense.weight\", \"text_model.encoder.layer.7.intermediate.dense.bias\", \"text_model.encoder.layer.7.output.dense.weight\", \"text_model.encoder.layer.7.output.dense.bias\", \"text_model.encoder.layer.7.output.LayerNorm.weight\", \"text_model.encoder.layer.7.output.LayerNorm.bias\", \"text_model.encoder.layer.8.attention.self.query.weight\", \"text_model.encoder.layer.8.attention.self.query.bias\", \"text_model.encoder.layer.8.attention.self.key.weight\", \"text_model.encoder.layer.8.attention.self.key.bias\", \"text_model.encoder.layer.8.attention.self.value.weight\", \"text_model.encoder.layer.8.attention.self.value.bias\", \"text_model.encoder.layer.8.attention.output.dense.weight\", \"text_model.encoder.layer.8.attention.output.dense.bias\", \"text_model.encoder.layer.8.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.8.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.8.intermediate.dense.weight\", \"text_model.encoder.layer.8.intermediate.dense.bias\", \"text_model.encoder.layer.8.output.dense.weight\", \"text_model.encoder.layer.8.output.dense.bias\", \"text_model.encoder.layer.8.output.LayerNorm.weight\", \"text_model.encoder.layer.8.output.LayerNorm.bias\", \"text_model.encoder.layer.9.attention.self.query.weight\", \"text_model.encoder.layer.9.attention.self.query.bias\", \"text_model.encoder.layer.9.attention.self.key.weight\", \"text_model.encoder.layer.9.attention.self.key.bias\", \"text_model.encoder.layer.9.attention.self.value.weight\", \"text_model.encoder.layer.9.attention.self.value.bias\", \"text_model.encoder.layer.9.attention.output.dense.weight\", \"text_model.encoder.layer.9.attention.output.dense.bias\", \"text_model.encoder.layer.9.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.9.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.9.intermediate.dense.weight\", \"text_model.encoder.layer.9.intermediate.dense.bias\", \"text_model.encoder.layer.9.output.dense.weight\", \"text_model.encoder.layer.9.output.dense.bias\", \"text_model.encoder.layer.9.output.LayerNorm.weight\", \"text_model.encoder.layer.9.output.LayerNorm.bias\", \"text_model.encoder.layer.10.attention.self.query.weight\", \"text_model.encoder.layer.10.attention.self.query.bias\", \"text_model.encoder.layer.10.attention.self.key.weight\", \"text_model.encoder.layer.10.attention.self.key.bias\", \"text_model.encoder.layer.10.attention.self.value.weight\", \"text_model.encoder.layer.10.attention.self.value.bias\", \"text_model.encoder.layer.10.attention.output.dense.weight\", \"text_model.encoder.layer.10.attention.output.dense.bias\", \"text_model.encoder.layer.10.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.10.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.10.intermediate.dense.weight\", \"text_model.encoder.layer.10.intermediate.dense.bias\", \"text_model.encoder.layer.10.output.dense.weight\", \"text_model.encoder.layer.10.output.dense.bias\", \"text_model.encoder.layer.10.output.LayerNorm.weight\", \"text_model.encoder.layer.10.output.LayerNorm.bias\", \"text_model.encoder.layer.11.attention.self.query.weight\", \"text_model.encoder.layer.11.attention.self.query.bias\", \"text_model.encoder.layer.11.attention.self.key.weight\", \"text_model.encoder.layer.11.attention.self.key.bias\", \"text_model.encoder.layer.11.attention.self.value.weight\", \"text_model.encoder.layer.11.attention.self.value.bias\", \"text_model.encoder.layer.11.attention.output.dense.weight\", \"text_model.encoder.layer.11.attention.output.dense.bias\", \"text_model.encoder.layer.11.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.11.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.11.intermediate.dense.weight\", \"text_model.encoder.layer.11.intermediate.dense.bias\", \"text_model.encoder.layer.11.output.dense.weight\", \"text_model.encoder.layer.11.output.dense.bias\", \"text_model.encoder.layer.11.output.LayerNorm.weight\", \"text_model.encoder.layer.11.output.LayerNorm.bias\", \"text_model.pooler.dense.weight\", \"text_model.pooler.dense.bias\", \"visual_projection.weight\". ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-65dd37a4de05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RN50\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#The model variable will hold the loaded model, while the preprocess variable will store the preprocessing function for preparing the input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/xz306/Data-Climate-and-AI/src/fine_tuning/try/clip-roberta-finetuned/pytorch_model.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0minput_resolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_resolution\u001b[0m \u001b[0;31m# (size) of the input images expected by the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2042\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CLIP:\n\tMissing key(s) in state_dict: \"positional_embedding\", \"text_projection\", \"visual.conv1.weight\", \"visual.bn1.weight\", \"visual.bn1.bias\", \"visual.bn1.running_mean\", \"visual.bn1.running_var\", \"visual.conv2.weight\", \"visual.bn2.weight\", \"visual.bn2.bias\", \"visual.bn2.running_mean\", \"visual.bn2.running_var\", \"visual.conv3.weight\", \"visual.bn3.weight\", \"visual.bn3.bias\", \"visual.bn3.running_mean\", \"visual.bn3.running_var\", \"visual.layer1.0.conv1.weight\", \"visual.layer1.0.bn1.weight\", \"visual.layer1.0.bn1.bias\", \"visual.layer1.0.bn1.running_mean\", \"visual.layer1.0.bn1.running_var\", \"visual.layer1.0.conv2.weight\", \"visual.layer1.0.bn2.weight\", \"visual.layer1.0.bn2.bias\", \"visual.layer1.0.bn2.running_mean\", \"visual.layer1.0.bn2.running_var\", \"visual.layer1.0.conv3.weight\", \"visual.layer1.0.bn3.weight\", \"visual.layer1.0.bn3.bias\", \"visual.layer1.0.bn3.running_mean\", \"visual.layer1.0.bn3.running_var\", \"visual.layer1.0.downsample.0.weight\", \"visual.layer1.0.downsample.1.weight\", \"visual.layer1.0.downsample.1.bias\", \"visual.layer1.0.downsample.1.running_mean\", \"visual.layer1.0.downsample.1.running_var\", \"visual.layer1.1.conv1.weight\", \"visual.layer1.1.bn1.weight\", \"visual.layer1.1.bn1.bias\", \"visual.layer1.1.bn1.running_mean\", \"visual.layer1.1.bn1.running_var\", \"visual.layer1.1.conv2.weight\", \"visual.layer1.1.bn2.weight\", \"visual.layer1.1.bn2.bias\", \"visual.layer1.1.bn2.running_mean\", \"visual.layer1.1.bn2.running_var\", \"visual.layer1.1.conv3.weight\", \"visual.layer1.1.bn3.weight\", \"visual.layer1.1.bn3.bias\", \"visual.layer1.1.bn3.running_mean\", \"visual.layer1.1.bn3.running_var\", \"visual.layer1.2.conv1.weight\", \"visual.layer1.2.bn1.weight\", \"visual.layer1.2.bn1.bias\", \"visual.layer1.2.bn1.running_mean\", \"visual.layer1.2.bn1.running_var\", \"visual.layer1.2.conv2.weight\", \"visual.layer1.2.bn2.weight\", \"visual.layer1.2.bn2.bias\", \"visual.layer1.2.bn2.running_mean\", \"visual.layer1.2.bn2.running_var\", \"visual.layer1.2.conv3.weight\", \"visual.layer1.2.bn3.weight\", \"visual.layer1.2.bn3.bias\", \"visual.layer1.2.bn3.running_mean\", \"visual.layer1.2.bn3.running_var\", \"visual.layer2.0.conv1.weight\", \"visual.layer2.0.bn1.weight\", \"visual.layer2.0.bn1.bias\", \"visual.layer2.0.bn1.running_mean\", \"visual.layer2.0.bn1.running_var\", \"visual.layer2.0.conv2.weight\", \"visual.layer2.0.bn2.weight\", \"visual.layer2.0.bn2.bias\", \"visual.layer2.0.bn2.running_mean\", \"visual.layer2.0.bn2.running_var\", \"visual.layer2.0.conv3.weight\", \"visual.layer2.0.bn3.weight\", \"visual.layer2.0.bn3.bias\", \"visual.layer2.0.bn3.running_mean\", \"visual.layer2.0.bn3.running_var\", \"visual.layer2.0.downsample.0.weight\", \"visual.layer2.0.downsample.1.weight\", \"visual.layer2.0.downsample.1.bias\", \"visual.layer2.0.downsample.1.running_mean\", \"visual.layer2.0.downsample.1.running_var\", \"visual.layer2.1.conv1.weight\", \"visual.layer2.1.bn1.weight\", \"visual.layer2.1.bn1.bias\", \"visual.layer2.1.bn1.running_mean\", \"visual.layer2.1.bn1.running_var\", \"visual.layer2.1.conv2.weight\", \"visual.layer2.1.bn2.weight\", \"visual.layer2.1.bn2.bias\", \"visual.layer2.1.bn2.running_mean\", \"visual.layer2.1.bn2.running_var\", \"visual.layer2.1.conv3.weight\", \"visual.layer2.1.bn3.weight\", \"visual.layer2.1.bn3.bias\", \"visual.layer2.1.bn3.running_mean\", \"visual.layer2.1.bn3.running_var\", \"visual.layer2.2.conv1.weight\", \"visual.layer2.2.bn1.weight\", \"visual.layer2.2.bn1.bias\", \"visual.layer2.2.bn1.running_mean\", \"visual.layer2.2.bn1.running_var\", \"visual.layer2.2.conv2.weight\", \"visual.layer2.2.bn2.weight\", \"visual.layer2.2.bn2.bias\", \"visual.layer2.2.bn2.running_mean\", \"visual.layer2.2.bn2.running_var\", \"visual.layer2.2.conv3.weight\", \"visual.layer2.2.bn3.weight\", \"visual.layer2.2.bn3.bias\", \"visual.layer2.2.bn3.running_mean\", \"visual.layer2.2.bn3.running_var\", \"visual.layer2.3.conv1.weight\", \"visual.layer2.3.bn1.weight\", \"visual.layer2.3.bn1.bias\", \"visual.layer2.3.bn1.running_mean\", \"visual.layer2.3.bn1.running_var\", \"visual.layer2.3.conv2.weight\", \"visual.layer2.3.bn2.weight\", \"visual.layer2.3.bn2.bias\", \"visual.layer2.3.bn2.running_mean\", \"visual.layer2.3.bn2.running_var\", \"visual.layer2.3.conv3.weight\", \"visual.layer2.3.bn3.weight\", \"visual.layer2.3.bn3.bias\", \"visual.layer2.3.bn3.running_mean\", \"visual.layer2.3.bn3.running_var\", \"visual.layer3.0.conv1.weight\", \"visual.layer3.0.bn1.weight\", \"visual.layer3.0.bn1.bias\", \"visual.layer3.0.bn1.running_mean\", \"visual.layer3.0.bn1.running_var\", \"visual.layer3.0.conv2.weight\", \"visual.layer3.0.bn2.weight\", \"visual.layer3.0.bn2.bias\", \"visual.layer3.0.bn2.running_mean\", \"visual.layer3.0.bn2.running_var\", \"visual.layer3.0.conv3.weight\", \"visual.layer3.0.bn3.weight\", \"visual.layer3.0.bn3.bias\", \"visual.layer3.0.bn3.running_mean\", \"visual.layer3.0.bn3.running_var\", \"visual.layer3.0.downsample.0.weight\", \"visual.layer3.0.downsample.1.weight\", \"visual.layer3.0.downsample.1.bias\", \"visual.layer3.0.downsample.1.running_mean\", \"visual.layer3.0.downsample.1.running_var\", \"visual.layer3.1.conv1.weight\", \"visual.layer3.1.bn1.weight\", \"visual.layer3.1.bn1.bias\", \"visual.layer3.1.bn1.running_mean\", \"visual.layer3.1.bn1.running_var\", \"visual.layer3.1.conv2.weight\", \"visual.layer3.1.bn2.weight\", \"visual.layer3.1.bn2.bias\", \"visual.layer3.1.bn2.running_mean\", \"visual.layer3.1.bn2.running_var\", \"visual.layer3.1.conv3.weight\", \"visual.layer3.1.bn3.weight\", \"visual.layer3.1.bn3.bias\", \"visual.layer3.1.bn3.running_mean\", \"visual.layer3.1.bn3.running_var\", \"visual.layer3.2.conv1.weight\", \"visual.layer3.2.bn1.weight\", \"visual.layer3.2.bn1.bias\", \"visual.layer3.2.bn1.running_mean\", \"visual.layer3.2.bn1.running_var\", \"visual.layer3.2.conv2.weight\", \"visual.layer3.2.bn2.weight\", \"visual.layer3.2.bn2.bias\", \"visual.layer3.2.bn2.running_mean\", \"visual.layer3.2.bn2.running_var\", \"visual.layer3.2.conv3.weight\", \"visual.layer3.2.bn3.weight\", \"visual.layer3.2.bn3.bias\", \"visual.layer3.2.bn3.running_mean\", \"visual.layer3.2.bn3.running_var\", \"visual.layer3.3.conv1.weight\", \"visual.layer3.3.bn1.weight\", \"visual.layer3.3.bn1.bias\", \"visual.layer3.3.bn1.running_mean\", \"visual.layer3.3.bn1.running_var\", \"visual.layer3.3.conv2.weight\", \"visual.layer3.3.bn2.weight\", \"visual.layer3.3.bn2.bias\", \"visual.layer3.3.bn2.running_mean\", \"visual.layer3.3.bn2.running_var\", \"visual.layer3.3.conv3.weight\", \"visual.layer3.3.bn3.weight\", \"visual.layer3.3.bn3.bias\", \"visual.layer3.3.bn3.running_mean\", \"visual.layer3.3.bn3.running_var\", \"visual.layer3.4.conv1.weight\", \"visual.layer3.4.bn1.weight\", \"visual.layer3.4.bn1.bias\", \"visual.layer3.4.bn1.running_mean\", \"visual.layer3.4.bn1.running_var\", \"visual.layer3.4.conv2.weight\", \"visual.layer3.4.bn2.weight\", \"visual.layer3.4.bn2.bias\", \"visual.layer3.4.bn2.running_mean\", \"visual.layer3.4.bn2.running_var\", \"visual.layer3.4.conv3.weight\", \"visual.layer3.4.bn3.weight\", \"visual.layer3.4.bn3.bias\", \"visual.layer3.4.bn3.running_mean\", \"visual.layer3.4.bn3.running_var\", \"visual.layer3.5.conv1.weight\", \"visual.layer3.5.bn1.weight\", \"visual.layer3.5.bn1.bias\", \"visual.layer3.5.bn1.running_mean\", \"visual.layer3.5.bn1.running_var\", \"visual.layer3.5.conv2.weight\", \"visual.layer3.5.bn2.weight\", \"visual.layer3.5.bn2.bias\", \"visual.layer3.5.bn2.running_mean\", \"visual.layer3.5.bn2.running_var\", \"visual.layer3.5.conv3.weight\", \"visual.layer3.5.bn3.weight\", \"visual.layer3.5.bn3.bias\", \"visual.layer3.5.bn3.running_mean\", \"visual.layer3.5.bn3.running_var\", \"visual.layer4.0.conv1.weight\", \"visual.layer4.0.bn1.weight\", \"visual.layer4.0.bn1.bias\", \"visual.layer4.0.bn1.running_mean\", \"visual.layer4.0.bn1.running_var\", \"visual.layer4.0.conv2.weight\", \"visual.layer4.0.bn2.weight\", \"visual.layer4.0.bn2.bias\", \"visual.layer4.0.bn2.running_mean\", \"visual.layer4.0.bn2.running_var\", \"visual.layer4.0.conv3.weight\", \"visual.layer4.0.bn3.weight\", \"visual.layer4.0.bn3.bias\", \"visual.layer4.0.bn3.running_mean\", \"visual.layer4.0.bn3.running_var\", \"visual.layer4.0.downsample.0.weight\", \"visual.layer4.0.downsample.1.weight\", \"visual.layer4.0.downsample.1.bias\", \"visual.layer4.0.downsample.1.running_mean\", \"visual.layer4.0.downsample.1.running_var\", \"visual.layer4.1.conv1.weight\", \"visual.layer4.1.bn1.weight\", \"visual.layer4.1.bn1.bias\", \"visual.layer4.1.bn1.running_mean\", \"visual.layer4.1.bn1.running_var\", \"visual.layer4.1.conv2.weight\", \"visual.layer4.1.bn2.weight\", \"visual.layer4.1.bn2.bias\", \"visual.layer4.1.bn2.running_mean\", \"visual.layer4.1.bn2.running_var\", \"visual.layer4.1.conv3.weight\", \"visual.layer4.1.bn3.weight\", \"visual.layer4.1.bn3.bias\", \"visual.layer4.1.bn3.running_mean\", \"visual.layer4.1.bn3.running_var\", \"visual.layer4.2.conv1.weight\", \"visual.layer4.2.bn1.weight\", \"visual.layer4.2.bn1.bias\", \"visual.layer4.2.bn1.running_mean\", \"visual.layer4.2.bn1.running_var\", \"visual.layer4.2.conv2.weight\", \"visual.layer4.2.bn2.weight\", \"visual.layer4.2.bn2.bias\", \"visual.layer4.2.bn2.running_mean\", \"visual.layer4.2.bn2.running_var\", \"visual.layer4.2.conv3.weight\", \"visual.layer4.2.bn3.weight\", \"visual.layer4.2.bn3.bias\", \"visual.layer4.2.bn3.running_mean\", \"visual.layer4.2.bn3.running_var\", \"visual.attnpool.positional_embedding\", \"visual.attnpool.k_proj.weight\", \"visual.attnpool.k_proj.bias\", \"visual.attnpool.q_proj.weight\", \"visual.attnpool.q_proj.bias\", \"visual.attnpool.v_proj.weight\", \"visual.attnpool.v_proj.bias\", \"visual.attnpool.c_proj.weight\", \"visual.attnpool.c_proj.bias\", \"transformer.resblocks.0.attn.in_proj_weight\", \"transformer.resblocks.0.attn.in_proj_bias\", \"transformer.resblocks.0.attn.out_proj.weight\", \"transformer.resblocks.0.attn.out_proj.bias\", \"transformer.resblocks.0.ln_1.weight\", \"transformer.resblocks.0.ln_1.bias\", \"transformer.resblocks.0.mlp.c_fc.weight\", \"transformer.resblocks.0.mlp.c_fc.bias\", \"transformer.resblocks.0.mlp.c_proj.weight\", \"transformer.resblocks.0.mlp.c_proj.bias\", \"transformer.resblocks.0.ln_2.weight\", \"transformer.resblocks.0.ln_2.bias\", \"transformer.resblocks.1.attn.in_proj_weight\", \"transformer.resblocks.1.attn.in_proj_bias\", \"transformer.resblocks.1.attn.out_proj.weight\", \"transformer.resblocks.1.attn.out_proj.bias\", \"transformer.resblocks.1.ln_1.weight\", \"transformer.resblocks.1.ln_1.bias\", \"transformer.resblocks.1.mlp.c_fc.weight\", \"transformer.resblocks.1.mlp.c_fc.bias\", \"transformer.resblocks.1.mlp.c_proj.weight\", \"transformer.resblocks.1.mlp.c_proj.bias\", \"transformer.resblocks.1.ln_2.weight\", \"transformer.resblocks.1.ln_2.bias\", \"transformer.resblocks.2.attn.in_proj_weight\", \"transformer.resblocks.2.attn.in_proj_bias\", \"transformer.resblocks.2.attn.out_proj.weight\", \"transformer.resblocks.2.attn.out_proj.bias\", \"transformer.resblocks.2.ln_1.weight\", \"transformer.resblocks.2.ln_1.bias\", \"transformer.resblocks.2.mlp.c_fc.weight\", \"transformer.resblocks.2.mlp.c_fc.bias\", \"transformer.resblocks.2.mlp.c_proj.weight\", \"transformer.resblocks.2.mlp.c_proj.bias\", \"transformer.resblocks.2.ln_2.weight\", \"transformer.resblocks.2.ln_2.bias\", \"transformer.resblocks.3.attn.in_proj_weight\", \"transformer.resblocks.3.attn.in_proj_bias\", \"transformer.resblocks.3.attn.out_proj.weight\", \"transformer.resblocks.3.attn.out_proj.bias\", \"transformer.resblocks.3.ln_1.weight\", \"transformer.resblocks.3.ln_1.bias\", \"transformer.resblocks.3.mlp.c_fc.weight\", \"transformer.resblocks.3.mlp.c_fc.bias\", \"transformer.resblocks.3.mlp.c_proj.weight\", \"transformer.resblocks.3.mlp.c_proj.bias\", \"transformer.resblocks.3.ln_2.weight\", \"transformer.resblocks.3.ln_2.bias\", \"transformer.resblocks.4.attn.in_proj_weight\", \"transformer.resblocks.4.attn.in_proj_bias\", \"transformer.resblocks.4.attn.out_proj.weight\", \"transformer.resblocks.4.attn.out_proj.bias\", \"transformer.resblocks.4.ln_1.weight\", \"transformer.resblocks.4.ln_1.bias\", \"transformer.resblocks.4.mlp.c_fc.weight\", \"transformer.resblocks.4.mlp.c_fc.bias\", \"transformer.resblocks.4.mlp.c_proj.weight\", \"transformer.resblocks.4.mlp.c_proj.bias\", \"transformer.resblocks.4.ln_2.weight\", \"transformer.resblocks.4.ln_2.bias\", \"transformer.resblocks.5.attn.in_proj_weight\", \"transformer.resblocks.5.attn.in_proj_bias\", \"transformer.resblocks.5.attn.out_proj.weight\", \"transformer.resblocks.5.attn.out_proj.bias\", \"transformer.resblocks.5.ln_1.weight\", \"transformer.resblocks.5.ln_1.bias\", \"transformer.resblocks.5.mlp.c_fc.weight\", \"transformer.resblocks.5.mlp.c_fc.bias\", \"transformer.resblocks.5.mlp.c_proj.weight\", \"transformer.resblocks.5.mlp.c_proj.bias\", \"transformer.resblocks.5.ln_2.weight\", \"transformer.resblocks.5.ln_2.bias\", \"transformer.resblocks.6.attn.in_proj_weight\", \"transformer.resblocks.6.attn.in_proj_bias\", \"transformer.resblocks.6.attn.out_proj.weight\", \"transformer.resblocks.6.attn.out_proj.bias\", \"transformer.resblocks.6.ln_1.weight\", \"transformer.resblocks.6.ln_1.bias\", \"transformer.resblocks.6.mlp.c_fc.weight\", \"transformer.resblocks.6.mlp.c_fc.bias\", \"transformer.resblocks.6.mlp.c_proj.weight\", \"transformer.resblocks.6.mlp.c_proj.bias\", \"transformer.resblocks.6.ln_2.weight\", \"transformer.resblocks.6.ln_2.bias\", \"transformer.resblocks.7.attn.in_proj_weight\", \"transformer.resblocks.7.attn.in_proj_bias\", \"transformer.resblocks.7.attn.out_proj.weight\", \"transformer.resblocks.7.attn.out_proj.bias\", \"transformer.resblocks.7.ln_1.weight\", \"transformer.resblocks.7.ln_1.bias\", \"transformer.resblocks.7.mlp.c_fc.weight\", \"transformer.resblocks.7.mlp.c_fc.bias\", \"transformer.resblocks.7.mlp.c_proj.weight\", \"transformer.resblocks.7.mlp.c_proj.bias\", \"transformer.resblocks.7.ln_2.weight\", \"transformer.resblocks.7.ln_2.bias\", \"transformer.resblocks.8.attn.in_proj_weight\", \"transformer.resblocks.8.attn.in_proj_bias\", \"transformer.resblocks.8.attn.out_proj.weight\", \"transformer.resblocks.8.attn.out_proj.bias\", \"transformer.resblocks.8.ln_1.weight\", \"transformer.resblocks.8.ln_1.bias\", \"transformer.resblocks.8.mlp.c_fc.weight\", \"transformer.resblocks.8.mlp.c_fc.bias\", \"transformer.resblocks.8.mlp.c_proj.weight\", \"transformer.resblocks.8.mlp.c_proj.bias\", \"transformer.resblocks.8.ln_2.weight\", \"transformer.resblocks.8.ln_2.bias\", \"transformer.resblocks.9.attn.in_proj_weight\", \"transformer.resblocks.9.attn.in_proj_bias\", \"transformer.resblocks.9.attn.out_proj.weight\", \"transformer.resblocks.9.attn.out_proj.bias\", \"transformer.resblocks.9.ln_1.weight\", \"transformer.resblocks.9.ln_1.bias\", \"transformer.resblocks.9.mlp.c_fc.weight\", \"transformer.resblocks.9.mlp.c_fc.bias\", \"transformer.resblocks.9.mlp.c_proj.weight\", \"transformer.resblocks.9.mlp.c_proj.bias\", \"transformer.resblocks.9.ln_2.weight\", \"transformer.resblocks.9.ln_2.bias\", \"transformer.resblocks.10.attn.in_proj_weight\", \"transformer.resblocks.10.attn.in_proj_bias\", \"transformer.resblocks.10.attn.out_proj.weight\", \"transformer.resblocks.10.attn.out_proj.bias\", \"transformer.resblocks.10.ln_1.weight\", \"transformer.resblocks.10.ln_1.bias\", \"transformer.resblocks.10.mlp.c_fc.weight\", \"transformer.resblocks.10.mlp.c_fc.bias\", \"transformer.resblocks.10.mlp.c_proj.weight\", \"transformer.resblocks.10.mlp.c_proj.bias\", \"transformer.resblocks.10.ln_2.weight\", \"transformer.resblocks.10.ln_2.bias\", \"transformer.resblocks.11.attn.in_proj_weight\", \"transformer.resblocks.11.attn.in_proj_bias\", \"transformer.resblocks.11.attn.out_proj.weight\", \"transformer.resblocks.11.attn.out_proj.bias\", \"transformer.resblocks.11.ln_1.weight\", \"transformer.resblocks.11.ln_1.bias\", \"transformer.resblocks.11.mlp.c_fc.weight\", \"transformer.resblocks.11.mlp.c_fc.bias\", \"transformer.resblocks.11.mlp.c_proj.weight\", \"transformer.resblocks.11.mlp.c_proj.bias\", \"transformer.resblocks.11.ln_2.weight\", \"transformer.resblocks.11.ln_2.bias\", \"token_embedding.weight\", \"ln_final.weight\", \"ln_final.bias\". \n\tUnexpected key(s) in state_dict: \"vision_model.vision_model.embeddings.class_embedding\", \"vision_model.vision_model.embeddings.patch_embedding.weight\", \"vision_model.vision_model.embeddings.position_embedding.weight\", \"vision_model.vision_model.pre_layrnorm.weight\", \"vision_model.vision_model.pre_layrnorm.bias\", \"vision_model.vision_model.encoder.layers.0.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.0.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.0.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.0.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.0.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.0.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.0.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.0.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.0.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.0.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.0.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.0.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.0.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.0.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.0.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.0.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.1.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.1.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.1.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.1.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.1.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.1.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.1.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.1.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.1.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.1.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.1.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.1.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.1.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.1.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.1.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.1.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.2.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.2.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.2.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.2.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.2.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.2.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.2.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.2.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.2.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.2.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.2.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.2.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.2.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.2.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.2.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.2.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.3.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.3.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.3.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.3.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.3.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.3.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.3.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.3.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.3.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.3.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.3.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.3.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.3.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.3.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.3.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.3.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.4.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.4.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.4.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.4.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.4.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.4.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.4.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.4.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.4.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.4.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.4.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.4.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.4.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.4.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.4.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.4.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.5.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.5.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.5.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.5.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.5.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.5.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.5.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.5.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.5.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.5.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.5.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.5.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.5.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.5.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.5.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.5.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.6.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.6.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.6.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.6.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.6.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.6.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.6.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.6.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.6.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.6.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.6.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.6.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.6.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.6.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.6.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.6.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.7.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.7.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.7.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.7.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.7.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.7.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.7.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.7.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.7.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.7.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.7.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.7.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.7.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.7.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.7.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.7.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.8.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.8.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.8.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.8.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.8.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.8.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.8.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.8.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.8.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.8.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.8.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.8.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.8.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.8.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.8.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.8.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.9.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.9.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.9.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.9.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.9.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.9.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.9.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.9.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.9.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.9.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.9.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.9.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.9.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.9.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.9.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.9.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.10.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.10.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.10.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.10.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.10.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.10.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.10.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.10.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.10.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.10.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.10.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.10.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.10.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.10.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.10.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.10.layer_norm2.bias\", \"vision_model.vision_model.encoder.layers.11.self_attn.k_proj.weight\", \"vision_model.vision_model.encoder.layers.11.self_attn.k_proj.bias\", \"vision_model.vision_model.encoder.layers.11.self_attn.v_proj.weight\", \"vision_model.vision_model.encoder.layers.11.self_attn.v_proj.bias\", \"vision_model.vision_model.encoder.layers.11.self_attn.q_proj.weight\", \"vision_model.vision_model.encoder.layers.11.self_attn.q_proj.bias\", \"vision_model.vision_model.encoder.layers.11.self_attn.out_proj.weight\", \"vision_model.vision_model.encoder.layers.11.self_attn.out_proj.bias\", \"vision_model.vision_model.encoder.layers.11.layer_norm1.weight\", \"vision_model.vision_model.encoder.layers.11.layer_norm1.bias\", \"vision_model.vision_model.encoder.layers.11.mlp.fc1.weight\", \"vision_model.vision_model.encoder.layers.11.mlp.fc1.bias\", \"vision_model.vision_model.encoder.layers.11.mlp.fc2.weight\", \"vision_model.vision_model.encoder.layers.11.mlp.fc2.bias\", \"vision_model.vision_model.encoder.layers.11.layer_norm2.weight\", \"vision_model.vision_model.encoder.layers.11.layer_norm2.bias\", \"vision_model.vision_model.post_layernorm.weight\", \"vision_model.vision_model.post_layernorm.bias\", \"text_model.embeddings.word_embeddings.weight\", \"text_model.embeddings.position_embeddings.weight\", \"text_model.embeddings.token_type_embeddings.weight\", \"text_model.embeddings.LayerNorm.weight\", \"text_model.embeddings.LayerNorm.bias\", \"text_model.encoder.layer.0.attention.self.query.weight\", \"text_model.encoder.layer.0.attention.self.query.bias\", \"text_model.encoder.layer.0.attention.self.key.weight\", \"text_model.encoder.layer.0.attention.self.key.bias\", \"text_model.encoder.layer.0.attention.self.value.weight\", \"text_model.encoder.layer.0.attention.self.value.bias\", \"text_model.encoder.layer.0.attention.output.dense.weight\", \"text_model.encoder.layer.0.attention.output.dense.bias\", \"text_model.encoder.layer.0.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.0.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.0.intermediate.dense.weight\", \"text_model.encoder.layer.0.intermediate.dense.bias\", \"text_model.encoder.layer.0.output.dense.weight\", \"text_model.encoder.layer.0.output.dense.bias\", \"text_model.encoder.layer.0.output.LayerNorm.weight\", \"text_model.encoder.layer.0.output.LayerNorm.bias\", \"text_model.encoder.layer.1.attention.self.query.weight\", \"text_model.encoder.layer.1.attention.self.query.bias\", \"text_model.encoder.layer.1.attention.self.key.weight\", \"text_model.encoder.layer.1.attention.self.key.bias\", \"text_model.encoder.layer.1.attention.self.value.weight\", \"text_model.encoder.layer.1.attention.self.value.bias\", \"text_model.encoder.layer.1.attention.output.dense.weight\", \"text_model.encoder.layer.1.attention.output.dense.bias\", \"text_model.encoder.layer.1.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.1.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.1.intermediate.dense.weight\", \"text_model.encoder.layer.1.intermediate.dense.bias\", \"text_model.encoder.layer.1.output.dense.weight\", \"text_model.encoder.layer.1.output.dense.bias\", \"text_model.encoder.layer.1.output.LayerNorm.weight\", \"text_model.encoder.layer.1.output.LayerNorm.bias\", \"text_model.encoder.layer.2.attention.self.query.weight\", \"text_model.encoder.layer.2.attention.self.query.bias\", \"text_model.encoder.layer.2.attention.self.key.weight\", \"text_model.encoder.layer.2.attention.self.key.bias\", \"text_model.encoder.layer.2.attention.self.value.weight\", \"text_model.encoder.layer.2.attention.self.value.bias\", \"text_model.encoder.layer.2.attention.output.dense.weight\", \"text_model.encoder.layer.2.attention.output.dense.bias\", \"text_model.encoder.layer.2.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.2.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.2.intermediate.dense.weight\", \"text_model.encoder.layer.2.intermediate.dense.bias\", \"text_model.encoder.layer.2.output.dense.weight\", \"text_model.encoder.layer.2.output.dense.bias\", \"text_model.encoder.layer.2.output.LayerNorm.weight\", \"text_model.encoder.layer.2.output.LayerNorm.bias\", \"text_model.encoder.layer.3.attention.self.query.weight\", \"text_model.encoder.layer.3.attention.self.query.bias\", \"text_model.encoder.layer.3.attention.self.key.weight\", \"text_model.encoder.layer.3.attention.self.key.bias\", \"text_model.encoder.layer.3.attention.self.value.weight\", \"text_model.encoder.layer.3.attention.self.value.bias\", \"text_model.encoder.layer.3.attention.output.dense.weight\", \"text_model.encoder.layer.3.attention.output.dense.bias\", \"text_model.encoder.layer.3.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.3.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.3.intermediate.dense.weight\", \"text_model.encoder.layer.3.intermediate.dense.bias\", \"text_model.encoder.layer.3.output.dense.weight\", \"text_model.encoder.layer.3.output.dense.bias\", \"text_model.encoder.layer.3.output.LayerNorm.weight\", \"text_model.encoder.layer.3.output.LayerNorm.bias\", \"text_model.encoder.layer.4.attention.self.query.weight\", \"text_model.encoder.layer.4.attention.self.query.bias\", \"text_model.encoder.layer.4.attention.self.key.weight\", \"text_model.encoder.layer.4.attention.self.key.bias\", \"text_model.encoder.layer.4.attention.self.value.weight\", \"text_model.encoder.layer.4.attention.self.value.bias\", \"text_model.encoder.layer.4.attention.output.dense.weight\", \"text_model.encoder.layer.4.attention.output.dense.bias\", \"text_model.encoder.layer.4.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.4.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.4.intermediate.dense.weight\", \"text_model.encoder.layer.4.intermediate.dense.bias\", \"text_model.encoder.layer.4.output.dense.weight\", \"text_model.encoder.layer.4.output.dense.bias\", \"text_model.encoder.layer.4.output.LayerNorm.weight\", \"text_model.encoder.layer.4.output.LayerNorm.bias\", \"text_model.encoder.layer.5.attention.self.query.weight\", \"text_model.encoder.layer.5.attention.self.query.bias\", \"text_model.encoder.layer.5.attention.self.key.weight\", \"text_model.encoder.layer.5.attention.self.key.bias\", \"text_model.encoder.layer.5.attention.self.value.weight\", \"text_model.encoder.layer.5.attention.self.value.bias\", \"text_model.encoder.layer.5.attention.output.dense.weight\", \"text_model.encoder.layer.5.attention.output.dense.bias\", \"text_model.encoder.layer.5.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.5.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.5.intermediate.dense.weight\", \"text_model.encoder.layer.5.intermediate.dense.bias\", \"text_model.encoder.layer.5.output.dense.weight\", \"text_model.encoder.layer.5.output.dense.bias\", \"text_model.encoder.layer.5.output.LayerNorm.weight\", \"text_model.encoder.layer.5.output.LayerNorm.bias\", \"text_model.encoder.layer.6.attention.self.query.weight\", \"text_model.encoder.layer.6.attention.self.query.bias\", \"text_model.encoder.layer.6.attention.self.key.weight\", \"text_model.encoder.layer.6.attention.self.key.bias\", \"text_model.encoder.layer.6.attention.self.value.weight\", \"text_model.encoder.layer.6.attention.self.value.bias\", \"text_model.encoder.layer.6.attention.output.dense.weight\", \"text_model.encoder.layer.6.attention.output.dense.bias\", \"text_model.encoder.layer.6.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.6.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.6.intermediate.dense.weight\", \"text_model.encoder.layer.6.intermediate.dense.bias\", \"text_model.encoder.layer.6.output.dense.weight\", \"text_model.encoder.layer.6.output.dense.bias\", \"text_model.encoder.layer.6.output.LayerNorm.weight\", \"text_model.encoder.layer.6.output.LayerNorm.bias\", \"text_model.encoder.layer.7.attention.self.query.weight\", \"text_model.encoder.layer.7.attention.self.query.bias\", \"text_model.encoder.layer.7.attention.self.key.weight\", \"text_model.encoder.layer.7.attention.self.key.bias\", \"text_model.encoder.layer.7.attention.self.value.weight\", \"text_model.encoder.layer.7.attention.self.value.bias\", \"text_model.encoder.layer.7.attention.output.dense.weight\", \"text_model.encoder.layer.7.attention.output.dense.bias\", \"text_model.encoder.layer.7.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.7.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.7.intermediate.dense.weight\", \"text_model.encoder.layer.7.intermediate.dense.bias\", \"text_model.encoder.layer.7.output.dense.weight\", \"text_model.encoder.layer.7.output.dense.bias\", \"text_model.encoder.layer.7.output.LayerNorm.weight\", \"text_model.encoder.layer.7.output.LayerNorm.bias\", \"text_model.encoder.layer.8.attention.self.query.weight\", \"text_model.encoder.layer.8.attention.self.query.bias\", \"text_model.encoder.layer.8.attention.self.key.weight\", \"text_model.encoder.layer.8.attention.self.key.bias\", \"text_model.encoder.layer.8.attention.self.value.weight\", \"text_model.encoder.layer.8.attention.self.value.bias\", \"text_model.encoder.layer.8.attention.output.dense.weight\", \"text_model.encoder.layer.8.attention.output.dense.bias\", \"text_model.encoder.layer.8.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.8.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.8.intermediate.dense.weight\", \"text_model.encoder.layer.8.intermediate.dense.bias\", \"text_model.encoder.layer.8.output.dense.weight\", \"text_model.encoder.layer.8.output.dense.bias\", \"text_model.encoder.layer.8.output.LayerNorm.weight\", \"text_model.encoder.layer.8.output.LayerNorm.bias\", \"text_model.encoder.layer.9.attention.self.query.weight\", \"text_model.encoder.layer.9.attention.self.query.bias\", \"text_model.encoder.layer.9.attention.self.key.weight\", \"text_model.encoder.layer.9.attention.self.key.bias\", \"text_model.encoder.layer.9.attention.self.value.weight\", \"text_model.encoder.layer.9.attention.self.value.bias\", \"text_model.encoder.layer.9.attention.output.dense.weight\", \"text_model.encoder.layer.9.attention.output.dense.bias\", \"text_model.encoder.layer.9.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.9.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.9.intermediate.dense.weight\", \"text_model.encoder.layer.9.intermediate.dense.bias\", \"text_model.encoder.layer.9.output.dense.weight\", \"text_model.encoder.layer.9.output.dense.bias\", \"text_model.encoder.layer.9.output.LayerNorm.weight\", \"text_model.encoder.layer.9.output.LayerNorm.bias\", \"text_model.encoder.layer.10.attention.self.query.weight\", \"text_model.encoder.layer.10.attention.self.query.bias\", \"text_model.encoder.layer.10.attention.self.key.weight\", \"text_model.encoder.layer.10.attention.self.key.bias\", \"text_model.encoder.layer.10.attention.self.value.weight\", \"text_model.encoder.layer.10.attention.self.value.bias\", \"text_model.encoder.layer.10.attention.output.dense.weight\", \"text_model.encoder.layer.10.attention.output.dense.bias\", \"text_model.encoder.layer.10.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.10.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.10.intermediate.dense.weight\", \"text_model.encoder.layer.10.intermediate.dense.bias\", \"text_model.encoder.layer.10.output.dense.weight\", \"text_model.encoder.layer.10.output.dense.bias\", \"text_model.encoder.layer.10.output.LayerNorm.weight\", \"text_model.encoder.layer.10.output.LayerNorm.bias\", \"text_model.encoder.layer.11.attention.self.query.weight\", \"text_model.encoder.layer.11.attention.self.query.bias\", \"text_model.encoder.layer.11.attention.self.key.weight\", \"text_model.encoder.layer.11.attention.self.key.bias\", \"text_model.encoder.layer.11.attention.self.value.weight\", \"text_model.encoder.layer.11.attention.self.value.bias\", \"text_model.encoder.layer.11.attention.output.dense.weight\", \"text_model.encoder.layer.11.attention.output.dense.bias\", \"text_model.encoder.layer.11.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.11.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.11.intermediate.dense.weight\", \"text_model.encoder.layer.11.intermediate.dense.bias\", \"text_model.encoder.layer.11.output.dense.weight\", \"text_model.encoder.layer.11.output.dense.bias\", \"text_model.encoder.layer.11.output.LayerNorm.weight\", \"text_model.encoder.layer.11.output.LayerNorm.bias\", \"text_model.pooler.dense.weight\", \"text_model.pooler.dense.bias\", \"visual_projection.weight\". "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pkg_resources import packaging\n",
        "import clip\n",
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import skimage\n",
        "import os\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "import torchvision\n",
        "from torchvision.datasets import EuroSAT\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "model, preprocess = clip.load(\"RN50\") #The model variable will hold the loaded model, while the preprocess variable will store the preprocessing function for preparing the input data.\n",
        "model.load_state_dict(torch.load('/home/xz306/Data-Climate-and-AI/src/fine_tuning/try/clip-roberta-finetuned/pytorch_model.bin'))\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution # (size) of the input images expected by the model.\n",
        "context_length = model.context_length # the length of the tokenized text input that the model can process.\n",
        "vocab_size = model.vocab_size # the size of the vocabulary used by the model.\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Main code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torch in ./.local/lib/python3.8/site-packages (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/lib/python3/dist-packages (from torch) (2.10.1)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in ./.local/lib/python3.8/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in ./.local/lib/python3.8/site-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in ./.local/lib/python3.8/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\" in ./.local/lib/python3.8/site-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/lib/python3/dist-packages (from torch) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\" in ./.local/lib/python3.8/site-packages (from torch) (11.7.101)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\" in ./.local/lib/python3.8/site-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied, skipping upgrade: triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in ./.local/lib/python3.8/site-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: networkx in /usr/lib/python3/dist-packages (from torch) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: sympy in ./.local/lib/python3.8/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in ./.local/lib/python3.8/site-packages (from torch) (11.4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in ./.local/lib/python3.8/site-packages (from torch) (11.7.91)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in ./.local/lib/python3.8/site-packages (from torch) (10.2.10.91)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in ./.local/lib/python3.8/site-packages (from torch) (11.7.4.91)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in ./.local/lib/python3.8/site-packages (from torch) (2.14.3)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in ./.local/lib/python3.8/site-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied, skipping upgrade: wheel in /usr/lib/python3/dist-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/lib/python3/dist-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (45.2.0)\n",
            "Requirement already satisfied, skipping upgrade: cmake in ./.local/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (3.26.4)\n",
            "Requirement already satisfied, skipping upgrade: lit in ./.local/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (16.0.6)\n",
            "Requirement already satisfied, skipping upgrade: mpmath>=0.19 in ./.local/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ViT-B/32\n",
            "Accuracy =  0.3382222222222222\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
        "models = ['ViT-B/32']\n",
        "for k in range(len(models)):\n",
        "    model, preprocess = clip.load(models[k])\n",
        "    model.cuda().eval()\n",
        "    Eurosat = EuroSAT(root='~/.cache', transform=preprocess, download=True)\n",
        "    classes = Eurosat.classes\n",
        "    classes[0] = 'annual crop land'\n",
        "    classes[2] = 'brushland or shrubland'\n",
        "    classes[4] = 'highway or road'\n",
        "    classes[5] = 'pasture land'\n",
        "    classes[6] = 'permanant crop land'\n",
        "    dataloader = DataLoader(Eurosat, batch_size=32, shuffle=True)\n",
        "    device = \"cuda\"\n",
        "    count = 0\n",
        "    top_k_accuracy = 1\n",
        "    count_total = 0\n",
        "    for i, (images, labels) in enumerate(dataloader):   \n",
        "        label = []\n",
        "        # Iterate over the dataset and store images and labels\n",
        "        for j in range(len(labels)):\n",
        "        # Retrieve image and label at index i\n",
        "            label.append(classes[labels[j]])\n",
        "        with torch.no_grad():\n",
        "            image_input = torch.tensor(np.stack(images)).cuda()\n",
        "            text_descriptions = [f\"a centered satellite photo of a {labe}\" for labe in classes]\n",
        "            text_tokens = clip.tokenize(text_descriptions).cuda()\n",
        "            image_features = model.encode_image(image_input).float() # image encoder\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            text_features = model.encode_text(text_tokens).float() # text encoder\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "        text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)   \n",
        "        for i in range(len(label)):\n",
        "            count_total += 1\n",
        "            values, indices = text_probs[i].topk(top_k_accuracy)\n",
        "            for value, index in zip(values, indices):\n",
        "                #print (Eurosat.classes[index])\n",
        "                if Eurosat.classes[index] == label[i]:\n",
        "                    count += 1\n",
        "                    break\n",
        "    print(models[k])\n",
        "    print (\"Accuracy = \", count/len(Eurosat))\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-1.1024e-02,  1.1208e-02,  3.1070e-02,  3.0421e-02, -9.6726e-03,\n",
              "        -2.2579e-02, -4.9175e-02, -1.8430e-02,  4.8614e-02,  9.2222e-03,\n",
              "         3.3699e-02,  2.9885e-03,  9.3034e-03, -4.2382e-03, -2.2801e-02,\n",
              "        -1.7809e-02,  3.2399e-02,  1.0618e-02,  1.4959e-02,  1.0293e-02,\n",
              "        -2.1161e-02, -2.5222e-02,  5.3192e-02, -1.0566e-02, -2.5134e-02,\n",
              "         7.8820e-04, -3.3817e-02,  2.5222e-02,  3.0066e-02, -3.9399e-02,\n",
              "         5.6153e-03, -3.8690e-02,  3.3492e-02, -5.8515e-03, -5.4964e-02,\n",
              "         4.0610e-03,  1.1356e-02, -1.4605e-02, -1.8075e-02, -6.2170e-03,\n",
              "         1.7174e-02, -1.2870e-02,  5.1095e-02,  7.2419e-02,  1.9877e-02,\n",
              "        -2.0674e-03, -1.8887e-02,  2.5119e-02,  1.9581e-02,  1.4915e-02,\n",
              "        -3.4869e-03,  2.2756e-02,  9.8276e-03, -2.3738e-03,  1.9700e-02,\n",
              "         5.5466e-02, -1.3460e-02,  3.3020e-02, -5.0504e-03, -1.2390e-02,\n",
              "        -1.6111e-02,  7.6051e-04,  2.5326e-02,  2.1058e-02,  1.9463e-02,\n",
              "         7.5165e-03, -2.1309e-02, -4.7255e-03, -1.5328e-02, -5.2808e-02,\n",
              "        -3.7243e-02, -2.9387e-02, -1.4908e-02,  4.9588e-02,  2.4218e-02,\n",
              "        -6.3795e-02,  1.6982e-02,  4.8053e-02, -9.8424e-03, -2.1900e-02,\n",
              "         2.8250e-02,  2.3303e-02, -2.5621e-02, -4.4390e-02, -3.8159e-02,\n",
              "         1.4834e-02,  1.4339e-02, -3.6505e-02,  2.5754e-02,  2.9475e-02,\n",
              "         3.5146e-02, -1.0596e-02, -7.5727e-02,  3.0273e-02, -2.2520e-02,\n",
              "        -1.0337e-02,  1.8932e-02,  4.1718e-03, -1.5447e-02, -8.4543e-03,\n",
              "        -6.7560e-03,  6.6512e-02,  2.6271e-02, -3.7597e-02, -2.3303e-02,\n",
              "        -1.6259e-02, -4.0256e-02, -4.9212e-03,  4.7196e-02,  8.0777e-03,\n",
              "        -3.3817e-02, -1.0892e-01, -2.0172e-02,  2.1634e-02, -3.8395e-02,\n",
              "         6.3093e-03,  4.2323e-02,  8.9017e-02, -3.1454e-02, -1.5284e-02,\n",
              "         2.2505e-02, -7.5904e-02,  2.8205e-02, -1.9153e-02, -1.2752e-02,\n",
              "         1.7455e-02,  1.5004e-02, -2.3539e-02,  5.7947e-02,  4.8643e-02,\n",
              "         8.8781e-02, -1.0994e-02, -2.7364e-02,  4.7066e-01, -1.9360e-02,\n",
              "         7.6347e-03, -2.8383e-02, -3.9960e-02, -6.3322e-02, -2.0630e-02,\n",
              "         2.1324e-02,  3.2134e-02, -5.4727e-02, -1.2213e-02, -2.1058e-02,\n",
              "         1.2663e-02,  1.0568e-03, -4.4361e-02,  2.5754e-02, -6.5508e-02,\n",
              "         1.9965e-02, -4.5158e-02,  1.6096e-02,  2.9712e-02,  1.8991e-02,\n",
              "         1.6702e-02,  2.5385e-02,  2.4883e-02, -1.7676e-02,  1.3955e-02,\n",
              "         3.0181e-03, -2.9667e-02, -7.4309e-02, -9.0966e-03,  6.9591e-03,\n",
              "        -4.3918e-02,  2.9738e-03,  4.4833e-02, -1.4118e-02,  1.0248e-02,\n",
              "         2.4322e-02, -3.2606e-02, -7.4545e-02, -1.4863e-02,  1.5476e-02,\n",
              "        -1.3724e-03, -5.5230e-02, -2.6330e-02, -1.9818e-02, -6.1314e-02,\n",
              "        -4.4745e-02, -2.5282e-02, -5.1279e-03,  3.7243e-02, -9.0597e-03,\n",
              "        -2.0453e-02,  5.3273e-03, -8.5502e-03, -1.6281e-03,  7.0588e-03,\n",
              "         6.8756e-02,  4.8466e-02,  1.5269e-02,  4.9581e-03, -2.6079e-02,\n",
              "        -3.9429e-02,  5.1538e-03, -1.2914e-02,  2.8575e-02,  3.2996e-04,\n",
              "         1.6259e-02, -5.9364e-03, -2.6227e-02, -5.5702e-02,  3.1100e-02,\n",
              "        -1.7219e-02, -5.7223e-03, -1.7957e-02, -1.0625e-02,  4.1407e-02,\n",
              "        -1.9360e-02,  7.7499e-02,  4.2161e-03,  4.7255e-02,  8.1811e-03,\n",
              "        -1.2153e-02, -6.4622e-02, -1.5240e-02,  1.3837e-02,  1.5284e-02,\n",
              "         3.7332e-02,  1.0714e-02,  6.8483e-03, -4.5542e-02,  5.0622e-02,\n",
              "        -3.3610e-02,  6.0767e-03, -6.5271e-02, -3.5552e-03, -1.1961e-02,\n",
              "         1.1851e-02,  1.3785e-02, -1.6096e-02, -5.2365e-02,  2.9741e-02,\n",
              "        -6.1247e-03, -2.7511e-02, -4.6192e-02,  5.9660e-03,  1.8666e-02,\n",
              "         4.9027e-03,  2.3716e-02, -6.2798e-03, -3.5353e-02, -1.6274e-02,\n",
              "        -3.6446e-02,  8.1737e-03, -1.7100e-02, -5.6529e-02,  2.8353e-02,\n",
              "        -7.0698e-03, -1.1895e-02,  2.1745e-03,  9.6578e-03,  1.3800e-02,\n",
              "         3.9370e-02, -1.0861e-02,  6.8815e-02,  2.6492e-02,  2.2121e-02,\n",
              "        -1.8533e-02,  4.0137e-02, -7.8444e-02, -4.0352e-03,  2.4794e-02,\n",
              "        -1.3121e-02, -7.5461e-03, -5.0061e-02, -2.9786e-02, -2.0305e-02,\n",
              "         4.5409e-04,  5.9955e-02,  6.5437e-04, -4.6251e-02, -1.7839e-02,\n",
              "         1.4967e-02, -1.9124e-03,  4.6546e-02, -3.1631e-02, -5.9881e-03,\n",
              "         9.1040e-03,  3.7010e-03,  2.8959e-02,  1.3460e-02, -5.3162e-02,\n",
              "        -1.7322e-02, -1.1910e-02,  5.5850e-02,  3.5441e-02, -5.8892e-02,\n",
              "         3.0716e-02,  3.5353e-02,  1.2825e-02,  4.0846e-02, -4.9795e-02,\n",
              "        -1.3556e-02,  1.6687e-02,  1.5609e-02,  5.1863e-02, -3.7095e-02,\n",
              "         3.1720e-02, -2.3362e-02, -1.0172e-01,  5.9305e-02, -5.7445e-02,\n",
              "        -6.6689e-02,  1.2294e-02, -2.3628e-04,  6.6859e-03, -1.5122e-02,\n",
              "         1.4943e-03,  3.9104e-02,  4.7113e-01,  2.8767e-02,  1.0012e-02,\n",
              "         1.1784e-02, -1.7706e-02, -4.4095e-02, -7.6937e-03, -5.8685e-02,\n",
              "        -4.7875e-02,  2.6463e-02,  4.9559e-02, -1.4989e-02, -4.2197e-03,\n",
              "        -4.2293e-02, -4.4686e-02,  1.0603e-02, -3.1513e-02, -3.6859e-02,\n",
              "        -7.3187e-02,  2.7334e-02, -3.7332e-02, -3.8631e-02,  6.3093e-03,\n",
              "         9.2738e-03, -2.2594e-02,  1.3505e-02, -3.1070e-02,  3.9687e-03,\n",
              "         6.9997e-03,  7.7233e-03,  8.0747e-02,  4.3593e-02,  6.4799e-02,\n",
              "         2.8855e-02,  7.3319e-03, -1.3394e-02,  4.7935e-02,  2.0083e-02,\n",
              "         2.2269e-02, -1.7706e-02,  5.0150e-02, -6.6807e-02, -2.9653e-02,\n",
              "        -5.6086e-02,  3.8690e-02, -6.5419e-03, -5.2719e-02,  2.1531e-02,\n",
              "         6.1727e-03, -9.0523e-03,  5.4993e-02,  3.1927e-02, -2.9579e-02,\n",
              "        -1.8791e-03, -1.3283e-02,  5.1272e-02,  4.2677e-02, -5.0209e-03,\n",
              "         4.6074e-03, -3.9606e-02, -1.1939e-02,  5.5525e-02, -2.1900e-02,\n",
              "         3.6003e-02, -4.1673e-02, -4.1644e-02, -3.3079e-02,  2.7526e-02,\n",
              "         2.4868e-02,  8.6167e-03,  7.0174e-02,  2.5798e-02, -1.8238e-03,\n",
              "        -1.2995e-02, -6.6807e-02,  1.3497e-02, -5.8390e-02,  3.7453e-03,\n",
              "        -8.4882e-02,  3.2518e-02,  3.1129e-02, -3.2842e-02,  2.0231e-02,\n",
              "         2.0231e-02,  2.2387e-02,  3.0302e-02, -4.7757e-02,  4.5719e-02,\n",
              "         4.8791e-02, -1.7839e-02, -1.0440e-02,  4.4420e-02,  2.7999e-02,\n",
              "         2.6980e-02, -1.2958e-02,  4.0795e-03, -1.3852e-02, -1.6495e-02,\n",
              "        -1.1858e-02,  1.6037e-02,  1.9622e-03, -6.6674e-03,  2.1782e-02,\n",
              "        -2.3738e-03, -1.7145e-02, -2.9933e-02, -1.1083e-02, -2.4846e-03,\n",
              "         1.3896e-02, -2.3820e-02, -2.9018e-02, -1.0550e-01,  1.3106e-02,\n",
              "         3.0598e-02, -1.6909e-02, -6.3979e-03, -1.2796e-02,  3.5560e-02,\n",
              "        -3.7243e-02, -2.1235e-02,  2.0069e-02, -3.3596e-03, -2.3096e-02,\n",
              "        -2.7578e-03, -1.2124e-02, -5.2904e-03,  4.6930e-02, -4.2736e-02,\n",
              "        -6.0900e-02, -9.8202e-03,  6.2318e-02, -5.4255e-02, -1.0817e-02,\n",
              "        -1.9590e-04,  3.8506e-03,  2.3155e-02,  1.1282e-02, -6.5389e-02,\n",
              "        -6.4208e-02,  4.7292e-03, -4.2559e-02, -7.2891e-02, -1.2368e-02,\n",
              "         3.5382e-02,  2.2830e-02,  9.9088e-03, -2.8087e-02, -3.4703e-02,\n",
              "        -1.0500e-02, -2.2313e-02, -1.1685e-03,  1.0123e-02,  1.9035e-02,\n",
              "         6.2613e-02,  8.3361e-03, -2.0187e-02,  2.8501e-03, -5.2188e-02,\n",
              "        -6.6194e-03,  1.7396e-02, -1.0559e-02, -1.1762e-02,  4.3888e-02,\n",
              "        -4.8990e-03,  3.4496e-02, -4.1673e-02,  4.8732e-04,  3.4821e-02,\n",
              "         2.7379e-02, -2.6182e-02,  2.4455e-02,  2.7792e-02,  2.3879e-02,\n",
              "        -2.3244e-02, -5.4122e-03,  1.0787e-02, -6.9886e-03,  3.5903e-03,\n",
              "         1.2102e-02, -3.5855e-02,  2.3155e-02,  1.0863e-01, -5.7777e-04,\n",
              "        -2.3731e-02, -3.2119e-03,  2.8781e-02, -6.1491e-02, -3.6051e-03,\n",
              "         2.0988e-03,  2.8667e-03,  1.3844e-02,  1.7499e-02,  5.0593e-02,\n",
              "         2.0482e-02, -8.6241e-03, -1.2729e-02, -2.0231e-02, -1.0189e-02,\n",
              "        -1.0263e-02, -4.2205e-02], device='cuda:0')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_features[0]\n",
        "text_features[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type vision-text-dual-encoder to instantiate a model of type clip. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of CLIPModel were not initialized from the model checkpoint at /home/xz306/Data-Climate-and-AI/src/fine_tuning/try/clip-roberta-finetuned and are newly initialized: ['text_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'text_model.embeddings.token_embedding.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'vision_model.pre_layrnorm.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.embeddings.class_embedding', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.post_layernorm.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CLIPModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(50265, 768)\n",
              "      (position_embedding): Embedding(514, 768)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (vision_model): CLIPVisionTransformer(\n",
              "    (embeddings): CLIPVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "      (position_embedding): Embedding(50, 768)\n",
              "    )\n",
              "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
              "  (text_projection): Linear(in_features=768, out_features=512, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_path = \"/home/xz306/Data-Climate-and-AI/src/fine_tuning/try/clip-roberta-finetuned\"  # Path to the fine-tuned model directory\n",
        "model = CLIPModel.from_pretrained(model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
