{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXpvCXwLlnkF",
        "outputId": "8ac156b3-7074-4ce9-9994-7334ddeed585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 2.0.1+cu117\n",
            "Model parameters: 102,007,137\n",
            "Input resolution: 224\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "import numpy as np\n",
        "import torch\n",
        "from pkg_resources import packaging\n",
        "import clip\n",
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import skimage\n",
        "import os\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "import torchvision\n",
        "from torchvision.datasets import EuroSAT\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "model, preprocess = clip.load(\"RN50\") #The model variable will hold the loaded model, while the preprocess variable will store the preprocessing function for preparing the input data.\n",
        "#model.load_state_dict(torch.load('/home/xz306/Data-Climate-and-AI/src/fine_tuning/try/clip-roberta-finetuned/pytorch_model.bin'))\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution # (size) of the input images expected by the model.\n",
        "context_length = model.context_length # the length of the tokenized text input that the model can process.\n",
        "vocab_size = model.vocab_size # the size of the vocabulary used by the model.\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Main code to test the performance of original CLIP on EuroSAT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ViT-B/32\n",
            "Accuracy =  0.3382222222222222\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
        "for k in range(len(models)):\n",
        "    model, preprocess = clip.load(models[k])\n",
        "    model.cuda().eval()\n",
        "    Eurosat = EuroSAT(root='~/.cache', transform=preprocess, download=True)\n",
        "    classes = Eurosat.classes\n",
        "    classes[0] = 'annual crop land'\n",
        "    classes[2] = 'brushland or shrubland'\n",
        "    classes[4] = 'highway or road'\n",
        "    classes[5] = 'pasture land'\n",
        "    classes[6] = 'permanant crop land'\n",
        "    dataloader = DataLoader(Eurosat, batch_size=32, shuffle=True)\n",
        "    device = \"cuda\"\n",
        "    count = 0\n",
        "    top_k_accuracy = 1\n",
        "    count_total = 0\n",
        "    for i, (images, labels) in enumerate(dataloader):   \n",
        "        label = []\n",
        "        # Iterate over the dataset and store images and labels\n",
        "        for j in range(len(labels)):\n",
        "        # Retrieve image and label at index i\n",
        "            label.append(classes[labels[j]])\n",
        "        with torch.no_grad():\n",
        "            image_input = torch.tensor(np.stack(images)).cuda()\n",
        "            text_descriptions = [f\"a centered satellite photo of a {labe}\" for labe in classes]\n",
        "            text_tokens = clip.tokenize(text_descriptions).cuda()\n",
        "            image_features = model.encode_image(image_input).float() # image encoder\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            text_features = model.encode_text(text_tokens).float() # text encoder\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "        text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)   \n",
        "        for i in range(len(label)):\n",
        "            count_total += 1\n",
        "            values, indices = text_probs[i].topk(top_k_accuracy)\n",
        "            for value, index in zip(values, indices):\n",
        "                #print (Eurosat.classes[index])\n",
        "                if Eurosat.classes[index] == label[i]:\n",
        "                    count += 1\n",
        "                    break\n",
        "    print(models[k])\n",
        "    print (\"Accuracy = \", count/len(Eurosat))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Code to test the performance of tuned model on EuroSAT. Change the paths of model and processor to where you save your saved model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.37\n"
          ]
        }
      ],
      "source": [
        "#model.cuda().eval()\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import transformers\n",
        "from transformers import (\n",
        "    VisionTextDualEncoderModel,\n",
        "    VisionTextDualEncoderProcessor,\n",
        "    AutoTokenizer,\n",
        "    AutoImageProcessor\n",
        ")\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import numpy as np\n",
        "import torch\n",
        "from pkg_resources import packaging\n",
        "import clip\n",
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import skimage\n",
        "import os\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "import torchvision\n",
        "from torchvision.datasets import EuroSAT\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "# seed = 42\n",
        "# torch.manual_seed(seed)\n",
        "# random.seed(seed)\n",
        "Eurosat = EuroSAT(root='~/.cache', download=True)\n",
        "classes = Eurosat.classes\n",
        "Eurosat = list(Eurosat)\n",
        "random.shuffle(Eurosat)\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "classes[0] = 'annual crop land'\n",
        "classes[2] = 'brushland or shrubland'\n",
        "classes[4] = 'highway or road'\n",
        "classes[5] = 'pasture land'\n",
        "classes[6] = 'permanant crop land'\n",
        "text = []\n",
        "count = 0\n",
        "for i in range(len(classes)):\n",
        "    text.append('a satellite photo of' + classes[i])\n",
        "labels = []\n",
        "images = []\n",
        "# Iterate over the dataset and store images and labels\n",
        "for i in range(100):\n",
        "    images.append(Eurosat[i][0])\n",
        "    labels.append(Eurosat[i][1])\n",
        "inputs = processor(images=images, text=text,  return_tensors=\"pt\", padding=True)\n",
        "outputs = model(**inputs)   \n",
        "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "probs = logits_per_image.softmax(dim=1)\n",
        "max_indices = torch.argmax(probs, dim=1)\n",
        "result = max_indices.tolist()\n",
        "count_same_values = 0\n",
        "for i in range(min(len(result), len(labels))):\n",
        "    if result[i] == labels[i]:\n",
        "        count_same_values += 1\n",
        "accuracy = count_same_values/100\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
