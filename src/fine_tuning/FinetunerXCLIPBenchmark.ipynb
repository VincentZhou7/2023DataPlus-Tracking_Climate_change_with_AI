{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08NvzS3_nCXl"
      },
      "source": [
        "# Finetuner X CLIP Benchmark\n",
        "\n",
        "@bo_wangbo\n",
        "@fissoreg\n",
        "\n",
        "In this Colab notebook, we'll try to use [Finetuner](https://github.com/jina-ai/finetuner) to fine-tune the CLIP model on `Flickr8k`, and compare the retrieval metrics produced by the fine-tuned model against pre-trained zero-shot results produced from CLIP Benchmark.\n",
        "\n",
        "*NOTE: Finetuner is a cloud-based training platform, which requires you to login and Finetuner will allocate computational resources automatically for free.*\n",
        "\n",
        "**Please Consider [Switching to a GPU Runtime](https://medium.com/@oribarel/getting-the-most-out-of-your-google-colab-2b0585f82403) for faster evaluation!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFB3KT4a59mn",
        "outputId": "a8ab1d4a-8770-4c19-b9a5-68a204cdbb71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: finetuner[full] in ./.local/lib/python3.8/site-packages (0.7.8)\n",
            "Requirement already satisfied: trimesh==3.16.4 in ./.local/lib/python3.8/site-packages (from finetuner[full]) (3.16.4)\n",
            "Requirement already satisfied: finetuner-stubs==0.13.7 in ./.local/lib/python3.8/site-packages (from finetuner[full]) (0.13.7)\n",
            "Requirement already satisfied: jina-hubble-sdk==0.33.1 in ./.local/lib/python3.8/site-packages (from finetuner[full]) (0.33.1)\n",
            "Requirement already satisfied: docarray[common]<0.30.0 in ./.local/lib/python3.8/site-packages (from finetuner[full]) (0.21.1)\n",
            "Requirement already satisfied: finetuner-commons==0.13.7; extra == \"full\" in ./.local/lib/python3.8/site-packages (from finetuner[full]) (0.13.7)\n",
            "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (from trimesh==3.16.4->finetuner[full]) (1.23.1)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in ./.local/lib/python3.8/site-packages (from finetuner-stubs==0.13.7->finetuner[full]) (4.5.0)\n",
            "Requirement already satisfied: pydantic~=1.9 in ./.local/lib/python3.8/site-packages (from finetuner-stubs==0.13.7->finetuner[full]) (1.10.9)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from jina-hubble-sdk==0.33.1->finetuner[full]) (2.22.0)\n",
            "Requirement already satisfied: docker in ./.local/lib/python3.8/site-packages (from jina-hubble-sdk==0.33.1->finetuner[full]) (6.1.3)\n",
            "Requirement already satisfied: importlib-metadata in ./.local/lib/python3.8/site-packages (from jina-hubble-sdk==0.33.1->finetuner[full]) (6.7.0)\n",
            "Requirement already satisfied: pyyaml in ./.local/lib/python3.8/site-packages (from jina-hubble-sdk==0.33.1->finetuner[full]) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from jina-hubble-sdk==0.33.1->finetuner[full]) (3.0.12)\n",
            "Requirement already satisfied: aiohttp in ./.local/lib/python3.8/site-packages (from jina-hubble-sdk==0.33.1->finetuner[full]) (3.8.4)\n",
            "Requirement already satisfied: pathspec in ./.local/lib/python3.8/site-packages (from jina-hubble-sdk==0.33.1->finetuner[full]) (0.11.1)\n",
            "Requirement already satisfied: python-jose in ./.local/lib/python3.8/site-packages (from jina-hubble-sdk==0.33.1->finetuner[full]) (3.3.0)\n",
            "Requirement already satisfied: rich in ./.local/lib/python3.8/site-packages (from jina-hubble-sdk==0.33.1->finetuner[full]) (13.4.2)\n",
            "Requirement already satisfied: fastapi; extra == \"common\" in ./.local/lib/python3.8/site-packages (from docarray[common]<0.30.0->finetuner[full]) (0.98.0)\n",
            "Requirement already satisfied: Pillow; extra == \"common\" in /usr/lib/python3/dist-packages (from docarray[common]<0.30.0->finetuner[full]) (7.0.0)\n",
            "Requirement already satisfied: protobuf>=3.13.0; extra == \"common\" in ./.local/lib/python3.8/site-packages (from docarray[common]<0.30.0->finetuner[full]) (3.20.3)\n",
            "Requirement already satisfied: matplotlib; extra == \"common\" in /usr/lib/python3/dist-packages (from docarray[common]<0.30.0->finetuner[full]) (3.1.2)\n",
            "Requirement already satisfied: lz4; extra == \"common\" in ./.local/lib/python3.8/site-packages (from docarray[common]<0.30.0->finetuner[full]) (4.3.2)\n",
            "Requirement already satisfied: uvicorn; extra == \"common\" in ./.local/lib/python3.8/site-packages (from docarray[common]<0.30.0->finetuner[full]) (0.22.0)\n",
            "Requirement already satisfied: torch~=1.12.0 in ./.local/lib/python3.8/site-packages (from finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (1.12.1)\n",
            "Requirement already satisfied: onnx>1.11.0 in ./.local/lib/python3.8/site-packages (from finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (1.14.0)\n",
            "Collecting transformers==4.20.1\n",
            "  Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "Requirement already satisfied: torchvision~=0.13.0 in ./.local/lib/python3.8/site-packages (from finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (0.13.1)\n",
            "Requirement already satisfied: open-clip-torch==2.5.0 in ./.local/lib/python3.8/site-packages (from finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (2.5.0)\n",
            "Requirement already satisfied: onnxruntime>1.11.1 in ./.local/lib/python3.8/site-packages (from finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (1.15.1)\n",
            "Requirement already satisfied: albumentations==1.2.0 in ./.local/lib/python3.8/site-packages (from finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (1.2.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in ./.local/lib/python3.8/site-packages (from docker->jina-hubble-sdk==0.33.1->finetuner[full]) (2.0.3)\n",
            "Requirement already satisfied: packaging>=14.0 in ./.local/lib/python3.8/site-packages (from docker->jina-hubble-sdk==0.33.1->finetuner[full]) (23.1)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in ./.local/lib/python3.8/site-packages (from docker->jina-hubble-sdk==0.33.1->finetuner[full]) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in ./.local/lib/python3.8/site-packages (from importlib-metadata->jina-hubble-sdk==0.33.1->finetuner[full]) (3.15.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in ./.local/lib/python3.8/site-packages (from aiohttp->jina-hubble-sdk==0.33.1->finetuner[full]) (1.9.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.local/lib/python3.8/site-packages (from aiohttp->jina-hubble-sdk==0.33.1->finetuner[full]) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.8/site-packages (from aiohttp->jina-hubble-sdk==0.33.1->finetuner[full]) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.8/site-packages (from aiohttp->jina-hubble-sdk==0.33.1->finetuner[full]) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.8/site-packages (from aiohttp->jina-hubble-sdk==0.33.1->finetuner[full]) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->jina-hubble-sdk==0.33.1->finetuner[full]) (19.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.local/lib/python3.8/site-packages (from aiohttp->jina-hubble-sdk==0.33.1->finetuner[full]) (3.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/lib/python3/dist-packages (from python-jose->jina-hubble-sdk==0.33.1->finetuner[full]) (0.4.2)\n",
            "Requirement already satisfied: rsa in /usr/lib/python3/dist-packages (from python-jose->jina-hubble-sdk==0.33.1->finetuner[full]) (4.0)\n",
            "Requirement already satisfied: ecdsa!=0.15 in ./.local/lib/python3.8/site-packages (from python-jose->jina-hubble-sdk==0.33.1->finetuner[full]) (0.18.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.local/lib/python3.8/site-packages (from rich->jina-hubble-sdk==0.33.1->finetuner[full]) (2.15.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.local/lib/python3.8/site-packages (from rich->jina-hubble-sdk==0.33.1->finetuner[full]) (3.0.0)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in ./.local/lib/python3.8/site-packages (from fastapi; extra == \"common\"->docarray[common]<0.30.0->finetuner[full]) (0.27.0)\n",
            "Requirement already satisfied: h11>=0.8 in ./.local/lib/python3.8/site-packages (from uvicorn; extra == \"common\"->docarray[common]<0.30.0->finetuner[full]) (0.14.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/lib/python3/dist-packages (from uvicorn; extra == \"common\"->docarray[common]<0.30.0->finetuner[full]) (7.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in ./.local/lib/python3.8/site-packages (from transformers==4.20.1->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.8/site-packages (from transformers==4.20.1->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.8/site-packages (from transformers==4.20.1->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (2023.6.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in ./.local/lib/python3.8/site-packages (from transformers==4.20.1->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (0.15.1)\n",
            "Requirement already satisfied: ftfy in ./.local/lib/python3.8/site-packages (from open-clip-torch==2.5.0->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (6.1.1)\n",
            "Requirement already satisfied: sympy in ./.local/lib/python3.8/site-packages (from onnxruntime>1.11.1->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (1.12)\n",
            "Requirement already satisfied: coloredlogs in ./.local/lib/python3.8/site-packages (from onnxruntime>1.11.1->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/lib/python3/dist-packages (from onnxruntime>1.11.1->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (1.12)\n",
            "Requirement already satisfied: scikit-image<0.19,>=0.16.1 in /usr/lib/python3/dist-packages (from albumentations==1.2.0->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (0.16.2)\n",
            "Requirement already satisfied: qudida>=0.0.4 in ./.local/lib/python3.8/site-packages (from albumentations==1.2.0->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (0.0.4)\n",
            "Requirement already satisfied: scipy in ./.local/lib/python3.8/site-packages (from albumentations==1.2.0->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (1.10.1)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in ./.local/lib/python3.8/site-packages (from albumentations==1.2.0->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (4.7.0.72)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->jina-hubble-sdk==0.33.1->finetuner[full]) (2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from ecdsa!=0.15->python-jose->jina-hubble-sdk==0.33.1->finetuner[full]) (1.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./.local/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich->jina-hubble-sdk==0.33.1->finetuner[full]) (0.1.2)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in ./.local/lib/python3.8/site-packages (from starlette<0.28.0,>=0.27.0->fastapi; extra == \"common\"->docarray[common]<0.30.0->finetuner[full]) (3.7.0)\n",
            "Requirement already satisfied: fsspec in ./.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (2023.6.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in ./.local/lib/python3.8/site-packages (from ftfy->open-clip-torch==2.5.0->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (0.2.6)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./.local/lib/python3.8/site-packages (from sympy->onnxruntime>1.11.1->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (1.3.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in ./.local/lib/python3.8/site-packages (from coloredlogs->onnxruntime>1.11.1->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (10.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in ./.local/lib/python3.8/site-packages (from qudida>=0.0.4->albumentations==1.2.0->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (1.2.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in ./.local/lib/python3.8/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi; extra == \"common\"->docarray[common]<0.30.0->finetuner[full]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup; python_version < \"3.11\" in ./.local/lib/python3.8/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi; extra == \"common\"->docarray[common]<0.30.0->finetuner[full]) (1.1.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in ./.local/lib/python3.8/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.2.0->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (1.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.8/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.2.0->finetuner-commons==0.13.7; extra == \"full\"->finetuner[full]) (3.1.0)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.31.0.dev0\n",
            "    Uninstalling transformers-4.31.0.dev0:\n",
            "      Successfully uninstalled transformers-4.31.0.dev0\n",
            "Successfully installed transformers-4.20.1\n",
            "Requirement already satisfied: kaggle in ./.local/lib/python3.8/site-packages (1.5.13)\n",
            "Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.14.0)\n",
            "Requirement already satisfied: python-slugify in ./.local/lib/python3.8/site-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/lib/python3/dist-packages (from kaggle) (2.7.3)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from kaggle) (2.22.0)\n",
            "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kaggle) (2019.11.28)\n",
            "Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: urllib3 in ./.local/lib/python3.8/site-packages (from kaggle) (2.0.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in ./.local/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
            "Collecting git+https://github.com/bwanglzu/CLIP_benchmark.git\n",
            "  Cloning https://github.com/bwanglzu/CLIP_benchmark.git to /tmp/pip-req-build-50ezrzc1\n",
            "  Running command git clone -q https://github.com/bwanglzu/CLIP_benchmark.git /tmp/pip-req-build-50ezrzc1\n",
            "Requirement already satisfied (use --upgrade to upgrade): clip-benchmark==0.1.0 from git+https://github.com/bwanglzu/CLIP_benchmark.git in ./.local/lib/python3.8/site-packages\n",
            "Requirement already satisfied: open_clip_torch>=0.2.1 in ./.local/lib/python3.8/site-packages (from clip-benchmark==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in ./.local/lib/python3.8/site-packages (from clip-benchmark==0.1.0) (1.2.2)\n",
            "Requirement already satisfied: torchvision~=0.13.0 in ./.local/lib/python3.8/site-packages (from clip-benchmark==0.1.0) (0.13.1)\n",
            "Requirement already satisfied: torch~=1.12.0 in ./.local/lib/python3.8/site-packages (from clip-benchmark==0.1.0) (1.12.1)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in ./.local/lib/python3.8/site-packages (from clip-benchmark==0.1.0) (4.65.0)\n",
            "Requirement already satisfied: ftfy in ./.local/lib/python3.8/site-packages (from open_clip_torch>=0.2.1->clip-benchmark==0.1.0) (6.1.1)\n",
            "Requirement already satisfied: huggingface-hub in ./.local/lib/python3.8/site-packages (from open_clip_torch>=0.2.1->clip-benchmark==0.1.0) (0.15.1)\n",
            "Requirement already satisfied: regex in ./.local/lib/python3.8/site-packages (from open_clip_torch>=0.2.1->clip-benchmark==0.1.0) (2023.6.3)\n",
            "Requirement already satisfied: numpy>=1.17.3 in ./.local/lib/python3.8/site-packages (from scikit-learn>=1.0->clip-benchmark==0.1.0) (1.23.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in ./.local/lib/python3.8/site-packages (from scikit-learn>=1.0->clip-benchmark==0.1.0) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in ./.local/lib/python3.8/site-packages (from scikit-learn>=1.0->clip-benchmark==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.8/site-packages (from scikit-learn>=1.0->clip-benchmark==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision~=0.13.0->clip-benchmark==0.1.0) (7.0.0)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchvision~=0.13.0->clip-benchmark==0.1.0) (2.22.0)\n",
            "Requirement already satisfied: typing-extensions in ./.local/lib/python3.8/site-packages (from torchvision~=0.13.0->clip-benchmark==0.1.0) (4.5.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in ./.local/lib/python3.8/site-packages (from ftfy->open_clip_torch>=0.2.1->clip-benchmark==0.1.0) (0.2.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.local/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch>=0.2.1->clip-benchmark==0.1.0) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in ./.local/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch>=0.2.1->clip-benchmark==0.1.0) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub->open_clip_torch>=0.2.1->clip-benchmark==0.1.0) (3.0.12)\n",
            "Requirement already satisfied: fsspec in ./.local/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch>=0.2.1->clip-benchmark==0.1.0) (2023.6.0)\n",
            "Building wheels for collected packages: clip-benchmark\n",
            "  Building wheel for clip-benchmark (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for clip-benchmark: filename=clip_benchmark-0.1.0-py2.py3-none-any.whl size=48107 sha256=3f2c7ad4c0cb5f534efda9c5df86933239a84519f2bd3b2ba25477f499a770f6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6_qkzxnr/wheels/39/c8/6a/f25b26b0019d40912127f21c4bf6600fde549ad58c4e3cda41\n",
            "Successfully built clip-benchmark\n"
          ]
        }
      ],
      "source": [
        "!pip install \"finetuner[full]\"\n",
        "# our fork of CLIP benchmark, resolved some minor issues in data builder and adjust the evaluator code to allow evaluator receive 2 models\n",
        "# when fine-tuning CLIP, Finetuner will un-wrap the CLIP model into 2 models and save them individually\n",
        "!pip install kaggle\n",
        "!pip install git+https://github.com/bwanglzu/CLIP_benchmark.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol2JJrSJsD-q"
      },
      "source": [
        "## Preparing the training data\n",
        "\n",
        "CLIP Benchmark comes with a dataset `builder` that does much of the work of assembling training data. However, for Finetuner, we need to convert it into Jina DocArray format.\n",
        "\n",
        "We will use:\n",
        "\n",
        "1. CLIP Benchmark contains a file named `captions.txt` which includes all Flickr8k image urls with captions.\n",
        "2. CLIP Benchmark reused the Karpathy split which split the `Flickr8k` into test sets and training sets. The test set includs 5000 images with annotations.\n",
        "\n",
        "We will build our training set by loading all images, and then then excluding the test set images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUbvP-jGD74b",
        "outputId": "2db3f3b5-fe80-4384-b694-5f96bc3d08c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (5.1.0) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset Flickr\n",
              "    Number of datapoints: 1000\n",
              "    Root location: root"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from clip_benchmark.datasets.builder import build_dataset\n",
        "\n",
        "# please fill in your kaggle token here, you should be able to get your kaggle\n",
        "# user name and key in kaggle personal settings.\n",
        "# CLIP Benchmark uses kaggle to download flickr8k dataset\n",
        "os.environ['KAGGLE_USERNAME'] = 'vincentzho'\n",
        "os.environ['KAGGLE_KEY'] = '162c937e7a869e55276c795e1c293876'\n",
        "\n",
        "build_dataset(dataset_name='flickr8k', annotation_file=None, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8RAM-4UD9aF",
        "outputId": "15558fce-9166-4827-9985-1134f617954c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of the full image set is 40455\n",
            "Size of the test image set is 5000\n"
          ]
        }
      ],
      "source": [
        "##### I have changed the root dir to my path ########\n",
        "root_dir = '/home/xz306/root/'\n",
        "full_annotation = root_dir + 'captions.txt'\n",
        "test_annotation = root_dir + 'flickr8k_test_karpathy.txt'\n",
        "\n",
        "all_imgs = []\n",
        "test_imgs = []\n",
        "with open(full_annotation, 'r') as f:\n",
        "    next(f) # exclude the header line\n",
        "    for idx, item in enumerate(f.readlines()):\n",
        "        all_imgs.append(item.split(',', 1)[0])\n",
        "\n",
        "with open(test_annotation, 'r') as f:\n",
        "    next(f) # exclude the header line\n",
        "    for idx, item in enumerate(f.readlines()):\n",
        "        test_imgs.append(item.split(',', 1)[0])\n",
        "\n",
        "print(f'Size of the full image set is {len(all_imgs)}')\n",
        "print(f'Size of the test image set is {len(test_imgs)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG3M2Cv7wfKU"
      },
      "source": [
        "Now we will convert the downloaded images into `DocumentArray` format like this:\n",
        "\n",
        "```python\n",
        "from docarray import Document, DocumentArray\n",
        "\n",
        "pairs = DocumentArray()\n",
        "pair_1 = Document(chunks=[\n",
        "    img_chunk = Document(uri='your-image.jpg', modality='image'),\n",
        "    txt_chunk = Document(content='the text descriptor', modality='text'),\n",
        "]}\n",
        "pair_2 = ...\n",
        "pairs.extend([pair_1, pair_2, ...])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLgNsciPvLlH",
        "outputId": "77538bb0-d0d8-4959-f723-43ace87630d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5000it [02:59, 27.88it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The size of the training data is 4376\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from docarray import Document, DocumentArray\n",
        "\n",
        "train = DocumentArray()\n",
        "with open(full_annotation, 'r') as f:\n",
        "    next(f) # exclude the header line\n",
        "    for idx, line in tqdm(enumerate(f.readlines())):\n",
        "        url, txt = line.split(',', 1)\n",
        "        if url in test_imgs:  # do not include test images into training set\n",
        "            continue\n",
        "        img_chunk = Document(uri=root_dir + url, modality='image')\n",
        "        txt_chunk = Document(content=txt, modality='text')\n",
        "        img_chunk.load_uri_to_image_tensor(224, 224)\n",
        "        img_chunk.pop('uri')\n",
        "        pair = Document(chunks=[img_chunk, txt_chunk])\n",
        "        train.append(pair)\n",
        "        if idx == 5000: # we only use a subset to train\n",
        "            break\n",
        "\n",
        "print(f'The size of the training data is {len(train)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhXVm8tV0VEr"
      },
      "source": [
        "The Flickr8k dataset contains 8,000 images, each with 5 descriptive texts, or 40,000 image-text pairs in total.\n",
        "\n",
        "+ The training set has ~35000 image-text pairs.\n",
        "+ The test set has ~5000 image-text pairs.\n",
        "\n",
        "## Start Fine-tuning\n",
        "\n",
        "Now that we have prepared the training and test data, the next step is to start the fine-tuning job using Finetuner. Finetuner takes a pre-trained model from a 3rd party library, such as `open_clip`, then jointly optimize the `CLIPLoss` function for the image encoder and text encoder.\n",
        "\n",
        "Finetuner will also reserve a cloud GPU for you for free."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "UC merced land use dateset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203,
          "referenced_widgets": [
            "5640c168070347138c6f12a89f3527f3",
            "74d5ed1405e14e7b82fa5c439509b08d",
            "d22020b9cdcb4db5b903ec0510d77bf9",
            "7a2cb071664a4ec985769c655941b3ab",
            "c7ba7e7ad7a1478498bd1b7035cfa262",
            "052b3cbaaf854241a5a9006b44b4164d",
            "c8429c73d5554f7e9f6578a7c04557af",
            "85713ec3389446e6869e69a262023f43",
            "cdd962679dbb49e3873289ef6bd13633",
            "f4b7f904482d4e5d9ab526fe7897f256",
            "f27061fd69d24cd6a77a694aec8906f2",
            "b2492a07ae4649f29c0d0a8c23c4047e",
            "0e7d0228b5b24d0ba23a493264adb815",
            "f75a1fb831f64ca59dfeb7ed8e411483",
            "a56509f7c23d495fa81dc07b5ed38419",
            "14fdf26fefb4453381b45dd79a53b42f",
            "668ad79a393f4d21bf601b2354ae3e84",
            "7ea64cf5a7064a5fbf870f46616187e6",
            "59221f747d3d472a8eb4c7e8a819bf71",
            "5361787acc0948be974a31bd054632b3",
            "35202cf8b3b64938b0437e3433ee7d1e",
            "2e16156e982840e7bcf859bc8bac43c0"
          ]
        },
        "id": "hG_siRL7zvEp",
        "outputId": "0b5bde65-8598-410f-a0da-90fcffdcc962"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f57e3206fb264609a4f617c615bb725a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value=\"\\n<div class='custom-container'>\\n    <style>\\n        .custom-container {\\n       …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import finetuner\n",
        "\n",
        "finetuner.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrFZqjvY26eJ"
      },
      "outputs": [],
      "source": [
        "# Note, we have push the training set below to the cloud, and set the dataset as public, so you don't have to push again.\n",
        "# train.push('finetuner-flickr8k-demo', public=True, show_progress=True)\n",
        "# finetuner.delete_run('clip-run')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "O7BbFKF71c-l"
      },
      "outputs": [],
      "source": [
        "run = finetuner.fit(\n",
        "    ####### I change the model name from ViT-B-32::openai to openai/clip-vit-base-patch32 because it shows there is no model called ViT-B-32 #####\n",
        "    model='openai/clip-vit-base-patch32', # we take ViT-B-32 trained from Open AI, model provided by OpenCLIP\n",
        "    train_data='finetuner-flickr8k-demo', # the dataset we prepared has been pushed to the cloud in the prev section\n",
        "    run_name='clip-run',\n",
        "    loss='CLIPLoss', # use CLIPLoss for fine-tuning CLIP model\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    learning_rate= 1e-6,\n",
        "    device='cuda',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "77fd7e85ea4346c5bb3886f2f4c42bca",
            "eedc32bcc61a430dadec9bca5a508745"
          ]
        },
        "id": "kGVQAWnh25SK",
        "outputId": "7aadb5ca-bad4-4081-ceea-74ed401a44c9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57090cc6f44b443da9a9b62a9598b117",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[23:46:01] INFO     Starting finetuner training run ...                                                  __main__.py:350\n",
            "DEBUG    Found Jina AI Cloud authentication token                                             __main__.py:362\n",
            "DEBUG    Running in online mode                                                               __main__.py:363\n",
            "INFO     Reading config ...                                                                   __main__.py:370\n",
            "DEBUG    Reading config from stream                                                           __main__.py:382\n",
            "INFO     Parsing config ...                                                                   __main__.py:385\n",
            "INFO     Config loaded 📜                                                                     __main__.py:389\n",
            "INFO     Run name: clip-run                                                                   __main__.py:391\n",
            "INFO     Experiment name: default                                                             __main__.py:392\n",
            "DEBUG    Device set to [cuda]                                                                 __main__.py:395\n",
            "DEBUG    Artifact ID set to 64a7509741beb2b5c3c260f9                                          __main__.py:141\n",
            "DEBUG    Artifact ID for metrics set to 64a7509841beb2b5c3c260fa                              __main__.py:144\n",
            "DEBUG    Artifact ID for example results set to 64a7509841beb2b5c3c260fb                      __main__.py:147\n",
            "INFO     Building the tuner components ...                                                    __main__.py:161\n",
            "INFO     Building models ...                                                                    tuning.py:326\n",
            "DEBUG    Model name: openai/clip-vit-base-patch32                                               tuning.py:328\n",
            "DEBUG    Model options: {}                                                                      tuning.py:329\n",
            "Downloading:   0%|          | 0.00/4.09k [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 4.09k/4.09k [00:00<00:00, 27.7MB/s]\n",
            "Downloading:   0%|          | 0.00/577M [00:00<?, ?B/s]\n",
            "Downloading:   1%|          | 6.90M/577M [00:00<00:08, 72.4MB/s]\n",
            "Downloading:   2%|▏         | 14.2M/577M [00:00<00:07, 75.0MB/s]\n",
            "Downloading:   4%|▎         | 21.5M/577M [00:00<00:07, 75.8MB/s]\n",
            "Downloading:   5%|▍         | 28.8M/577M [00:00<00:07, 76.1MB/s]\n",
            "Downloading:   6%|▋         | 36.1M/577M [00:00<00:07, 76.2MB/s]\n",
            "Downloading:   8%|▊         | 43.5M/577M [00:00<00:07, 76.4MB/s]\n",
            "Downloading:   9%|▉         | 50.8M/577M [00:00<00:07, 76.5MB/s]\n",
            "Downloading:  10%|█         | 58.1M/577M [00:00<00:07, 76.6MB/s]\n",
            "Downloading:  11%|█▏        | 65.4M/577M [00:00<00:07, 76.6MB/s]\n",
            "Downloading:  13%|█▎        | 72.7M/577M [00:01<00:06, 76.6MB/s]\n",
            "Downloading:  14%|█▍        | 80.0M/577M [00:01<00:06, 76.7MB/s]\n",
            "Downloading:  15%|█▌        | 87.4M/577M [00:01<00:06, 76.7MB/s]\n",
            "Downloading:  16%|█▋        | 94.7M/577M [00:01<00:06, 76.2MB/s]\n",
            "Downloading:  18%|█▊        | 102M/577M [00:01<00:06, 76.1MB/s] \n",
            "Downloading:  19%|█▉        | 109M/577M [00:01<00:06, 76.0MB/s]\n",
            "Downloading:  20%|██        | 116M/577M [00:01<00:06, 76.0MB/s]\n",
            "Downloading:  21%|██▏       | 124M/577M [00:01<00:06, 75.9MB/s]\n",
            "Downloading:  23%|██▎       | 131M/577M [00:01<00:06, 75.7MB/s]\n",
            "Downloading:  24%|██▍       | 138M/577M [00:01<00:06, 75.8MB/s]\n",
            "Downloading:  25%|██▌       | 145M/577M [00:02<00:05, 75.8MB/s]\n",
            "Downloading:  26%|██▋       | 153M/577M [00:02<00:05, 76.0MB/s]\n",
            "Downloading:  28%|██▊       | 160M/577M [00:02<00:05, 76.0MB/s]\n",
            "Downloading:  29%|██▉       | 167M/577M [00:02<00:05, 76.0MB/s]\n",
            "Downloading:  30%|███       | 174M/577M [00:02<00:05, 76.1MB/s]\n",
            "Downloading:  31%|███▏      | 182M/577M [00:02<00:05, 76.1MB/s]\n",
            "Downloading:  33%|███▎      | 189M/577M [00:02<00:05, 76.1MB/s]\n",
            "Downloading:  34%|███▍      | 196M/577M [00:02<00:05, 76.2MB/s]\n",
            "Downloading:  35%|███▌      | 204M/577M [00:02<00:05, 76.2MB/s]\n",
            "Downloading:  37%|███▋      | 211M/577M [00:02<00:05, 75.3MB/s]\n",
            "Downloading:  38%|███▊      | 218M/577M [00:03<00:05, 75.2MB/s]\n",
            "Downloading:  39%|███▉      | 225M/577M [00:03<00:04, 74.9MB/s]\n",
            "Downloading:  40%|████      | 232M/577M [00:03<00:04, 74.9MB/s]\n",
            "Downloading:  41%|████▏     | 239M/577M [00:03<00:04, 74.7MB/s]\n",
            "Downloading:  43%|████▎     | 247M/577M [00:03<00:04, 74.2MB/s]\n",
            "Downloading:  44%|████▍     | 254M/577M [00:03<00:04, 73.9MB/s]\n",
            "Downloading:  45%|████▌     | 261M/577M [00:03<00:04, 74.0MB/s]\n",
            "Downloading:  46%|████▋     | 268M/577M [00:03<00:04, 74.3MB/s]\n",
            "Downloading:  48%|████▊     | 275M/577M [00:03<00:04, 74.6MB/s]\n",
            "Downloading:  49%|████▉     | 282M/577M [00:03<00:04, 74.7MB/s]\n",
            "Downloading:  50%|█████     | 289M/577M [00:04<00:04, 74.9MB/s]\n",
            "Downloading:  51%|█████▏    | 297M/577M [00:04<00:03, 75.0MB/s]\n",
            "Downloading:  53%|█████▎    | 304M/577M [00:04<00:03, 75.0MB/s]\n",
            "Downloading:  54%|█████▍    | 311M/577M [00:04<00:03, 75.1MB/s]\n",
            "Downloading:  55%|█████▌    | 318M/577M [00:04<00:03, 75.2MB/s]\n",
            "Downloading:  56%|█████▋    | 325M/577M [00:04<00:03, 75.2MB/s]\n",
            "Downloading:  58%|█████▊    | 333M/577M [00:04<00:03, 75.3MB/s]\n",
            "Downloading:  59%|█████▉    | 340M/577M [00:04<00:03, 75.4MB/s]\n",
            "Downloading:  60%|██████    | 347M/577M [00:04<00:03, 75.6MB/s]\n",
            "Downloading:  61%|██████▏   | 354M/577M [00:04<00:03, 75.5MB/s]\n",
            "Downloading:  63%|██████▎   | 361M/577M [00:05<00:03, 75.3MB/s]\n",
            "Downloading:  64%|██████▍   | 369M/577M [00:05<00:02, 75.5MB/s]\n",
            "Downloading:  65%|██████▌   | 376M/577M [00:05<00:02, 75.7MB/s]\n",
            "Downloading:  66%|██████▋   | 383M/577M [00:05<00:02, 75.7MB/s]\n",
            "Downloading:  68%|██████▊   | 390M/577M [00:05<00:02, 75.0MB/s]\n",
            "Downloading:  69%|██████▉   | 398M/577M [00:05<00:02, 75.0MB/s]\n",
            "Downloading:  70%|███████   | 405M/577M [00:05<00:02, 75.4MB/s]\n",
            "Downloading:  71%|███████▏  | 412M/577M [00:05<00:02, 74.5MB/s]\n",
            "Downloading:  73%|███████▎  | 419M/577M [00:05<00:02, 74.4MB/s]\n",
            "Downloading:  74%|███████▍  | 426M/577M [00:05<00:02, 74.7MB/s]\n",
            "Downloading:  75%|███████▌  | 433M/577M [00:06<00:02, 74.7MB/s]\n",
            "Downloading:  76%|███████▋  | 441M/577M [00:06<00:01, 74.7MB/s]\n",
            "Downloading:  78%|███████▊  | 448M/577M [00:06<00:01, 74.9MB/s]\n",
            "Downloading:  79%|███████▉  | 455M/577M [00:06<00:01, 75.1MB/s]\n",
            "Downloading:  80%|████████  | 462M/577M [00:06<00:01, 75.3MB/s]\n",
            "Downloading:  81%|████████▏ | 469M/577M [00:06<00:01, 75.2MB/s]\n",
            "Downloading:  83%|████████▎ | 477M/577M [00:06<00:01, 75.4MB/s]\n",
            "Downloading:  84%|████████▍ | 484M/577M [00:06<00:01, 75.7MB/s]\n",
            "Downloading:  85%|████████▌ | 491M/577M [00:06<00:01, 75.7MB/s]\n",
            "Downloading:  86%|████████▋ | 498M/577M [00:06<00:01, 75.6MB/s]\n",
            "Downloading:  88%|████████▊ | 506M/577M [00:07<00:00, 75.4MB/s]\n",
            "Downloading:  89%|████████▉ | 513M/577M [00:07<00:00, 75.4MB/s]\n",
            "Downloading:  90%|█████████ | 520M/577M [00:07<00:00, 74.7MB/s]\n",
            "Downloading:  91%|█████████▏| 527M/577M [00:07<00:00, 75.1MB/s]\n",
            "Downloading:  93%|█████████▎| 534M/577M [00:07<00:00, 75.3MB/s]\n",
            "Downloading:  94%|█████████▍| 542M/577M [00:07<00:00, 74.8MB/s]\n",
            "Downloading:  95%|█████████▌| 549M/577M [00:07<00:00, 74.4MB/s]\n",
            "Downloading:  96%|█████████▋| 556M/577M [00:07<00:00, 74.5MB/s]\n",
            "Downloading:  98%|█████████▊| 563M/577M [00:07<00:00, 74.8MB/s]\n",
            "Downloading:  99%|█████████▉| 570M/577M [00:07<00:00, 75.2MB/s]\n",
            "Downloading: 100%|██████████| 577M/577M [00:08<00:00, 75.4MB/s]\n",
            "Downloading:   0%|          | 0.00/568 [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 568/568 [00:00<00:00, 4.76MB/s]\n",
            "Downloading:   0%|          | 0.00/842k [00:00<?, ?B/s]\n",
            "Downloading:  31%|███       | 257k/842k [00:00<00:00, 1.98MB/s]\n",
            "Downloading:  72%|███████▏  | 609k/842k [00:00<00:00, 2.38MB/s]\n",
            "Downloading: 100%|██████████| 842k/842k [00:00<00:00, 3.17MB/s]\n",
            "Downloading:   0%|          | 0.00/512k [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 512k/512k [00:00<00:00, 39.3MB/s]\n",
            "Downloading:   0%|          | 0.00/2.12M [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 2.12M/2.12M [00:00<00:00, 72.4MB/s]\n",
            "Downloading:   0%|          | 0.00/389 [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 389/389 [00:00<00:00, 2.91MB/s]\n",
            "Downloading:   0%|          | 0.00/316 [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 316/316 [00:00<00:00, 3.40MB/s]\n",
            "[23:46:15] INFO     Loading the training data ...                                                          tuning.py:158\n",
            "INFO     Pulling data from cloud storage ...                                                      utils.py:80\n",
            "🔐 You are logged in to Jina AI as zhouxuanang63 (username: zhouxuanang63). To log out, use jina auth logout.\n",
            "ping\n",
            "2023-07-06 23:46:28.541004\n",
            "ping\n",
            "2023-07-06 23:46:43.541056\n",
            "ping\n",
            "2023-07-06 23:46:58.541350\n",
            "ping\n",
            "2023-07-06 23:47:13.542220\n",
            "Deserializing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4376/4376 • 52 QPS • 0:00:00 • 545.2 MB\n",
            "[23:47:18] DEBUG    # training samples: 4376                                                               tuning.py:171\n",
            "INFO     Building loss ...                                                                      tuning.py:431\n",
            "DEBUG    Loss name: CLIPLoss                                                                    tuning.py:432\n",
            "DEBUG    Loss options: {}                                                                       tuning.py:433\n",
            "INFO     Building optimizer ...                                                                 tuning.py:538\n",
            "DEBUG    Optimizer name: Adam                                                                   tuning.py:539\n",
            "DEBUG    Optimizer options: {'lr': 1e-06}                                                       tuning.py:540\n",
            "INFO     Model, data, callbacks, loss, miner and optimizer built successfully 🔥              __main__.py:165\n",
            "INFO     Finetuning ...                                                                       __main__.py:170\n",
            "ping\n",
            "2023-07-06 23:47:28.542310\n",
            "ping\n",
            "2023-07-06 23:47:43.542654\n",
            "ping\n",
            "2023-07-06 23:47:58.542708\n",
            "ping\n",
            "2023-07-06 23:48:13.542315\n",
            "ping\n",
            "2023-07-06 23:48:28.542706\n",
            "ping\n",
            "2023-07-06 23:48:43.542371\n",
            "ping\n",
            "2023-07-06 23:48:58.542386\n",
            "ping\n",
            "2023-07-06 23:49:13.542652\n",
            "Training [5/5] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 136/136 0:00:00 0:00:21 • loss: 0.200\n",
            "[23:49:12] INFO     Done ✨                                                                              __main__.py:192\n",
            "DEBUG    Finetuning took 0 days, 0 hours 1 minutes and 53 seconds                             __main__.py:194\n",
            "INFO     Building the artifact ...                                                            __main__.py:231\n",
            "INFO     Pushing artifact to Jina AI Cloud ...                                                __main__.py:260\n",
            "ping\n",
            "2023-07-06 23:49:28.542717\n",
            "ping\n",
            "2023-07-06 23:49:43.542391\n",
            "ping\n",
            "2023-07-06 23:49:58.542719\n",
            "ping\n",
            "2023-07-06 23:50:13.543141\n",
            "ping\n",
            "2023-07-06 23:50:28.543352\n",
            "ping\n",
            "2023-07-06 23:50:43.543717\n",
            "ping\n",
            "2023-07-06 23:50:58.543574\n",
            "[23:51:00] INFO     Artifact pushed under ID '64a7509741beb2b5c3c260f9'                                  __main__.py:266\n",
            "DEBUG    Artifact size is 535.390 MB                                                          __main__.py:268\n",
            "INFO     Finished 🚀                                                                          __main__.py:415\n",
            "time=\"2023-07-06T23:51:02.709Z\" level=info msg=\"sub-process exited\" argo=true error=\"<nil>\"\n",
            "ping\n",
            "2023-07-06 23:51:13.544182\n"
          ]
        }
      ],
      "source": [
        "# takes around ~10 minutes to finish\n",
        "for log_entry in run.stream_logs():\n",
        "    print(log_entry)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV5tKv0sUoBa"
      },
      "source": [
        "## Inference\n",
        "\n",
        "After fine-tuning is finished, your fine-tuned model is saved in the cloud as an `artifact`. An `artifact` contains the model weights, and some metadata such as evaluation metrics and hyper-parameters.\n",
        "\n",
        "In order to download your artifact, call the method `run.save_artifact()`.\n",
        "\n",
        "Since CLIP is actually two models and we are fine-tuning them in parallel, there will be two models downloaded as one artifact: a text encoder and an image encoder. To use these models to do encodings, you will need the `finetuner.get_model()` with a `select_model` -- either `clip-text` or `clip-vision` -- get access to CLIPs constituent models individually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35,
          "referenced_widgets": [
            "da795a9e9c8c47c489b520483c95713a",
            "f33b28cfdc71491baa49f843b1f51389"
          ]
        },
        "id": "Do1nK3sY2OS7",
        "outputId": "50a6b94b-335b-4427-dcfd-9a74e8787b60"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3756d13920624c269bc5a32b54f66988",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/xz306/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:146: UserWarning: \n",
            "NVIDIA GeForce RTX 3080 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
            "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
            "If you want to use the NVIDIA GeForce RTX 3080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
            "\n",
            "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
          ]
        }
      ],
      "source": [
        "artifact = run.save_artifact('clip-model')\n",
        "\n",
        "clip_txt_encoder = finetuner.get_model(artifact=artifact, select_model='clip-text')\n",
        "clip_img_encoder = finetuner.get_model(artifact=artifact, select_model='clip-vision')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnlCnAiPZLjm"
      },
      "source": [
        "With these two models and Finetuner, you can encode your image and text data with:\n",
        "\n",
        "```python\n",
        "data = DocumentArray([Document(content='some text to encode')])\n",
        "finetuner.encode(model=clip_txt_encoder, data=data)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRDeIQRe2aiG"
      },
      "source": [
        "In order to use CLIP Benchmark, we must provide a PyTorch file rather than a Finetuner inference runtime. The code below is a hack to overcome this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!unzip clip-model/clip-run.zip # as said, artifact are saved as zip together with weights and some metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q503pCw4bS4E",
        "outputId": "a2c85428-bfe5-4c87-8e2a-46918a509f4d"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for OpenCLIPVisionModel:\n\tMissing key(s) in state_dict: \"_model.positional_embedding\", \"_model.text_projection\", \"_model.logit_scale\", \"_model.visual.class_embedding\", \"_model.visual.positional_embedding\", \"_model.visual.proj\", \"_model.visual.conv1.weight\", \"_model.visual.ln_pre.weight\", \"_model.visual.ln_pre.bias\", \"_model.visual.transformer.resblocks.0.ln_1.weight\", \"_model.visual.transformer.resblocks.0.ln_1.bias\", \"_model.visual.transformer.resblocks.0.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.0.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.0.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.0.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.0.ln_2.weight\", \"_model.visual.transformer.resblocks.0.ln_2.bias\", \"_model.visual.transformer.resblocks.0.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.0.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.0.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.0.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.1.ln_1.weight\", \"_model.visual.transformer.resblocks.1.ln_1.bias\", \"_model.visual.transformer.resblocks.1.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.1.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.1.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.1.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.1.ln_2.weight\", \"_model.visual.transformer.resblocks.1.ln_2.bias\", \"_model.visual.transformer.resblocks.1.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.1.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.1.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.1.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.2.ln_1.weight\", \"_model.visual.transformer.resblocks.2.ln_1.bias\", \"_model.visual.transformer.resblocks.2.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.2.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.2.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.2.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.2.ln_2.weight\", \"_model.visual.transformer.resblocks.2.ln_2.bias\", \"_model.visual.transformer.resblocks.2.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.2.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.2.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.2.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.3.ln_1.weight\", \"_model.visual.transformer.resblocks.3.ln_1.bias\", \"_model.visual.transformer.resblocks.3.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.3.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.3.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.3.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.3.ln_2.weight\", \"_model.visual.transformer.resblocks.3.ln_2.bias\", \"_model.visual.transformer.resblocks.3.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.3.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.3.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.3.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.4.ln_1.weight\", \"_model.visual.transformer.resblocks.4.ln_1.bias\", \"_model.visual.transformer.resblocks.4.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.4.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.4.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.4.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.4.ln_2.weight\", \"_model.visual.transformer.resblocks.4.ln_2.bias\", \"_model.visual.transformer.resblocks.4.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.4.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.4.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.4.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.5.ln_1.weight\", \"_model.visual.transformer.resblocks.5.ln_1.bias\", \"_model.visual.transformer.resblocks.5.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.5.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.5.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.5.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.5.ln_2.weight\", \"_model.visual.transformer.resblocks.5.ln_2.bias\", \"_model.visual.transformer.resblocks.5.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.5.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.5.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.5.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.6.ln_1.weight\", \"_model.visual.transformer.resblocks.6.ln_1.bias\", \"_model.visual.transformer.resblocks.6.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.6.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.6.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.6.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.6.ln_2.weight\", \"_model.visual.transformer.resblocks.6.ln_2.bias\", \"_model.visual.transformer.resblocks.6.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.6.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.6.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.6.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.7.ln_1.weight\", \"_model.visual.transformer.resblocks.7.ln_1.bias\", \"_model.visual.transformer.resblocks.7.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.7.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.7.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.7.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.7.ln_2.weight\", \"_model.visual.transformer.resblocks.7.ln_2.bias\", \"_model.visual.transformer.resblocks.7.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.7.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.7.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.7.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.8.ln_1.weight\", \"_model.visual.transformer.resblocks.8.ln_1.bias\", \"_model.visual.transformer.resblocks.8.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.8.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.8.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.8.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.8.ln_2.weight\", \"_model.visual.transformer.resblocks.8.ln_2.bias\", \"_model.visual.transformer.resblocks.8.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.8.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.8.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.8.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.9.ln_1.weight\", \"_model.visual.transformer.resblocks.9.ln_1.bias\", \"_model.visual.transformer.resblocks.9.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.9.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.9.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.9.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.9.ln_2.weight\", \"_model.visual.transformer.resblocks.9.ln_2.bias\", \"_model.visual.transformer.resblocks.9.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.9.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.9.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.9.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.10.ln_1.weight\", \"_model.visual.transformer.resblocks.10.ln_1.bias\", \"_model.visual.transformer.resblocks.10.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.10.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.10.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.10.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.10.ln_2.weight\", \"_model.visual.transformer.resblocks.10.ln_2.bias\", \"_model.visual.transformer.resblocks.10.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.10.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.10.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.10.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.11.ln_1.weight\", \"_model.visual.transformer.resblocks.11.ln_1.bias\", \"_model.visual.transformer.resblocks.11.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.11.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.11.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.11.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.11.ln_2.weight\", \"_model.visual.transformer.resblocks.11.ln_2.bias\", \"_model.visual.transformer.resblocks.11.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.11.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.11.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.11.mlp.c_proj.bias\", \"_model.visual.ln_post.weight\", \"_model.visual.ln_post.bias\", \"_model.transformer.resblocks.0.ln_1.weight\", \"_model.transformer.resblocks.0.ln_1.bias\", \"_model.transformer.resblocks.0.attn.in_proj_weight\", \"_model.transformer.resblocks.0.attn.in_proj_bias\", \"_model.transformer.resblocks.0.attn.out_proj.weight\", \"_model.transformer.resblocks.0.attn.out_proj.bias\", \"_model.transformer.resblocks.0.ln_2.weight\", \"_model.transformer.resblocks.0.ln_2.bias\", \"_model.transformer.resblocks.0.mlp.c_fc.weight\", \"_model.transformer.resblocks.0.mlp.c_fc.bias\", \"_model.transformer.resblocks.0.mlp.c_proj.weight\", \"_model.transformer.resblocks.0.mlp.c_proj.bias\", \"_model.transformer.resblocks.1.ln_1.weight\", \"_model.transformer.resblocks.1.ln_1.bias\", \"_model.transformer.resblocks.1.attn.in_proj_weight\", \"_model.transformer.resblocks.1.attn.in_proj_bias\", \"_model.transformer.resblocks.1.attn.out_proj.weight\", \"_model.transformer.resblocks.1.attn.out_proj.bias\", \"_model.transformer.resblocks.1.ln_2.weight\", \"_model.transformer.resblocks.1.ln_2.bias\", \"_model.transformer.resblocks.1.mlp.c_fc.weight\", \"_model.transformer.resblocks.1.mlp.c_fc.bias\", \"_model.transformer.resblocks.1.mlp.c_proj.weight\", \"_model.transformer.resblocks.1.mlp.c_proj.bias\", \"_model.transformer.resblocks.2.ln_1.weight\", \"_model.transformer.resblocks.2.ln_1.bias\", \"_model.transformer.resblocks.2.attn.in_proj_weight\", \"_model.transformer.resblocks.2.attn.in_proj_bias\", \"_model.transformer.resblocks.2.attn.out_proj.weight\", \"_model.transformer.resblocks.2.attn.out_proj.bias\", \"_model.transformer.resblocks.2.ln_2.weight\", \"_model.transformer.resblocks.2.ln_2.bias\", \"_model.transformer.resblocks.2.mlp.c_fc.weight\", \"_model.transformer.resblocks.2.mlp.c_fc.bias\", \"_model.transformer.resblocks.2.mlp.c_proj.weight\", \"_model.transformer.resblocks.2.mlp.c_proj.bias\", \"_model.transformer.resblocks.3.ln_1.weight\", \"_model.transformer.resblocks.3.ln_1.bias\", \"_model.transformer.resblocks.3.attn.in_proj_weight\", \"_model.transformer.resblocks.3.attn.in_proj_bias\", \"_model.transformer.resblocks.3.attn.out_proj.weight\", \"_model.transformer.resblocks.3.attn.out_proj.bias\", \"_model.transformer.resblocks.3.ln_2.weight\", \"_model.transformer.resblocks.3.ln_2.bias\", \"_model.transformer.resblocks.3.mlp.c_fc.weight\", \"_model.transformer.resblocks.3.mlp.c_fc.bias\", \"_model.transformer.resblocks.3.mlp.c_proj.weight\", \"_model.transformer.resblocks.3.mlp.c_proj.bias\", \"_model.transformer.resblocks.4.ln_1.weight\", \"_model.transformer.resblocks.4.ln_1.bias\", \"_model.transformer.resblocks.4.attn.in_proj_weight\", \"_model.transformer.resblocks.4.attn.in_proj_bias\", \"_model.transformer.resblocks.4.attn.out_proj.weight\", \"_model.transformer.resblocks.4.attn.out_proj.bias\", \"_model.transformer.resblocks.4.ln_2.weight\", \"_model.transformer.resblocks.4.ln_2.bias\", \"_model.transformer.resblocks.4.mlp.c_fc.weight\", \"_model.transformer.resblocks.4.mlp.c_fc.bias\", \"_model.transformer.resblocks.4.mlp.c_proj.weight\", \"_model.transformer.resblocks.4.mlp.c_proj.bias\", \"_model.transformer.resblocks.5.ln_1.weight\", \"_model.transformer.resblocks.5.ln_1.bias\", \"_model.transformer.resblocks.5.attn.in_proj_weight\", \"_model.transformer.resblocks.5.attn.in_proj_bias\", \"_model.transformer.resblocks.5.attn.out_proj.weight\", \"_model.transformer.resblocks.5.attn.out_proj.bias\", \"_model.transformer.resblocks.5.ln_2.weight\", \"_model.transformer.resblocks.5.ln_2.bias\", \"_model.transformer.resblocks.5.mlp.c_fc.weight\", \"_model.transformer.resblocks.5.mlp.c_fc.bias\", \"_model.transformer.resblocks.5.mlp.c_proj.weight\", \"_model.transformer.resblocks.5.mlp.c_proj.bias\", \"_model.transformer.resblocks.6.ln_1.weight\", \"_model.transformer.resblocks.6.ln_1.bias\", \"_model.transformer.resblocks.6.attn.in_proj_weight\", \"_model.transformer.resblocks.6.attn.in_proj_bias\", \"_model.transformer.resblocks.6.attn.out_proj.weight\", \"_model.transformer.resblocks.6.attn.out_proj.bias\", \"_model.transformer.resblocks.6.ln_2.weight\", \"_model.transformer.resblocks.6.ln_2.bias\", \"_model.transformer.resblocks.6.mlp.c_fc.weight\", \"_model.transformer.resblocks.6.mlp.c_fc.bias\", \"_model.transformer.resblocks.6.mlp.c_proj.weight\", \"_model.transformer.resblocks.6.mlp.c_proj.bias\", \"_model.transformer.resblocks.7.ln_1.weight\", \"_model.transformer.resblocks.7.ln_1.bias\", \"_model.transformer.resblocks.7.attn.in_proj_weight\", \"_model.transformer.resblocks.7.attn.in_proj_bias\", \"_model.transformer.resblocks.7.attn.out_proj.weight\", \"_model.transformer.resblocks.7.attn.out_proj.bias\", \"_model.transformer.resblocks.7.ln_2.weight\", \"_model.transformer.resblocks.7.ln_2.bias\", \"_model.transformer.resblocks.7.mlp.c_fc.weight\", \"_model.transformer.resblocks.7.mlp.c_fc.bias\", \"_model.transformer.resblocks.7.mlp.c_proj.weight\", \"_model.transformer.resblocks.7.mlp.c_proj.bias\", \"_model.transformer.resblocks.8.ln_1.weight\", \"_model.transformer.resblocks.8.ln_1.bias\", \"_model.transformer.resblocks.8.attn.in_proj_weight\", \"_model.transformer.resblocks.8.attn.in_proj_bias\", \"_model.transformer.resblocks.8.attn.out_proj.weight\", \"_model.transformer.resblocks.8.attn.out_proj.bias\", \"_model.transformer.resblocks.8.ln_2.weight\", \"_model.transformer.resblocks.8.ln_2.bias\", \"_model.transformer.resblocks.8.mlp.c_fc.weight\", \"_model.transformer.resblocks.8.mlp.c_fc.bias\", \"_model.transformer.resblocks.8.mlp.c_proj.weight\", \"_model.transformer.resblocks.8.mlp.c_proj.bias\", \"_model.transformer.resblocks.9.ln_1.weight\", \"_model.transformer.resblocks.9.ln_1.bias\", \"_model.transformer.resblocks.9.attn.in_proj_weight\", \"_model.transformer.resblocks.9.attn.in_proj_bias\", \"_model.transformer.resblocks.9.attn.out_proj.weight\", \"_model.transformer.resblocks.9.attn.out_proj.bias\", \"_model.transformer.resblocks.9.ln_2.weight\", \"_model.transformer.resblocks.9.ln_2.bias\", \"_model.transformer.resblocks.9.mlp.c_fc.weight\", \"_model.transformer.resblocks.9.mlp.c_fc.bias\", \"_model.transformer.resblocks.9.mlp.c_proj.weight\", \"_model.transformer.resblocks.9.mlp.c_proj.bias\", \"_model.transformer.resblocks.10.ln_1.weight\", \"_model.transformer.resblocks.10.ln_1.bias\", \"_model.transformer.resblocks.10.attn.in_proj_weight\", \"_model.transformer.resblocks.10.attn.in_proj_bias\", \"_model.transformer.resblocks.10.attn.out_proj.weight\", \"_model.transformer.resblocks.10.attn.out_proj.bias\", \"_model.transformer.resblocks.10.ln_2.weight\", \"_model.transformer.resblocks.10.ln_2.bias\", \"_model.transformer.resblocks.10.mlp.c_fc.weight\", \"_model.transformer.resblocks.10.mlp.c_fc.bias\", \"_model.transformer.resblocks.10.mlp.c_proj.weight\", \"_model.transformer.resblocks.10.mlp.c_proj.bias\", \"_model.transformer.resblocks.11.ln_1.weight\", \"_model.transformer.resblocks.11.ln_1.bias\", \"_model.transformer.resblocks.11.attn.in_proj_weight\", \"_model.transformer.resblocks.11.attn.in_proj_bias\", \"_model.transformer.resblocks.11.attn.out_proj.weight\", \"_model.transformer.resblocks.11.attn.out_proj.bias\", \"_model.transformer.resblocks.11.ln_2.weight\", \"_model.transformer.resblocks.11.ln_2.bias\", \"_model.transformer.resblocks.11.mlp.c_fc.weight\", \"_model.transformer.resblocks.11.mlp.c_fc.bias\", \"_model.transformer.resblocks.11.mlp.c_proj.weight\", \"_model.transformer.resblocks.11.mlp.c_proj.bias\", \"_model.token_embedding.weight\", \"_model.ln_final.weight\", \"_model.ln_final.bias\". \n\tUnexpected key(s) in state_dict: \"_projection.weight\", \"_model.embeddings.class_embedding\", \"_model.embeddings.position_ids\", \"_model.embeddings.patch_embedding.weight\", \"_model.embeddings.position_embedding.weight\", \"_model.pre_layrnorm.weight\", \"_model.pre_layrnorm.bias\", \"_model.encoder.layers.0.self_attn.k_proj.weight\", \"_model.encoder.layers.0.self_attn.k_proj.bias\", \"_model.encoder.layers.0.self_attn.v_proj.weight\", \"_model.encoder.layers.0.self_attn.v_proj.bias\", \"_model.encoder.layers.0.self_attn.q_proj.weight\", \"_model.encoder.layers.0.self_attn.q_proj.bias\", \"_model.encoder.layers.0.self_attn.out_proj.weight\", \"_model.encoder.layers.0.self_attn.out_proj.bias\", \"_model.encoder.layers.0.layer_norm1.weight\", \"_model.encoder.layers.0.layer_norm1.bias\", \"_model.encoder.layers.0.mlp.fc1.weight\", \"_model.encoder.layers.0.mlp.fc1.bias\", \"_model.encoder.layers.0.mlp.fc2.weight\", \"_model.encoder.layers.0.mlp.fc2.bias\", \"_model.encoder.layers.0.layer_norm2.weight\", \"_model.encoder.layers.0.layer_norm2.bias\", \"_model.encoder.layers.1.self_attn.k_proj.weight\", \"_model.encoder.layers.1.self_attn.k_proj.bias\", \"_model.encoder.layers.1.self_attn.v_proj.weight\", \"_model.encoder.layers.1.self_attn.v_proj.bias\", \"_model.encoder.layers.1.self_attn.q_proj.weight\", \"_model.encoder.layers.1.self_attn.q_proj.bias\", \"_model.encoder.layers.1.self_attn.out_proj.weight\", \"_model.encoder.layers.1.self_attn.out_proj.bias\", \"_model.encoder.layers.1.layer_norm1.weight\", \"_model.encoder.layers.1.layer_norm1.bias\", \"_model.encoder.layers.1.mlp.fc1.weight\", \"_model.encoder.layers.1.mlp.fc1.bias\", \"_model.encoder.layers.1.mlp.fc2.weight\", \"_model.encoder.layers.1.mlp.fc2.bias\", \"_model.encoder.layers.1.layer_norm2.weight\", \"_model.encoder.layers.1.layer_norm2.bias\", \"_model.encoder.layers.2.self_attn.k_proj.weight\", \"_model.encoder.layers.2.self_attn.k_proj.bias\", \"_model.encoder.layers.2.self_attn.v_proj.weight\", \"_model.encoder.layers.2.self_attn.v_proj.bias\", \"_model.encoder.layers.2.self_attn.q_proj.weight\", \"_model.encoder.layers.2.self_attn.q_proj.bias\", \"_model.encoder.layers.2.self_attn.out_proj.weight\", \"_model.encoder.layers.2.self_attn.out_proj.bias\", \"_model.encoder.layers.2.layer_norm1.weight\", \"_model.encoder.layers.2.layer_norm1.bias\", \"_model.encoder.layers.2.mlp.fc1.weight\", \"_model.encoder.layers.2.mlp.fc1.bias\", \"_model.encoder.layers.2.mlp.fc2.weight\", \"_model.encoder.layers.2.mlp.fc2.bias\", \"_model.encoder.layers.2.layer_norm2.weight\", \"_model.encoder.layers.2.layer_norm2.bias\", \"_model.encoder.layers.3.self_attn.k_proj.weight\", \"_model.encoder.layers.3.self_attn.k_proj.bias\", \"_model.encoder.layers.3.self_attn.v_proj.weight\", \"_model.encoder.layers.3.self_attn.v_proj.bias\", \"_model.encoder.layers.3.self_attn.q_proj.weight\", \"_model.encoder.layers.3.self_attn.q_proj.bias\", \"_model.encoder.layers.3.self_attn.out_proj.weight\", \"_model.encoder.layers.3.self_attn.out_proj.bias\", \"_model.encoder.layers.3.layer_norm1.weight\", \"_model.encoder.layers.3.layer_norm1.bias\", \"_model.encoder.layers.3.mlp.fc1.weight\", \"_model.encoder.layers.3.mlp.fc1.bias\", \"_model.encoder.layers.3.mlp.fc2.weight\", \"_model.encoder.layers.3.mlp.fc2.bias\", \"_model.encoder.layers.3.layer_norm2.weight\", \"_model.encoder.layers.3.layer_norm2.bias\", \"_model.encoder.layers.4.self_attn.k_proj.weight\", \"_model.encoder.layers.4.self_attn.k_proj.bias\", \"_model.encoder.layers.4.self_attn.v_proj.weight\", \"_model.encoder.layers.4.self_attn.v_proj.bias\", \"_model.encoder.layers.4.self_attn.q_proj.weight\", \"_model.encoder.layers.4.self_attn.q_proj.bias\", \"_model.encoder.layers.4.self_attn.out_proj.weight\", \"_model.encoder.layers.4.self_attn.out_proj.bias\", \"_model.encoder.layers.4.layer_norm1.weight\", \"_model.encoder.layers.4.layer_norm1.bias\", \"_model.encoder.layers.4.mlp.fc1.weight\", \"_model.encoder.layers.4.mlp.fc1.bias\", \"_model.encoder.layers.4.mlp.fc2.weight\", \"_model.encoder.layers.4.mlp.fc2.bias\", \"_model.encoder.layers.4.layer_norm2.weight\", \"_model.encoder.layers.4.layer_norm2.bias\", \"_model.encoder.layers.5.self_attn.k_proj.weight\", \"_model.encoder.layers.5.self_attn.k_proj.bias\", \"_model.encoder.layers.5.self_attn.v_proj.weight\", \"_model.encoder.layers.5.self_attn.v_proj.bias\", \"_model.encoder.layers.5.self_attn.q_proj.weight\", \"_model.encoder.layers.5.self_attn.q_proj.bias\", \"_model.encoder.layers.5.self_attn.out_proj.weight\", \"_model.encoder.layers.5.self_attn.out_proj.bias\", \"_model.encoder.layers.5.layer_norm1.weight\", \"_model.encoder.layers.5.layer_norm1.bias\", \"_model.encoder.layers.5.mlp.fc1.weight\", \"_model.encoder.layers.5.mlp.fc1.bias\", \"_model.encoder.layers.5.mlp.fc2.weight\", \"_model.encoder.layers.5.mlp.fc2.bias\", \"_model.encoder.layers.5.layer_norm2.weight\", \"_model.encoder.layers.5.layer_norm2.bias\", \"_model.encoder.layers.6.self_attn.k_proj.weight\", \"_model.encoder.layers.6.self_attn.k_proj.bias\", \"_model.encoder.layers.6.self_attn.v_proj.weight\", \"_model.encoder.layers.6.self_attn.v_proj.bias\", \"_model.encoder.layers.6.self_attn.q_proj.weight\", \"_model.encoder.layers.6.self_attn.q_proj.bias\", \"_model.encoder.layers.6.self_attn.out_proj.weight\", \"_model.encoder.layers.6.self_attn.out_proj.bias\", \"_model.encoder.layers.6.layer_norm1.weight\", \"_model.encoder.layers.6.layer_norm1.bias\", \"_model.encoder.layers.6.mlp.fc1.weight\", \"_model.encoder.layers.6.mlp.fc1.bias\", \"_model.encoder.layers.6.mlp.fc2.weight\", \"_model.encoder.layers.6.mlp.fc2.bias\", \"_model.encoder.layers.6.layer_norm2.weight\", \"_model.encoder.layers.6.layer_norm2.bias\", \"_model.encoder.layers.7.self_attn.k_proj.weight\", \"_model.encoder.layers.7.self_attn.k_proj.bias\", \"_model.encoder.layers.7.self_attn.v_proj.weight\", \"_model.encoder.layers.7.self_attn.v_proj.bias\", \"_model.encoder.layers.7.self_attn.q_proj.weight\", \"_model.encoder.layers.7.self_attn.q_proj.bias\", \"_model.encoder.layers.7.self_attn.out_proj.weight\", \"_model.encoder.layers.7.self_attn.out_proj.bias\", \"_model.encoder.layers.7.layer_norm1.weight\", \"_model.encoder.layers.7.layer_norm1.bias\", \"_model.encoder.layers.7.mlp.fc1.weight\", \"_model.encoder.layers.7.mlp.fc1.bias\", \"_model.encoder.layers.7.mlp.fc2.weight\", \"_model.encoder.layers.7.mlp.fc2.bias\", \"_model.encoder.layers.7.layer_norm2.weight\", \"_model.encoder.layers.7.layer_norm2.bias\", \"_model.encoder.layers.8.self_attn.k_proj.weight\", \"_model.encoder.layers.8.self_attn.k_proj.bias\", \"_model.encoder.layers.8.self_attn.v_proj.weight\", \"_model.encoder.layers.8.self_attn.v_proj.bias\", \"_model.encoder.layers.8.self_attn.q_proj.weight\", \"_model.encoder.layers.8.self_attn.q_proj.bias\", \"_model.encoder.layers.8.self_attn.out_proj.weight\", \"_model.encoder.layers.8.self_attn.out_proj.bias\", \"_model.encoder.layers.8.layer_norm1.weight\", \"_model.encoder.layers.8.layer_norm1.bias\", \"_model.encoder.layers.8.mlp.fc1.weight\", \"_model.encoder.layers.8.mlp.fc1.bias\", \"_model.encoder.layers.8.mlp.fc2.weight\", \"_model.encoder.layers.8.mlp.fc2.bias\", \"_model.encoder.layers.8.layer_norm2.weight\", \"_model.encoder.layers.8.layer_norm2.bias\", \"_model.encoder.layers.9.self_attn.k_proj.weight\", \"_model.encoder.layers.9.self_attn.k_proj.bias\", \"_model.encoder.layers.9.self_attn.v_proj.weight\", \"_model.encoder.layers.9.self_attn.v_proj.bias\", \"_model.encoder.layers.9.self_attn.q_proj.weight\", \"_model.encoder.layers.9.self_attn.q_proj.bias\", \"_model.encoder.layers.9.self_attn.out_proj.weight\", \"_model.encoder.layers.9.self_attn.out_proj.bias\", \"_model.encoder.layers.9.layer_norm1.weight\", \"_model.encoder.layers.9.layer_norm1.bias\", \"_model.encoder.layers.9.mlp.fc1.weight\", \"_model.encoder.layers.9.mlp.fc1.bias\", \"_model.encoder.layers.9.mlp.fc2.weight\", \"_model.encoder.layers.9.mlp.fc2.bias\", \"_model.encoder.layers.9.layer_norm2.weight\", \"_model.encoder.layers.9.layer_norm2.bias\", \"_model.encoder.layers.10.self_attn.k_proj.weight\", \"_model.encoder.layers.10.self_attn.k_proj.bias\", \"_model.encoder.layers.10.self_attn.v_proj.weight\", \"_model.encoder.layers.10.self_attn.v_proj.bias\", \"_model.encoder.layers.10.self_attn.q_proj.weight\", \"_model.encoder.layers.10.self_attn.q_proj.bias\", \"_model.encoder.layers.10.self_attn.out_proj.weight\", \"_model.encoder.layers.10.self_attn.out_proj.bias\", \"_model.encoder.layers.10.layer_norm1.weight\", \"_model.encoder.layers.10.layer_norm1.bias\", \"_model.encoder.layers.10.mlp.fc1.weight\", \"_model.encoder.layers.10.mlp.fc1.bias\", \"_model.encoder.layers.10.mlp.fc2.weight\", \"_model.encoder.layers.10.mlp.fc2.bias\", \"_model.encoder.layers.10.layer_norm2.weight\", \"_model.encoder.layers.10.layer_norm2.bias\", \"_model.encoder.layers.11.self_attn.k_proj.weight\", \"_model.encoder.layers.11.self_attn.k_proj.bias\", \"_model.encoder.layers.11.self_attn.v_proj.weight\", \"_model.encoder.layers.11.self_attn.v_proj.bias\", \"_model.encoder.layers.11.self_attn.q_proj.weight\", \"_model.encoder.layers.11.self_attn.q_proj.bias\", \"_model.encoder.layers.11.self_attn.out_proj.weight\", \"_model.encoder.layers.11.self_attn.out_proj.bias\", \"_model.encoder.layers.11.layer_norm1.weight\", \"_model.encoder.layers.11.layer_norm1.bias\", \"_model.encoder.layers.11.mlp.fc1.weight\", \"_model.encoder.layers.11.mlp.fc1.bias\", \"_model.encoder.layers.11.mlp.fc2.weight\", \"_model.encoder.layers.11.mlp.fc2.bias\", \"_model.encoder.layers.11.layer_norm2.weight\", \"_model.encoder.layers.11.layer_norm2.bias\", \"_model.post_layernorm.weight\", \"_model.post_layernorm.bias\". ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-28c441543257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclip_vision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenCLIPVisionBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ViT-B-32::openai'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclip_vision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/home/xz306/{run.name}/models/clip-vision/model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mclip_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenCLIPTextBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ViT-B-32::openai'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1604\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1605\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for OpenCLIPVisionModel:\n\tMissing key(s) in state_dict: \"_model.positional_embedding\", \"_model.text_projection\", \"_model.logit_scale\", \"_model.visual.class_embedding\", \"_model.visual.positional_embedding\", \"_model.visual.proj\", \"_model.visual.conv1.weight\", \"_model.visual.ln_pre.weight\", \"_model.visual.ln_pre.bias\", \"_model.visual.transformer.resblocks.0.ln_1.weight\", \"_model.visual.transformer.resblocks.0.ln_1.bias\", \"_model.visual.transformer.resblocks.0.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.0.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.0.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.0.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.0.ln_2.weight\", \"_model.visual.transformer.resblocks.0.ln_2.bias\", \"_model.visual.transformer.resblocks.0.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.0.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.0.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.0.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.1.ln_1.weight\", \"_model.visual.transformer.resblocks.1.ln_1.bias\", \"_model.visual.transformer.resblocks.1.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.1.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.1.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.1.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.1.ln_2.weight\", \"_model.visual.transformer.resblocks.1.ln_2.bias\", \"_model.visual.transformer.resblocks.1.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.1.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.1.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.1.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.2.ln_1.weight\", \"_model.visual.transformer.resblocks.2.ln_1.bias\", \"_model.visual.transformer.resblocks.2.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.2.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.2.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.2.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.2.ln_2.weight\", \"_model.visual.transformer.resblocks.2.ln_2.bias\", \"_model.visual.transformer.resblocks.2.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.2.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.2.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.2.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.3.ln_1.weight\", \"_model.visual.transformer.resblocks.3.ln_1.bias\", \"_model.visual.transformer.resblocks.3.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.3.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.3.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.3.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.3.ln_2.weight\", \"_model.visual.transformer.resblocks.3.ln_2.bias\", \"_model.visual.transformer.resblocks.3.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.3.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.3.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.3.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.4.ln_1.weight\", \"_model.visual.transformer.resblocks.4.ln_1.bias\", \"_model.visual.transformer.resblocks.4.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.4.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.4.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.4.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.4.ln_2.weight\", \"_model.visual.transformer.resblocks.4.ln_2.bias\", \"_model.visual.transformer.resblocks.4.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.4.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.4.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.4.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.5.ln_1.weight\", \"_model.visual.transformer.resblocks.5.ln_1.bias\", \"_model.visual.transformer.resblocks.5.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.5.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.5.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.5.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.5.ln_2.weight\", \"_model.visual.transformer.resblocks.5.ln_2.bias\", \"_model.visual.transformer.resblocks.5.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.5.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.5.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.5.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.6.ln_1.weight\", \"_model.visual.transformer.resblocks.6.ln_1.bias\", \"_model.visual.transformer.resblocks.6.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.6.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.6.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.6.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.6.ln_2.weight\", \"_model.visual.transformer.resblocks.6.ln_2.bias\", \"_model.visual.transformer.resblocks.6.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.6.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.6.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.6.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.7.ln_1.weight\", \"_model.visual.transformer.resblocks.7.ln_1.bias\", \"_model.visual.transformer.resblocks.7.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.7.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.7.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.7.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.7.ln_2.weight\", \"_model.visual.transformer.resblocks.7.ln_2.bias\", \"_model.visual.transformer.resblocks.7.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.7.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.7.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.7.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.8.ln_1.weight\", \"_model.visual.transformer.resblocks.8.ln_1.bias\", \"_model.visual.transformer.resblocks.8.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.8.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.8.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.8.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.8.ln_2.weight\", \"_model.visual.transformer.resblocks.8.ln_2.bias\", \"_model.visual.transformer.resblocks.8.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.8.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.8.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.8.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.9.ln_1.weight\", \"_model.visual.transformer.resblocks.9.ln_1.bias\", \"_model.visual.transformer.resblocks.9.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.9.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.9.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.9.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.9.ln_2.weight\", \"_model.visual.transformer.resblocks.9.ln_2.bias\", \"_model.visual.transformer.resblocks.9.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.9.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.9.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.9.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.10.ln_1.weight\", \"_model.visual.transformer.resblocks.10.ln_1.bias\", \"_model.visual.transformer.resblocks.10.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.10.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.10.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.10.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.10.ln_2.weight\", \"_model.visual.transformer.resblocks.10.ln_2.bias\", \"_model.visual.transformer.resblocks.10.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.10.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.10.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.10.mlp.c_proj.bias\", \"_model.visual.transformer.resblocks.11.ln_1.weight\", \"_model.visual.transformer.resblocks.11.ln_1.bias\", \"_model.visual.transformer.resblocks.11.attn.in_proj_weight\", \"_model.visual.transformer.resblocks.11.attn.in_proj_bias\", \"_model.visual.transformer.resblocks.11.attn.out_proj.weight\", \"_model.visual.transformer.resblocks.11.attn.out_proj.bias\", \"_model.visual.transformer.resblocks.11.ln_2.weight\", \"_model.visual.transformer.resblocks.11.ln_2.bias\", \"_model.visual.transformer.resblocks.11.mlp.c_fc.weight\", \"_model.visual.transformer.resblocks.11.mlp.c_fc.bias\", \"_model.visual.transformer.resblocks.11.mlp.c_proj.weight\", \"_model.visual.transformer.resblocks.11.mlp.c_proj.bias\", \"_model.visual.ln_post.weight\", \"_model.visual.ln_post.bias\", \"_model.transformer.resblocks.0.ln_1.weight\", \"_model.transformer.resblocks.0.ln_1.bias\", \"_model.transformer.resblocks.0.attn.in_proj_weight\", \"_model.transformer.resblocks.0.attn.in_proj_bias\", \"_model.transformer.resblocks.0.attn.out_proj.weight\", \"_model.transformer.resblocks.0.attn.out_proj.bias\", \"_model.transformer.resblocks.0.ln_2.weight\", \"_model.transformer.resblocks.0.ln_2.bias\", \"_model.transformer.resblocks.0.mlp.c_fc.weight\", \"_model.transformer.resblocks.0.mlp.c_fc.bias\", \"_model.transformer.resblocks.0.mlp.c_proj.weight\", \"_model.transformer.resblocks.0.mlp.c_proj.bias\", \"_model.transformer.resblocks.1.ln_1.weight\", \"_model.transformer.resblocks.1.ln_1.bias\", \"_model.transformer.resblocks.1.attn.in_proj_weight\", \"_model.transformer.resblocks.1.attn.in_proj_bias\", \"_model.transformer.resblocks.1.attn.out_proj.weight\", \"_model.transformer.resblocks.1.attn.out_proj.bias\", \"_model.transformer.resblocks.1.ln_2.weight\", \"_model.transformer.resblocks.1.ln_2.bias\", \"_model.transformer.resblocks.1.mlp.c_fc.weight\", \"_model.transformer.resblocks.1.mlp.c_fc.bias\", \"_model.transformer.resblocks.1.mlp.c_proj.weight\", \"_model.transformer.resblocks.1.mlp.c_proj.bias\", \"_model.transformer.resblocks.2.ln_1.weight\", \"_model.transformer.resblocks.2.ln_1.bias\", \"_model.transformer.resblocks.2.attn.in_proj_weight\", \"_model.transformer.resblocks.2.attn.in_proj_bias\", \"_model.transformer.resblocks.2.attn.out_proj.weight\", \"_model.transformer.resblocks.2.attn.out_proj.bias\", \"_model.transformer.resblocks.2.ln_2.weight\", \"_model.transformer.resblocks.2.ln_2.bias\", \"_model.transformer.resblocks.2.mlp.c_fc.weight\", \"_model.transformer.resblocks.2.mlp.c_fc.bias\", \"_model.transformer.resblocks.2.mlp.c_proj.weight\", \"_model.transformer.resblocks.2.mlp.c_proj.bias\", \"_model.transformer.resblocks.3.ln_1.weight\", \"_model.transformer.resblocks.3.ln_1.bias\", \"_model.transformer.resblocks.3.attn.in_proj_weight\", \"_model.transformer.resblocks.3.attn.in_proj_bias\", \"_model.transformer.resblocks.3.attn.out_proj.weight\", \"_model.transformer.resblocks.3.attn.out_proj.bias\", \"_model.transformer.resblocks.3.ln_2.weight\", \"_model.transformer.resblocks.3.ln_2.bias\", \"_model.transformer.resblocks.3.mlp.c_fc.weight\", \"_model.transformer.resblocks.3.mlp.c_fc.bias\", \"_model.transformer.resblocks.3.mlp.c_proj.weight\", \"_model.transformer.resblocks.3.mlp.c_proj.bias\", \"_model.transformer.resblocks.4.ln_1.weight\", \"_model.transformer.resblocks.4.ln_1.bias\", \"_model.transformer.resblocks.4.attn.in_proj_weight\", \"_model.transformer.resblocks.4.attn.in_proj_bias\", \"_model.transformer.resblocks.4.attn.out_proj.weight\", \"_model.transformer.resblocks.4.attn.out_proj.bias\", \"_model.transformer.resblocks.4.ln_2.weight\", \"_model.transformer.resblocks.4.ln_2.bias\", \"_model.transformer.resblocks.4.mlp.c_fc.weight\", \"_model.transformer.resblocks.4.mlp.c_fc.bias\", \"_model.transformer.resblocks.4.mlp.c_proj.weight\", \"_model.transformer.resblocks.4.mlp.c_proj.bias\", \"_model.transformer.resblocks.5.ln_1.weight\", \"_model.transformer.resblocks.5.ln_1.bias\", \"_model.transformer.resblocks.5.attn.in_proj_weight\", \"_model.transformer.resblocks.5.attn.in_proj_bias\", \"_model.transformer.resblocks.5.attn.out_proj.weight\", \"_model.transformer.resblocks.5.attn.out_proj.bias\", \"_model.transformer.resblocks.5.ln_2.weight\", \"_model.transformer.resblocks.5.ln_2.bias\", \"_model.transformer.resblocks.5.mlp.c_fc.weight\", \"_model.transformer.resblocks.5.mlp.c_fc.bias\", \"_model.transformer.resblocks.5.mlp.c_proj.weight\", \"_model.transformer.resblocks.5.mlp.c_proj.bias\", \"_model.transformer.resblocks.6.ln_1.weight\", \"_model.transformer.resblocks.6.ln_1.bias\", \"_model.transformer.resblocks.6.attn.in_proj_weight\", \"_model.transformer.resblocks.6.attn.in_proj_bias\", \"_model.transformer.resblocks.6.attn.out_proj.weight\", \"_model.transformer.resblocks.6.attn.out_proj.bias\", \"_model.transformer.resblocks.6.ln_2.weight\", \"_model.transformer.resblocks.6.ln_2.bias\", \"_model.transformer.resblocks.6.mlp.c_fc.weight\", \"_model.transformer.resblocks.6.mlp.c_fc.bias\", \"_model.transformer.resblocks.6.mlp.c_proj.weight\", \"_model.transformer.resblocks.6.mlp.c_proj.bias\", \"_model.transformer.resblocks.7.ln_1.weight\", \"_model.transformer.resblocks.7.ln_1.bias\", \"_model.transformer.resblocks.7.attn.in_proj_weight\", \"_model.transformer.resblocks.7.attn.in_proj_bias\", \"_model.transformer.resblocks.7.attn.out_proj.weight\", \"_model.transformer.resblocks.7.attn.out_proj.bias\", \"_model.transformer.resblocks.7.ln_2.weight\", \"_model.transformer.resblocks.7.ln_2.bias\", \"_model.transformer.resblocks.7.mlp.c_fc.weight\", \"_model.transformer.resblocks.7.mlp.c_fc.bias\", \"_model.transformer.resblocks.7.mlp.c_proj.weight\", \"_model.transformer.resblocks.7.mlp.c_proj.bias\", \"_model.transformer.resblocks.8.ln_1.weight\", \"_model.transformer.resblocks.8.ln_1.bias\", \"_model.transformer.resblocks.8.attn.in_proj_weight\", \"_model.transformer.resblocks.8.attn.in_proj_bias\", \"_model.transformer.resblocks.8.attn.out_proj.weight\", \"_model.transformer.resblocks.8.attn.out_proj.bias\", \"_model.transformer.resblocks.8.ln_2.weight\", \"_model.transformer.resblocks.8.ln_2.bias\", \"_model.transformer.resblocks.8.mlp.c_fc.weight\", \"_model.transformer.resblocks.8.mlp.c_fc.bias\", \"_model.transformer.resblocks.8.mlp.c_proj.weight\", \"_model.transformer.resblocks.8.mlp.c_proj.bias\", \"_model.transformer.resblocks.9.ln_1.weight\", \"_model.transformer.resblocks.9.ln_1.bias\", \"_model.transformer.resblocks.9.attn.in_proj_weight\", \"_model.transformer.resblocks.9.attn.in_proj_bias\", \"_model.transformer.resblocks.9.attn.out_proj.weight\", \"_model.transformer.resblocks.9.attn.out_proj.bias\", \"_model.transformer.resblocks.9.ln_2.weight\", \"_model.transformer.resblocks.9.ln_2.bias\", \"_model.transformer.resblocks.9.mlp.c_fc.weight\", \"_model.transformer.resblocks.9.mlp.c_fc.bias\", \"_model.transformer.resblocks.9.mlp.c_proj.weight\", \"_model.transformer.resblocks.9.mlp.c_proj.bias\", \"_model.transformer.resblocks.10.ln_1.weight\", \"_model.transformer.resblocks.10.ln_1.bias\", \"_model.transformer.resblocks.10.attn.in_proj_weight\", \"_model.transformer.resblocks.10.attn.in_proj_bias\", \"_model.transformer.resblocks.10.attn.out_proj.weight\", \"_model.transformer.resblocks.10.attn.out_proj.bias\", \"_model.transformer.resblocks.10.ln_2.weight\", \"_model.transformer.resblocks.10.ln_2.bias\", \"_model.transformer.resblocks.10.mlp.c_fc.weight\", \"_model.transformer.resblocks.10.mlp.c_fc.bias\", \"_model.transformer.resblocks.10.mlp.c_proj.weight\", \"_model.transformer.resblocks.10.mlp.c_proj.bias\", \"_model.transformer.resblocks.11.ln_1.weight\", \"_model.transformer.resblocks.11.ln_1.bias\", \"_model.transformer.resblocks.11.attn.in_proj_weight\", \"_model.transformer.resblocks.11.attn.in_proj_bias\", \"_model.transformer.resblocks.11.attn.out_proj.weight\", \"_model.transformer.resblocks.11.attn.out_proj.bias\", \"_model.transformer.resblocks.11.ln_2.weight\", \"_model.transformer.resblocks.11.ln_2.bias\", \"_model.transformer.resblocks.11.mlp.c_fc.weight\", \"_model.transformer.resblocks.11.mlp.c_fc.bias\", \"_model.transformer.resblocks.11.mlp.c_proj.weight\", \"_model.transformer.resblocks.11.mlp.c_proj.bias\", \"_model.token_embedding.weight\", \"_model.ln_final.weight\", \"_model.ln_final.bias\". \n\tUnexpected key(s) in state_dict: \"_projection.weight\", \"_model.embeddings.class_embedding\", \"_model.embeddings.position_ids\", \"_model.embeddings.patch_embedding.weight\", \"_model.embeddings.position_embedding.weight\", \"_model.pre_layrnorm.weight\", \"_model.pre_layrnorm.bias\", \"_model.encoder.layers.0.self_attn.k_proj.weight\", \"_model.encoder.layers.0.self_attn.k_proj.bias\", \"_model.encoder.layers.0.self_attn.v_proj.weight\", \"_model.encoder.layers.0.self_attn.v_proj.bias\", \"_model.encoder.layers.0.self_attn.q_proj.weight\", \"_model.encoder.layers.0.self_attn.q_proj.bias\", \"_model.encoder.layers.0.self_attn.out_proj.weight\", \"_model.encoder.layers.0.self_attn.out_proj.bias\", \"_model.encoder.layers.0.layer_norm1.weight\", \"_model.encoder.layers.0.layer_norm1.bias\", \"_model.encoder.layers.0.mlp.fc1.weight\", \"_model.encoder.layers.0.mlp.fc1.bias\", \"_model.encoder.layers.0.mlp.fc2.weight\", \"_model.encoder.layers.0.mlp.fc2.bias\", \"_model.encoder.layers.0.layer_norm2.weight\", \"_model.encoder.layers.0.layer_norm2.bias\", \"_model.encoder.layers.1.self_attn.k_proj.weight\", \"_model.encoder.layers.1.self_attn.k_proj.bias\", \"_model.encoder.layers.1.self_attn.v_proj.weight\", \"_model.encoder.layers.1.self_attn.v_proj.bias\", \"_model.encoder.layers.1.self_attn.q_proj.weight\", \"_model.encoder.layers.1.self_attn.q_proj.bias\", \"_model.encoder.layers.1.self_attn.out_proj.weight\", \"_model.encoder.layers.1.self_attn.out_proj.bias\", \"_model.encoder.layers.1.layer_norm1.weight\", \"_model.encoder.layers.1.layer_norm1.bias\", \"_model.encoder.layers.1.mlp.fc1.weight\", \"_model.encoder.layers.1.mlp.fc1.bias\", \"_model.encoder.layers.1.mlp.fc2.weight\", \"_model.encoder.layers.1.mlp.fc2.bias\", \"_model.encoder.layers.1.layer_norm2.weight\", \"_model.encoder.layers.1.layer_norm2.bias\", \"_model.encoder.layers.2.self_attn.k_proj.weight\", \"_model.encoder.layers.2.self_attn.k_proj.bias\", \"_model.encoder.layers.2.self_attn.v_proj.weight\", \"_model.encoder.layers.2.self_attn.v_proj.bias\", \"_model.encoder.layers.2.self_attn.q_proj.weight\", \"_model.encoder.layers.2.self_attn.q_proj.bias\", \"_model.encoder.layers.2.self_attn.out_proj.weight\", \"_model.encoder.layers.2.self_attn.out_proj.bias\", \"_model.encoder.layers.2.layer_norm1.weight\", \"_model.encoder.layers.2.layer_norm1.bias\", \"_model.encoder.layers.2.mlp.fc1.weight\", \"_model.encoder.layers.2.mlp.fc1.bias\", \"_model.encoder.layers.2.mlp.fc2.weight\", \"_model.encoder.layers.2.mlp.fc2.bias\", \"_model.encoder.layers.2.layer_norm2.weight\", \"_model.encoder.layers.2.layer_norm2.bias\", \"_model.encoder.layers.3.self_attn.k_proj.weight\", \"_model.encoder.layers.3.self_attn.k_proj.bias\", \"_model.encoder.layers.3.self_attn.v_proj.weight\", \"_model.encoder.layers.3.self_attn.v_proj.bias\", \"_model.encoder.layers.3.self_attn.q_proj.weight\", \"_model.encoder.layers.3.self_attn.q_proj.bias\", \"_model.encoder.layers.3.self_attn.out_proj.weight\", \"_model.encoder.layers.3.self_attn.out_proj.bias\", \"_model.encoder.layers.3.layer_norm1.weight\", \"_model.encoder.layers.3.layer_norm1.bias\", \"_model.encoder.layers.3.mlp.fc1.weight\", \"_model.encoder.layers.3.mlp.fc1.bias\", \"_model.encoder.layers.3.mlp.fc2.weight\", \"_model.encoder.layers.3.mlp.fc2.bias\", \"_model.encoder.layers.3.layer_norm2.weight\", \"_model.encoder.layers.3.layer_norm2.bias\", \"_model.encoder.layers.4.self_attn.k_proj.weight\", \"_model.encoder.layers.4.self_attn.k_proj.bias\", \"_model.encoder.layers.4.self_attn.v_proj.weight\", \"_model.encoder.layers.4.self_attn.v_proj.bias\", \"_model.encoder.layers.4.self_attn.q_proj.weight\", \"_model.encoder.layers.4.self_attn.q_proj.bias\", \"_model.encoder.layers.4.self_attn.out_proj.weight\", \"_model.encoder.layers.4.self_attn.out_proj.bias\", \"_model.encoder.layers.4.layer_norm1.weight\", \"_model.encoder.layers.4.layer_norm1.bias\", \"_model.encoder.layers.4.mlp.fc1.weight\", \"_model.encoder.layers.4.mlp.fc1.bias\", \"_model.encoder.layers.4.mlp.fc2.weight\", \"_model.encoder.layers.4.mlp.fc2.bias\", \"_model.encoder.layers.4.layer_norm2.weight\", \"_model.encoder.layers.4.layer_norm2.bias\", \"_model.encoder.layers.5.self_attn.k_proj.weight\", \"_model.encoder.layers.5.self_attn.k_proj.bias\", \"_model.encoder.layers.5.self_attn.v_proj.weight\", \"_model.encoder.layers.5.self_attn.v_proj.bias\", \"_model.encoder.layers.5.self_attn.q_proj.weight\", \"_model.encoder.layers.5.self_attn.q_proj.bias\", \"_model.encoder.layers.5.self_attn.out_proj.weight\", \"_model.encoder.layers.5.self_attn.out_proj.bias\", \"_model.encoder.layers.5.layer_norm1.weight\", \"_model.encoder.layers.5.layer_norm1.bias\", \"_model.encoder.layers.5.mlp.fc1.weight\", \"_model.encoder.layers.5.mlp.fc1.bias\", \"_model.encoder.layers.5.mlp.fc2.weight\", \"_model.encoder.layers.5.mlp.fc2.bias\", \"_model.encoder.layers.5.layer_norm2.weight\", \"_model.encoder.layers.5.layer_norm2.bias\", \"_model.encoder.layers.6.self_attn.k_proj.weight\", \"_model.encoder.layers.6.self_attn.k_proj.bias\", \"_model.encoder.layers.6.self_attn.v_proj.weight\", \"_model.encoder.layers.6.self_attn.v_proj.bias\", \"_model.encoder.layers.6.self_attn.q_proj.weight\", \"_model.encoder.layers.6.self_attn.q_proj.bias\", \"_model.encoder.layers.6.self_attn.out_proj.weight\", \"_model.encoder.layers.6.self_attn.out_proj.bias\", \"_model.encoder.layers.6.layer_norm1.weight\", \"_model.encoder.layers.6.layer_norm1.bias\", \"_model.encoder.layers.6.mlp.fc1.weight\", \"_model.encoder.layers.6.mlp.fc1.bias\", \"_model.encoder.layers.6.mlp.fc2.weight\", \"_model.encoder.layers.6.mlp.fc2.bias\", \"_model.encoder.layers.6.layer_norm2.weight\", \"_model.encoder.layers.6.layer_norm2.bias\", \"_model.encoder.layers.7.self_attn.k_proj.weight\", \"_model.encoder.layers.7.self_attn.k_proj.bias\", \"_model.encoder.layers.7.self_attn.v_proj.weight\", \"_model.encoder.layers.7.self_attn.v_proj.bias\", \"_model.encoder.layers.7.self_attn.q_proj.weight\", \"_model.encoder.layers.7.self_attn.q_proj.bias\", \"_model.encoder.layers.7.self_attn.out_proj.weight\", \"_model.encoder.layers.7.self_attn.out_proj.bias\", \"_model.encoder.layers.7.layer_norm1.weight\", \"_model.encoder.layers.7.layer_norm1.bias\", \"_model.encoder.layers.7.mlp.fc1.weight\", \"_model.encoder.layers.7.mlp.fc1.bias\", \"_model.encoder.layers.7.mlp.fc2.weight\", \"_model.encoder.layers.7.mlp.fc2.bias\", \"_model.encoder.layers.7.layer_norm2.weight\", \"_model.encoder.layers.7.layer_norm2.bias\", \"_model.encoder.layers.8.self_attn.k_proj.weight\", \"_model.encoder.layers.8.self_attn.k_proj.bias\", \"_model.encoder.layers.8.self_attn.v_proj.weight\", \"_model.encoder.layers.8.self_attn.v_proj.bias\", \"_model.encoder.layers.8.self_attn.q_proj.weight\", \"_model.encoder.layers.8.self_attn.q_proj.bias\", \"_model.encoder.layers.8.self_attn.out_proj.weight\", \"_model.encoder.layers.8.self_attn.out_proj.bias\", \"_model.encoder.layers.8.layer_norm1.weight\", \"_model.encoder.layers.8.layer_norm1.bias\", \"_model.encoder.layers.8.mlp.fc1.weight\", \"_model.encoder.layers.8.mlp.fc1.bias\", \"_model.encoder.layers.8.mlp.fc2.weight\", \"_model.encoder.layers.8.mlp.fc2.bias\", \"_model.encoder.layers.8.layer_norm2.weight\", \"_model.encoder.layers.8.layer_norm2.bias\", \"_model.encoder.layers.9.self_attn.k_proj.weight\", \"_model.encoder.layers.9.self_attn.k_proj.bias\", \"_model.encoder.layers.9.self_attn.v_proj.weight\", \"_model.encoder.layers.9.self_attn.v_proj.bias\", \"_model.encoder.layers.9.self_attn.q_proj.weight\", \"_model.encoder.layers.9.self_attn.q_proj.bias\", \"_model.encoder.layers.9.self_attn.out_proj.weight\", \"_model.encoder.layers.9.self_attn.out_proj.bias\", \"_model.encoder.layers.9.layer_norm1.weight\", \"_model.encoder.layers.9.layer_norm1.bias\", \"_model.encoder.layers.9.mlp.fc1.weight\", \"_model.encoder.layers.9.mlp.fc1.bias\", \"_model.encoder.layers.9.mlp.fc2.weight\", \"_model.encoder.layers.9.mlp.fc2.bias\", \"_model.encoder.layers.9.layer_norm2.weight\", \"_model.encoder.layers.9.layer_norm2.bias\", \"_model.encoder.layers.10.self_attn.k_proj.weight\", \"_model.encoder.layers.10.self_attn.k_proj.bias\", \"_model.encoder.layers.10.self_attn.v_proj.weight\", \"_model.encoder.layers.10.self_attn.v_proj.bias\", \"_model.encoder.layers.10.self_attn.q_proj.weight\", \"_model.encoder.layers.10.self_attn.q_proj.bias\", \"_model.encoder.layers.10.self_attn.out_proj.weight\", \"_model.encoder.layers.10.self_attn.out_proj.bias\", \"_model.encoder.layers.10.layer_norm1.weight\", \"_model.encoder.layers.10.layer_norm1.bias\", \"_model.encoder.layers.10.mlp.fc1.weight\", \"_model.encoder.layers.10.mlp.fc1.bias\", \"_model.encoder.layers.10.mlp.fc2.weight\", \"_model.encoder.layers.10.mlp.fc2.bias\", \"_model.encoder.layers.10.layer_norm2.weight\", \"_model.encoder.layers.10.layer_norm2.bias\", \"_model.encoder.layers.11.self_attn.k_proj.weight\", \"_model.encoder.layers.11.self_attn.k_proj.bias\", \"_model.encoder.layers.11.self_attn.v_proj.weight\", \"_model.encoder.layers.11.self_attn.v_proj.bias\", \"_model.encoder.layers.11.self_attn.q_proj.weight\", \"_model.encoder.layers.11.self_attn.q_proj.bias\", \"_model.encoder.layers.11.self_attn.out_proj.weight\", \"_model.encoder.layers.11.self_attn.out_proj.bias\", \"_model.encoder.layers.11.layer_norm1.weight\", \"_model.encoder.layers.11.layer_norm1.bias\", \"_model.encoder.layers.11.mlp.fc1.weight\", \"_model.encoder.layers.11.mlp.fc1.bias\", \"_model.encoder.layers.11.mlp.fc2.weight\", \"_model.encoder.layers.11.mlp.fc2.bias\", \"_model.encoder.layers.11.layer_norm2.weight\", \"_model.encoder.layers.11.layer_norm2.bias\", \"_model.post_layernorm.weight\", \"_model.post_layernorm.bias\". "
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "from _finetuner.models.builders import OpenCLIPVisionBuilder, OpenCLIPTextBuilder\n",
        "\n",
        "clip_vision = OpenCLIPVisionBuilder(descriptor='ViT-B-32::openai').build()\n",
        "##### I have chaned the dir to my path #####\n",
        "clip_vision.load_state_dict(torch.load(f'/home/xz306/{run.name}/models/clip-vision/model.pt'))\n",
        "\n",
        "clip_text = OpenCLIPTextBuilder(descriptor='ViT-B-32::openai').build()\n",
        "clip_text.load_state_dict(torch.load(f'/home/xz306/{run.name}/models/clip-text/model.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j_H2gzY8zGW"
      },
      "source": [
        "Then we can run CLIP benchmark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnbGx73pbxFU",
        "outputId": "d1f01226-28af-45e5-c7a3-aeeeecc03460"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "16it [00:16,  1.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dataset': 'flickr8k',\n",
            " 'metrics': {'image_retrieval_recall@5': 0.8537999987602234,\n",
            "             'text_retrieval_recall@5': 0.9100000262260437},\n",
            " 'model': 'ViT-B-32',\n",
            " 'pretrained': 'openai',\n",
            " 'task': 'finetuned'}\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Console script for clip_benchmark.\n",
        "Code copied from CLIP Benchmark with minor adjusts to run in colab.\n",
        "\"\"\"\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import open_clip\n",
        "from pprint import pprint\n",
        "\n",
        "from clip_benchmark.datasets.builder import build_dataset, get_dataset_collate_fn\n",
        "from clip_benchmark.metrics import  zeroshot_retrieval\n",
        "\n",
        "from torch.utils.data import default_collate\n",
        "\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "image_encoder = clip_vision.to(device)\n",
        "text_encoder = clip_text.to(device)\n",
        "_, _, transform = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
        "dataset = build_dataset(\n",
        "    dataset_name='flickr8k',\n",
        "    root='root',\n",
        "    transform=transform,\n",
        "    split='test',\n",
        "    annotation_file=None,\n",
        "    download=True,\n",
        ")\n",
        "collate_fn = get_dataset_collate_fn('flickr8k')\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=64,\n",
        "    shuffle=False, num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "metrics = zeroshot_retrieval.evaluate(\n",
        "    image_encoder,\n",
        "    text_encoder,\n",
        "    dataloader,\n",
        "    open_clip.tokenizer.tokenize,\n",
        "    recall_k_list=[5],\n",
        "    device=device,\n",
        "    amp=True\n",
        ")\n",
        "dump = {\n",
        "    \"dataset\": 'flickr8k',\n",
        "    \"model\": 'ViT-B-32',\n",
        "    \"pretrained\": 'openai',\n",
        "    \"task\": 'finetuned',\n",
        "    \"metrics\": metrics\n",
        "}\n",
        "pprint(dump)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODbzFgVGlBd_"
      },
      "source": [
        "## Results: Pre-Trained Zero-Shot vs Fine-Tuned\n",
        "\n",
        "The people responsible for CLIP Benchmark have published benchmarking results for a wide variety of models and configuarations in [this csv](https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/benchmark.csv).\n",
        "\n",
        "For simplicity, we show the comparsion below:\n",
        "\n",
        "+ `image_retrieval_recall@5`: use text queries to find top 5 similar images.\n",
        "+ `text_retrieval_recall@5`: use image to find top 5 similar text.\n",
        "\n",
        "\n",
        "| model                            | dataset       | imageRecall@5(zero-shot) | textRecall@5(zero-shot) | imageRecall@5(fine-tuned) | textRecall@5(fine-tuned) |\n",
        "|----------------------------------|---------------|-------------------|----------------------|---------|-------------|\n",
        "| ViT-B-32#openai                  | flickr8k      |0.5319737792015076 | 0.6991719007492065   |0.8537999987602234| 0.9100000262260437 |\n",
        "\n",
        "Apart from that, we have done some extensive experiments on three datasets, these are the results we get:\n",
        "\n",
        "\n",
        "| model                            | dataset       | imageRecall@5(zero-shot) | textRecall@5(zero-shot) | imageRecall@5(fine-tuned) | textRecall@5(fine-tuned) |\n",
        "|----------------------------------|---------------|-------------------|----------------------|---------|-------------|\n",
        "| ViT-B-32#openai                  | flickr8k      |0.5319737792015076 | 0.6991719007492065   |0.8651999831199646| 0.9079999923706055 |\n",
        "| ViT-B-16-plus-240                | flickr8k      |0.6441478133201599 | 0.7916203141212463   |0.8784000277519226| 0.9200000166893005 |\n",
        "| ViT-B-32-quickgelu#laion400m_e32 | flickr8k      |0.5787171125411987 | 0.7392163872718811   |0.849399983882904 | 0.9020000100135803 |\n",
        "| ViT-B-32#openai                  | flickr30k     |0.8338000178337097 | 0.9490000009536743   |0.9016000032424927| 0.9480000138282776 |\n",
        "| ViT-B-16-plus-240                | flickr30k     |0.8894000053405762 | 0.9710000157356262   |0.9169999957084656| 0.9710000157356262 |\n",
        "| ViT-B-32-quickgelu#laion400m_e32 | flickr30k     |0.8546000123023987 | 0.9409999847412109   |0.8715999722480774| 0.9290000200271606 |\n",
        "| ViT-B-32#openai                  | coco captions |0.5584565997123718 | 0.748199999332428    |0.6546581387519836| 0.7454000115394592 |\n",
        "| ViT-B-16-plus-240                | coco captions |0.6620951890945435 | 0.8101999759674072   |0.7120751738548279| 0.8136000037193298 |\n",
        "| ViT-B-32-quickgelu#laion400m_e32 | coco captions |0.6084766387939453 | 0.7675999999046326   |0.6713714599609375| 0.7635999917984009 |\n",
        "\n",
        "Our Finetuner hyper-parameters were: `learning_rate: 1e-6`, `epochs: 5`, `optimizer: Adam`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIPy1Bh5lxGf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "052b3cbaaf854241a5a9006b44b4164d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e7d0228b5b24d0ba23a493264adb815": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": "10px 0 0 0",
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "300px"
          }
        },
        "14fdf26fefb4453381b45dd79a53b42f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_668ad79a393f4d21bf601b2354ae3e84",
            "placeholder": "​",
            "style": "IPY_MODEL_7ea64cf5a7064a5fbf870f46616187e6",
            "value": "\n<div class='custom-container'>\n    <style>\n        .custom-container {\n            margin-top: 10px;\n            margin-bottom: 0;\n        }\n        .spaced {\n            margin: 20px 0;\n        }\n    </style>\n    <center>\n        <img src=https://d2vchdhjlcm3i6.cloudfront.net/Company+Logo/Light/Company+logo_light.svg width=175 alt=\"Jina AI\">\n        <div class='spaced'></div>\n        <p>\n            Please open <a href='https://jina-ai.us.auth0.com/authorize?response_mode=form_post&nonce=fe52fd0fa296f2302b33aab321a97623&state=fe52fd0fa296f2302b33aab321a97623&scope=profile+openid+email&redirect_uri=https%3A%2F%2Fapi.hubble.jina.ai%2Fv2%2Foidc%2FidpAuthorized&client_id=7pXAUAtiRqruNd6KJ6U3Zd9uhk5oLqZA&response_type=code&code_challenge=_XDNsaIW8OxdXBPEROkXG-mMtZlynYZuII8TN_l-SUA&code_challenge_method=S256' target='_blank'>this link</a> to continue the login process.\n        </p>\n    </center>\n</div>\n"
          }
        },
        "2e16156e982840e7bcf859bc8bac43c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35202cf8b3b64938b0437e3433ee7d1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5361787acc0948be974a31bd054632b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35202cf8b3b64938b0437e3433ee7d1e",
            "placeholder": "​",
            "style": "IPY_MODEL_2e16156e982840e7bcf859bc8bac43c0",
            "value": "\n<div class='custom-container'>\n    <style>\n        .custom-container {\n            margin-top: 10px;\n            margin-bottom: 0;\n        }\n        .spaced {\n            margin: 20px 0;\n        }\n    </style>\n    <center>\n        <img src=https://d2vchdhjlcm3i6.cloudfront.net/Company+Logo/Light/Company+logo_light.svg width=175 alt='Jina AI'>\n        <div class='spaced'></div>\n        <p>\n            You are logged in to Jina AI!\n        </p>\n        <p>\n            If you want to log in again, run <code>notebook_login(force=True)</code>.\n        </p>\n    </center>\n</div>\n"
          }
        },
        "5640c168070347138c6f12a89f3527f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74d5ed1405e14e7b82fa5c439509b08d",
              "IPY_MODEL_d22020b9cdcb4db5b903ec0510d77bf9",
              "IPY_MODEL_7a2cb071664a4ec985769c655941b3ab",
              "IPY_MODEL_c7ba7e7ad7a1478498bd1b7035cfa262"
            ],
            "layout": "IPY_MODEL_052b3cbaaf854241a5a9006b44b4164d"
          }
        },
        "59221f747d3d472a8eb4c7e8a819bf71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5361787acc0948be974a31bd054632b3"
            ],
            "layout": "IPY_MODEL_052b3cbaaf854241a5a9006b44b4164d"
          }
        },
        "668ad79a393f4d21bf601b2354ae3e84": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74d5ed1405e14e7b82fa5c439509b08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8429c73d5554f7e9f6578a7c04557af",
            "placeholder": "​",
            "style": "IPY_MODEL_85713ec3389446e6869e69a262023f43",
            "value": "\n<div class='custom-container'>\n    <style>\n        .button1 {\n            color: white;\n            background-color: #009191;\n            border: 1px solid #009191;\n        }\n        .button2 {\n            color: #009191;\n            background-color: white;\n            border: 1px solid #009191;\n        }\n        .link1 {\n            color:#009191;\n            position: relative;\n            top: 22px;\n            right: -120px;\n            z-index: 99;\n        }\n        .custom-container {\n            margin-top: 10px;\n            margin-bottom: -10px;\n        }\n        .spaced {\n            margin: 20px 0;\n        }\n    </style>\n    <center>\n        <img src=https://d2vchdhjlcm3i6.cloudfront.net/Company+Logo/Light/Company+logo_light.svg width=175 alt='Jina AI'>\n        <div class='spaced'></div>\n        <p>\n            Copy a <b>Personal Access Token</b>, paste it below, and press the <b>Token login</b> button.\n            <br>\n            If you don't have a token, press the <b>Browser login</b> button to log in via the browser.\n        </p>\n        <a\n            href='https://hub.jina.ai/user/tokens'\n            target='__blank'\n            class='link1'>\n                Create\n        </a>\n    </center>\n</div>\n"
          }
        },
        "77fd7e85ea4346c5bb3886f2f4c42bca": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_eedc32bcc61a430dadec9bca5a508745",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠦</span> Preparing to run, logs will be ready to pull when `status` is `STARTED`. Current status is `CREATED`\n</pre>\n",
                  "text/plain": "\u001b[32m⠦\u001b[0m Preparing to run, logs will be ready to pull when `status` is `STARTED`. Current status is `CREATED`\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "7a2cb071664a4ec985769c655941b3ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [
              "button1"
            ],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Token login",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_f27061fd69d24cd6a77a694aec8906f2",
            "style": "IPY_MODEL_b2492a07ae4649f29c0d0a8c23c4047e",
            "tooltip": ""
          }
        },
        "7ea64cf5a7064a5fbf870f46616187e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85713ec3389446e6869e69a262023f43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a56509f7c23d495fa81dc07b5ed38419": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14fdf26fefb4453381b45dd79a53b42f"
            ],
            "layout": "IPY_MODEL_052b3cbaaf854241a5a9006b44b4164d"
          }
        },
        "b2492a07ae4649f29c0d0a8c23c4047e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c7ba7e7ad7a1478498bd1b7035cfa262": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [
              "button2"
            ],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Browser login",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_0e7d0228b5b24d0ba23a493264adb815",
            "style": "IPY_MODEL_f75a1fb831f64ca59dfeb7ed8e411483",
            "tooltip": ""
          }
        },
        "c8429c73d5554f7e9f6578a7c04557af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdd962679dbb49e3873289ef6bd13633": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d22020b9cdcb4db5b903ec0510d77bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": true,
            "layout": "IPY_MODEL_cdd962679dbb49e3873289ef6bd13633",
            "placeholder": "Personal Access Token (PAT)",
            "style": "IPY_MODEL_f4b7f904482d4e5d9ab526fe7897f256",
            "value": ""
          }
        },
        "da795a9e9c8c47c489b520483c95713a": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f33b28cfdc71491baa49f843b1f51389",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠋</span> <span style=\"font-weight: bold\">Downloading</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">950009856/953017547</span> • <span style=\"color: #800000; text-decoration-color: #800000\">15933119 QPS</span> • <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> • <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">953.0 MB</span>\n</pre>\n",
                  "text/plain": "\u001b[32m⠋\u001b[0m \u001b[1mDownloading\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m950009856/953017547\u001b[0m • \u001b[31m15933119 QPS\u001b[0m • \u001b[36m0:00:01\u001b[0m • \u001b[1;34m953.0 MB\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "eedc32bcc61a430dadec9bca5a508745": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f27061fd69d24cd6a77a694aec8906f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "300px"
          }
        },
        "f33b28cfdc71491baa49f843b1f51389": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4b7f904482d4e5d9ab526fe7897f256": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f75a1fb831f64ca59dfeb7ed8e411483": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
